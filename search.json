[
  {
    "objectID": "fastai.html",
    "href": "fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "A collection of workbooks for Fast AI Practical Deep Learning for Coders\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Part 2 Lesson 10\n\n\nStable Diffusion from ‘Scratch’\n\n\n\nfastai\n\n\nstable diffusion\n\n\n\n\n\n\n\n\n\nJun 2, 2024\n\n\nspa-dev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dsa.html",
    "href": "dsa.html",
    "title": "LeetCode",
    "section": "",
    "text": "Data Structures & Algorithms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 49: Group Anagrams\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 217: Contains Duplicate\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\neasy\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 1: Two Integer Sum\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\neasy\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Data Structures & Algorithms\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nspa-dev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dsaposts/group-anagrams/index.html",
    "href": "dsaposts/group-anagrams/index.html",
    "title": "LeetCode 49: Group Anagrams",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of strings strs, group all anagrams together into sublists. You may return the output in any order.\nExample 1:\nInput: strs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\nOutput: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\nExample 2:\nInput: strs = [\"x\"]\nOutput: [[\"x\"]]\nExample 3:\nInput: strs = [\"\"]\nOutput: [[\"\"]]"
  },
  {
    "objectID": "dsaposts/group-anagrams/index.html#problem-description",
    "href": "dsaposts/group-anagrams/index.html#problem-description",
    "title": "LeetCode 49: Group Anagrams",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of strings strs, group all anagrams together into sublists. You may return the output in any order.\nExample 1:\nInput: strs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\nOutput: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\nExample 2:\nInput: strs = [\"x\"]\nOutput: [[\"x\"]]\nExample 3:\nInput: strs = [\"\"]\nOutput: [[\"\"]]"
  },
  {
    "objectID": "dsaposts/group-anagrams/index.html#initial-solution",
    "href": "dsaposts/group-anagrams/index.html#initial-solution",
    "title": "LeetCode 49: Group Anagrams",
    "section": "Initial Solution",
    "text": "Initial Solution\nThe key to this problem was to recognize that when anagrams are sorted alphabetically, they become the same ‘word’. With that in mind, a decent solution seemed relatively easy to come up with. Let’s just sort the words and place the original word in a dictionary with the keys being the unique sorted/garbled word.\nThere are a few parts that could potentially trip up a beginner:\nFirstly, calling sorted() on a string returns a list of the sorted characters and not a whole sorted word:\nword = 'hello'\nsorted_word = sorted(word)\nprint(sorted_word)\n&gt;&gt;&gt; ['e', 'h', 'l', 'l', 'o']\nSo we must join() the list back together afterwards.\nSecondly, adding values to a pre-existing key in the dictionary, instead of overwriting them, takes a little care, but there are a few options noted in this post. For example, using append() and extend(). The TLDR is to use extend() when you have multiple values to append, rather than using append() multiple times. E.g.\na = {}\na.setdefault('abc', []).append(1)       # {'abc': [1]}\na.setdefault('abc', []).extend([2, 3])  # a is now {'abc': [1, 2, 3]}\nsetdefault() will avoid a KeyError when we try to access a key that does not exist yet. It inserts the key with the specified default: []\nLet’s put this all together in our initial attempt at solving the anagram problem.\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        # will work fine without the if statement, but let's save some later computation:\n        if strs == [\"\"]:\n            return [[\"\"]]\n        \n        words = {}\n        for _, word in enumerate(strs):\n            sorted_word = ''.join(sorted(word)) # n⋅log(n) character sorting * m strings in the list\n            print(\"sorted_word:\", sorted_word)\n            words.setdefault(sorted_word, []).append(word)\n            print(\"dict value(s) of sorted_word:\", words[sorted_word])\n \n        return list(words.values())\n\n\n# Example for testing purposes.\nstrs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\n# Output: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\n# Note that output order does not matter.\n\nsolution = InitialSolution()\nsolution.groupAnagrams(strs)\n\nInitial Results:\nIntialSolution passes NeetCode submission.\n\nTime Complexity: O(m⋅n⋅logn) — due to sorting each string.\nSpace Complexity: O(m⋅n) — due to storing the strings in the dictionary keys and values."
  },
  {
    "objectID": "dsaposts/group-anagrams/index.html#neetcode-solution",
    "href": "dsaposts/group-anagrams/index.html#neetcode-solution",
    "title": "LeetCode 49: Group Anagrams",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nfrom collections import defaultdict\n\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        result = defaultdict(list)\n\n        for s in strs:\n            count = [0] * 26 # one for each character in a-z\n            for c in s:\n                # use ord to get unicode value of the character (minus that of 'a' to zero out 'a'):\n                count[ord(c) - ord('a')] += 1 # update the count for the corresponding character\n            result[tuple(count)].append(s) # convert count list to tuple, as lists can't be dict keys.\n        return result.values()\n\n\nsolution = Solution()\nlist(solution.groupAnagrams(strs)) \n\nI used list() above to display the result nicely in the notebook. The code seems to work fine in NeetCode both with and without the conversion to a list; no TypeError is given.\nConclusion:\nNeetcode’s solution uses a hashmap (defaultdict) called result that stores lists of anagrams, where the key is a tuple representing the character counts of the strings. Note that a defaultdict never raises a KeyError, but instead provides a default value for a key that does not exist.\n\nTime complexity O(m⋅n) — where m is the total number of strings and n is the length of each string (multiplied by 26 possible letters)\nSpace Complexity: O(m⋅n) — due to the space needed for storing the strings in the defaultdict values.\n\nGiven an extremely long string, the above solution would be optimal: O(m⋅n⋅logn) vs. O(m⋅n⋅26)"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html",
    "href": "dsaposts/contains-duplicate/index.html",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "",
    "text": "Difficulty: Easy\nGiven an integer array nums, return True if any value appears more than once in the array, otherwise return False.\nExample 1:\nInput: nums = [1, 2, 3, 3]\nOutput: True\nExample 2:\nInput: nums = [1, 2, 3, 4]\nOutput: False"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#problem-description",
    "href": "dsaposts/contains-duplicate/index.html#problem-description",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "",
    "text": "Difficulty: Easy\nGiven an integer array nums, return True if any value appears more than once in the array, otherwise return False.\nExample 1:\nInput: nums = [1, 2, 3, 3]\nOutput: True\nExample 2:\nInput: nums = [1, 2, 3, 4]\nOutput: False"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#initial-solution",
    "href": "dsaposts/contains-duplicate/index.html#initial-solution",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "Initial Solution",
    "text": "Initial Solution\nThis was really quite easy, using the Python set() function to create a set of unique numbers. We just need to ensure it returns True if the amount (length) of numbers in the set is different to that of the original list. Hence, the not operator is used below. Alternatively, replace == with != for the same result.\nNote that set() will not retain the order of the list, so we must compare its overall length and not try to compare the content one-by-one (or convert it to a list and see if the two lists match exactly).\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def hasDuplicate(self, nums: List[int]) -&gt; bool:\n        return not len(set(nums)) == len(nums)\n\n\nnums1 = [1, 2, 3, 3]\nnums2 = [1, 2, 3, 4]\n\n\nsolution = InitialSolution()\nprint(solution.hasDuplicate(nums1))\nprint(solution.hasDuplicate(nums2))\n\nInitial Results:\nSuccess. This solution passes the full suite of tests on NeetCode."
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#neetcode-solution",
    "href": "dsaposts/contains-duplicate/index.html#neetcode-solution",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\nThe following code makes it clear that we are using a hashset to store the values. It does not create the entire set all at once (like we do above, always giving O(n) time complexity as it runs through the whole list). Instead, it goes step by step; thus only in the worst case would this be O(n). We must create the hashset, which in the worst case uses O(n) space.\n\nclass Solution:\n    def hasDuplicate(self, nums: List[int]) -&gt; bool:\n        hashset = set()\n        for n in nums:\n            if n in hashset:\n                return True\n            hashset.add(n)\n        return False\n\n\nsolution = Solution()\nprint(solution.hasDuplicate(nums1))\nprint(solution.hasDuplicate(nums2))\n\nConclusion:\nAn easy start to NeetCode problems. The solution has a worst-case complexity of:\n\nTime complexity: O(n)\nSpace complexity: O(n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website. Work in Progress."
  },
  {
    "objectID": "dsaposts/welcome/index.html",
    "href": "dsaposts/welcome/index.html",
    "title": "Welcome To Data Structures & Algorithms",
    "section": "",
    "text": "Welcome! This is intended to be a repository of my solutions to LeetCode problems."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html",
    "href": "dsaposts/two-sum-integer/index.html",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "",
    "text": "Difficulty: Easy\nGiven an array of integers nums and an integer target, return the indices i and j such that nums[i] + nums[j] == target and i != j.\nAssume that every input has exactly one pair of indices i and j that satisfy the condition.\nReturn the answer with the smaller index first.\nExample:\nInput: \nnums = [3,4,5,6], target = 7\nOutput: [0,1]\n# Explanation: nums[0] + nums[1] == 7, so we return [0, 1].\nAdditional examples are provided in the tests below."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#problem-description",
    "href": "dsaposts/two-sum-integer/index.html#problem-description",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "",
    "text": "Difficulty: Easy\nGiven an array of integers nums and an integer target, return the indices i and j such that nums[i] + nums[j] == target and i != j.\nAssume that every input has exactly one pair of indices i and j that satisfy the condition.\nReturn the answer with the smaller index first.\nExample:\nInput: \nnums = [3,4,5,6], target = 7\nOutput: [0,1]\n# Explanation: nums[0] + nums[1] == 7, so we return [0, 1].\nAdditional examples are provided in the tests below."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#initial-solution",
    "href": "dsaposts/two-sum-integer/index.html#initial-solution",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "Initial Solution",
    "text": "Initial Solution\n\nfrom typing import List, Callable\n\n\nclass InitialSolution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        for i, i_num in enumerate(nums):\n            for j, j_num in enumerate(nums[1:]): \n                if i_num + j_num == target and i != j+1: # must 'and' to ensure i != j \n                    return [i,j+1]\n        return [] # not really needed as we are told a solution exists\n\nNote: I first tried to ensure i != j by starting the for j loop at [1:] instead of 0. This failed the following case, which led me to add the and operator in the equality check, at which point it was apparent the slicing was unnecessary. Careful reading of the question indicates that the indices are not allowed to be equal, but there is no restriction on the numbers being equal.\nnums = [2,5,5,11]\ntarget = 10\noutput = [1,2]\nLet’s clean up the function:\n\nclass BruteForceSolution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        for i, i_num in enumerate(nums):\n            for j, j_num in enumerate(nums): \n                if i_num + j_num == target and i != j:\n                    return [i,j]\n        return []\n\n\n# Examples for testing purposes.\n#nums, target = [3,4,5,6], 7 # output [0,1]\n#nums, target = [4,5,6], 10 # output [0,2]\n#nums, target = [3,3], 6 # output [0,1] \n#nums, target = [3,2,4], 6 # output [1,2]\nnums, target = [2,5,5,11], 10 # output [1,2] # tricky\n\n#solution = InitialSolution()\nsolution = BruteForceSolution()\nsolution.twoSum(nums, target)\n\nInitial Results:\nBruteForceSolution passes NeetCode submission.\n\nTime Complexity: O(n)*O(n) = O(n2)"
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#tests",
    "href": "dsaposts/two-sum-integer/index.html#tests",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "Tests",
    "text": "Tests\n\ndef test(fn: Callable[[List[int], int], List[int]]) -&gt; None:\n    nums, target = [3, 4, 5, 6], 7\n    output = [0, 1]\n    assert output == fn(nums, target)\n    \n    nums, target = [4, 5, 6], 10\n    output = [0, 2]\n    assert output == fn(nums, target)\n    \n    nums, target = [3, 3], 6\n    output = [0, 1]\n    assert output == fn(nums, target)\n    \n    nums, target = [3, 2, 4], 6\n    output = [1, 2]\n    assert output == fn(nums, target)\n    \n    nums, target = [2, 5, 5, 11], 10\n    output = [1, 2]\n    assert output == fn(nums, target)\n    print('Tests Passed')\n\n\nsolution = BruteForceSolution()\ntest(solution.twoSum)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "spa-dev: an obvious work-in-progress"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html",
    "title": "FastAI Part 2 Lesson 10",
    "section": "",
    "text": "This notebook is based on the notebook for FastAI Practical Deep Learning for Coders: Part 2 Lesson 10.\nIt’s not really from Stable Diffusion from scratch, but instead we will develop the diffusion model from its component parts on Huggingface. Some content, including text explanations, was copied from the official Huggingface blog post.\nSee also: FastAI notebook on GitHub"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#what-is-stable-diffusion",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#what-is-stable-diffusion",
    "title": "FastAI Part 2 Lesson 10",
    "section": "What is Stable Diffusion",
    "text": "What is Stable Diffusion\nThere are three main components in latent diffusion.\n\nAn autoencoder (VAE).\nA U-Net.\nA text-encoder, e.g. CLIP’s Text Encoder.\n\nThe output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n\nPNDM scheduler (used by default)\nDDIM scheduler\nK-LMS scheduler\n\nTo make things a bit different, we’ll use another scheduler. The standard pipeline uses the PNDM Scheduler, but we’ll use Katherine Crowson’s excellent K-LMS scheduler.\nWe need to be careful to use the same noising schedule that was used during training. The schedule is defined by the number of noising steps and the amount of noise added at each step, which is derived from the beta parameters."
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#set-up-environment",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#set-up-environment",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Set up environment",
    "text": "Set up environment\n\n# Kaggle Python 3 environment is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Installations and imports\n\n!pip install -Uq diffusers transformers fastcore\n\nimport logging\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\n# Show a smart progress meter: just wrap any iterable with tqdm(iterable)\nfrom tqdm.auto import tqdm\n\nlogging.disable(logging.WARNING)\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x7f98fa3730b0&gt;\n\n\n\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n\n\n# Set device\ndevice = (\n    \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cuda\"\n    if torch.cuda.is_available()\n    else \"cpu\"\n)\nprint(device)\n\ncuda\n\n\nIf your GPU is not big enough to use pipe, run pipe.enable_attention_slicing()\nAs described in the docs:\n&gt; When this option is enabled, the attention module will split the input tensor in slices, to compute attention in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n#pipe.enable_attention_slicing()"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#import-and-initialize-model-components",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#import-and-initialize-model-components",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Import and initialize model components",
    "text": "Import and initialize model components\nHere we perform the following actions:\n\nImport and initialize the tokenizer and text encoder for processing the prompts.\nImport and initialize the VAE and U-Net models.\nImport and initialize the LMSD scheduler.\n\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(device)\n\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(device)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(device)\n\n\nfrom diffusers import LMSDiscreteScheduler\n\nscheduler = LMSDiscreteScheduler(\n    beta_start = 0.00085,\n    beta_end = 0.012,\n    beta_schedule = 'scaled_linear',\n    num_train_timesteps = 1000\n); scheduler\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.28.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"steps_offset\": 0,\n  \"timestep_spacing\": \"linspace\",\n  \"trained_betas\": null,\n  \"use_karras_sigmas\": false\n}"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#create-initial-functions-for-testing",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#create-initial-functions-for-testing",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Create initial functions for testing",
    "text": "Create initial functions for testing\nLet’s create a few functions to perform the image generation, specifically:\n\nA text encoder to parse the prompt and return the text embeddings tensor\nA function to generate image samples based on the given text prompts\nA function to convert the tensor representations into images for display\n\nAs part of the FastAI ‘homework’, the ability to use negative prompts is included.\n\ndef text_enc(prompts, maxlen=None):\n    \"\"\"\n    Encodes text prompts into text embeddings using a pre-trained tokenizer and text encoder.\n    \n    Parameters:\n        prompts (list or str): A single text prompt or a list of text prompts to be encoded.\n        maxlen (int, optional): Maximum length for the tokenized sequences. \n                               Defaults to the maximum length supported by the tokenizer.\n    \n    Returns: torch.Tensor: Text embeddings corresponding to the input prompts.\n    \"\"\"\n    if maxlen is None:\n        maxlen = tokenizer.model_max_length\n    # Tokenize the prompts and create input tensors\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    # Generate text embeddings from the input tensors using a text encoder\n    text_embeddings = text_encoder(inp.input_ids.to(device))[0].half()\n    return text_embeddings\n\nThe main parameters needed for image generation are:\n\nPrompt(s). In our implementation, by default, the negative prompt is an empty string.\nImage dimensions (height and width).\nNumber of inference steps. Less inference will result in a noisier image.\nGuidance scale, which controls the influence of the text prompt on image generation. Lower guidance gives the model more freedom to ‘imagine’.\nBatch size\nRandom seed\n\nDefault values are provided in the function definition below, where applicable.\n\ndef mk_samples(prompts, negative_prompt=[''], guidance=7.5, seed=100, steps=70, height = 512, width = 512):\n    \"\"\"\n    Generates image samples based on the given text prompts using a pre-trained diffusion model.\n\n    Parameters:\n        prompts (list[str]): A list containing text string(s).\n        negative_prompt (list[str]), optional): A list of containing the negative text prompt. One string only.\n        guidance (float, optional): Guidance scale for the diffusion process.\n        seed (int, optional): Random seed for reproducibility.\n        steps (int, optional): Number of diffusion steps.\n        height (int, optional): Height of the output images.\n        width (int, optional): Width of the output images.\n\n    Returns:\n        torch.Tensor: Image samples generated based on the input prompts.\n    \"\"\"\n    bs = len(prompts)\n    text = text_enc(prompts)\n    #uncond = text_enc([\"\"] * bs, text.shape[1]) # implemented negative prompt instead:\n    uncond = text_enc(negative_prompt * bs, maxlen=text.shape[1])\n    \n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.config.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(device).half() * scheduler.init_noise_sigma\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        # predict the noise residual (and separate the text_embeddings and uncond_embeddings):\n        with torch.no_grad(): \n            pred_uncond, pred_text = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        # perform guidance\n        pred = pred_uncond + guidance * (pred_text - pred_uncond) \n        # compute the \"previous\" (next step) noisy sample\n        latents = scheduler.step(pred, ts, latents).prev_sample\n    #decompress latents\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n   \n\n\ndef mk_img(t):\n    \"\"\" \n    Converts a tensor representation of an image to a PIL Image for display.\n    Parameters: t (torch.Tensor): Tensor representation of an image, where values are in the range -1 to 1.\n    Returns: PIL.Image.Image: Image object suitable for display, with pixel values scaled to the range 0 to 255.\n    \"\"\"\n    # Scale and convert tensor values to a numpy array for image creation\n    image = (t / 2 + 0.5).clamp(0, 1).detach().cpu().permute(1, 2, 0).numpy()\n    # Convert the numpy array to a PIL Image with pixel values in the range 0 to 255\n    return Image.fromarray((image * 255).round().astype(\"uint8\"))\n\n\nTesting the functions\n\nprompts = [\n    'A spaceman with Martian sunset in the background',\n    'A great ape eating a plate of chips. Realistic fur.'\n]\nnegative_prompt = ['deformed, anime, cartoon, art'] # Note: current implementation accepts only a single string in the list, not a list of strings.\n\n\n# Example outputs and debugging.\n\ntext_input = tokenizer(prompts[0], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nprint(\"tokenizer 'input_ids' key: \" + str(text_input.input_ids))\n\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\nprint(\"text embeddings shape: \" + str(text_embeddings.shape))\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * len(prompts[0]), padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nprint(\"uncond embeddings shape: \" + str(uncond_embeddings.shape))\n\ntokenizer 'input_ids' key: tensor([[49406,   320,  7857,   786,   593, 30214,  3424,   530,   518,  5994,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\ntext embeddings shape: torch.Size([1, 77, 768])\nuncond embeddings shape: torch.Size([48, 77, 768])\n\n\n\nimages = mk_samples(prompts, negative_prompt)\n\n\n\n\n\nfrom IPython.display import display\n\n\nfor img in images: display(mk_img(img))"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#implement-diffuser-class",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#implement-diffuser-class",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Implement Diffuser Class",
    "text": "Implement Diffuser Class\nThe above functions work quite nicely, although the negative prompts could use a little work. At the moment we can only specify one negative prompt per batch of images. In most cases, that is fine anyway. For example, generally, we’d always want to avoid deformed images.\nLet’s put all the pieces together into a class:\n\nclass Diffuser:\n    \"\"\"\n    A class representing a text-to-image diffusion model.\n    \n    Args:\n        prompts (list[str]): List of text prompts.\n        negative_prompt (list[str], optional): Negative text prompt (a single string only). Default is an empty string.\n        guidance (float, optional): Guidance for diffusion process. Default is 7.5.\n        seed (int, optional): Random seed for reproducibility. Default is 100.\n        steps (int, optional): Number of diffusion steps. Default is 70.\n        width (int, optional): Width of the output image. Default is 512.\n        height (int, optional): Height of the output image. Default is 512.\n    \"\"\"\n    \n    def __init__(self, prompts, negative_prompt=[''], guidance=7.5, seed=100, steps=70, width=512, height=512):\n        self.prompts = prompts\n        self.bs = len(prompts)\n        self.negative_prompt = negative_prompt\n        self.guidance = guidance\n        self.seed = seed\n        self.steps = steps\n        self.w = width\n        self.h = height\n  \n    def diffuse(self, progress=0): # Progress indicator. Default is 0.\n        embs = self.set_embs()\n        lats = self.set_lats()\n        for i, ts in enumerate(tqdm(scheduler.timesteps)): lats = self.denoise(lats, embs, ts)\n        return self.decompress_lats(lats)\n  \n    def set_embs(self):\n        txt_inp = self.tokenizer_seq(self.prompts)\n        neg_inp = self.tokenizer_seq(self.negative_prompt * len(self.prompts))\n\n        txt_embs = self.make_embs(txt_inp['input_ids'])\n        neg_embs = self.make_embs(neg_inp['input_ids'])\n        return torch.cat([neg_embs, txt_embs])\n  \n    def tokenizer_seq(self, prompts, max_len=None):\n        if max_len is None: max_len = tokenizer.model_max_length\n        return tokenizer(prompts, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')    \n  \n    def make_embs(self, input_ids):\n        return text_encoder(input_ids.to(device))[0].half()\n\n    def set_lats(self):\n        torch.manual_seed(self.seed)\n        lats = torch.randn((self.bs, unet.config.in_channels, self.h//8, self.w//8))\n        scheduler.set_timesteps(self.steps)\n        return lats.to(device).half() * scheduler.init_noise_sigma\n\n    def denoise(self, latents, embeddings, timestep):\n        inp = scheduler.scale_model_input(torch.cat([latents]*2), timestep)\n        with torch.no_grad(): pred_neg, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n        pred = pred_neg + self.guidance * (pred_txt - pred_neg)\n        return scheduler.step(pred, timestep, latents).prev_sample\n\n    def decompress_lats(self, latents):\n        with torch.no_grad(): imgs = vae.decode(1/0.18215*latents).sample\n        imgs = (imgs / 2 + 0.5).clamp(0, 1)\n        imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n        return [(img*255).round().astype('uint8') for img in imgs]\n\n    def update_params(self, **kwargs):\n        allowed_params = ['prompts', 'negative_prompt', 'guidance', 'seed', 'steps', 'width', 'height']\n        for k, v in kwargs.items():\n            if k not in allowed_params:\n                raise ValueError(f\"Invalid parameter name: {k}\")\n        if k == 'prompts':\n            self.prompts = v\n            self.bs = len(v)\n        else:\n            setattr(self, k, v)\n\n\nprompts = [\n    'A spaceman with Martian sunset in the background',\n    'A great ape eating a plate of chips. Realistic fur.'\n]\nnegative_prompt = ['deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, \\\n                    text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, \\\n                    extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, \\\n                    bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, \\\n                    missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\n]\n\n\n# Create an instance of the Diffuser class\ndiffuser = Diffuser(prompts, negative_prompt)\n\n# Perform diffusion\nresult_images = diffuser.diffuse()\n\nfor img_array in result_images:\n    plt.imshow(img_array)\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#latents-and-callbacks",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#latents-and-callbacks",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Latents and callbacks",
    "text": "Latents and callbacks\nStable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models.\nGeneral diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image. For a more detailed overview of how they work, check this colab.\nDiffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.\nLatent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\nThe Stable Diffusion pipeline can send intermediate latents to a callback function we provide. By running these latents through the image decoder, we can see how the denoising process progresses and the image unfolds.\n\nImplement callbacks\nFor the next part of the FastAI ‘homework’, let’s implement callbacks. We’ll modify Diffuser.diffuse() to output the latent at a pre-specified interval. A big thank you to ForBo7 for the key ideas here.\nSee: ForBo7 blog post\n\ndef diffuse_with_callback(self, interval=0):\n    \"\"\"\n    Diffuses the input text prompts to generate images using a pre-trained diffusion model.\n    \n    Parameters:\n        interval (int, optional): Specifies the interval for displaying image callbacks.\n    \n    Returns:\n        torch.Tensor: Image samples generated based on the input prompts.\n    \"\"\"\n    embs = self.set_embs()\n    lats = self.set_lats()\n    \n    if interval &gt; 0: # Check if callbacks are needed.\n        row = [] \n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n            # Check if desired interval is reached.\n            # If the current loop number matches the interval, it should divide the interval cleanly.\n            if (i % interval) == 0: \n                row.append(self.decompress_lats(lats)[0])\n  \n        row = np.concatenate(row, axis=1) # Place all images into one long line.\n        display(Image.fromarray(row))\n\n    else:\n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n    \n    return self.decompress_lats(lats)\n\n\nprompts = ['A spaceman with Martian sunset in the background']\nnegative_prompt = ['deformed, anime, cartoon, art']\n\n\n# Create an instance of the Diffuser class\ndiffuser = Diffuser(prompts, negative_prompt)\n\n# Replace the existing diffuse method with the new diffuse_and_callback method.\ndiffuser.diffuse = diffuse_with_callback.__get__(diffuser, Diffuser)\n\n# Perform diffusion\nresult_images = diffuser.diffuse(interval=5)[0]\n\nImage.fromarray(result_images)"
  }
]