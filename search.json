[
  {
    "objectID": "fastai.html",
    "href": "fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "A collection of workbooks for Fast AI Practical Deep Learning for Coders\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Part 2 Lesson 12\n\n\nMean Shift Clustering\n\n\n\nfastai\n\n\nmeanshift\n\n\nk-means\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Part 2 Lesson 10\n\n\nStable Diffusion from ‘Scratch’\n\n\n\nfastai\n\n\nstable diffusion\n\n\n\n\n\n\n\n\n\nJun 2, 2024\n\n\nspa-dev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html",
    "title": "FastAI Part 2 Lesson 10",
    "section": "",
    "text": "This notebook is based on the notebook for FastAI Practical Deep Learning for Coders: Part 2 Lesson 10.\nIt’s not really from Stable Diffusion from scratch, but instead we will develop the diffusion model from its component parts on Huggingface. Some content, including text explanations, was copied from the official Huggingface blog post.\nSee also: FastAI notebook on GitHub"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#what-is-stable-diffusion",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#what-is-stable-diffusion",
    "title": "FastAI Part 2 Lesson 10",
    "section": "What is Stable Diffusion",
    "text": "What is Stable Diffusion\nThere are three main components in latent diffusion.\n\nAn autoencoder (VAE).\nA U-Net.\nA text-encoder, e.g. CLIP’s Text Encoder.\n\nThe output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n\nPNDM scheduler (used by default)\nDDIM scheduler\nK-LMS scheduler\n\nTo make things a bit different, we’ll use another scheduler. The standard pipeline uses the PNDM Scheduler, but we’ll use Katherine Crowson’s excellent K-LMS scheduler.\nWe need to be careful to use the same noising schedule that was used during training. The schedule is defined by the number of noising steps and the amount of noise added at each step, which is derived from the beta parameters."
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#set-up-environment",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#set-up-environment",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Set up environment",
    "text": "Set up environment\n\n# Kaggle Python 3 environment is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Installations and imports\n\n!pip install -Uq diffusers transformers fastcore\n\nimport logging\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\n# Show a smart progress meter: just wrap any iterable with tqdm(iterable)\nfrom tqdm.auto import tqdm\n\nlogging.disable(logging.WARNING)\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x7f98fa3730b0&gt;\n\n\n\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n\n\n# Set device\ndevice = (\n    \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cuda\"\n    if torch.cuda.is_available()\n    else \"cpu\"\n)\nprint(device)\n\ncuda\n\n\nIf your GPU is not big enough to use pipe, run pipe.enable_attention_slicing()\nAs described in the docs:\n&gt; When this option is enabled, the attention module will split the input tensor in slices, to compute attention in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n#pipe.enable_attention_slicing()"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#import-and-initialize-model-components",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#import-and-initialize-model-components",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Import and initialize model components",
    "text": "Import and initialize model components\nHere we perform the following actions:\n\nImport and initialize the tokenizer and text encoder for processing the prompts.\nImport and initialize the VAE and U-Net models.\nImport and initialize the LMSD scheduler.\n\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(device)\n\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(device)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(device)\n\n\nfrom diffusers import LMSDiscreteScheduler\n\nscheduler = LMSDiscreteScheduler(\n    beta_start = 0.00085,\n    beta_end = 0.012,\n    beta_schedule = 'scaled_linear',\n    num_train_timesteps = 1000\n); scheduler\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.28.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"steps_offset\": 0,\n  \"timestep_spacing\": \"linspace\",\n  \"trained_betas\": null,\n  \"use_karras_sigmas\": false\n}"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#create-initial-functions-for-testing",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#create-initial-functions-for-testing",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Create initial functions for testing",
    "text": "Create initial functions for testing\nLet’s create a few functions to perform the image generation, specifically:\n\nA text encoder to parse the prompt and return the text embeddings tensor\nA function to generate image samples based on the given text prompts\nA function to convert the tensor representations into images for display\n\nAs part of the FastAI ‘homework’, the ability to use negative prompts is included.\n\ndef text_enc(prompts, maxlen=None):\n    \"\"\"\n    Encodes text prompts into text embeddings using a pre-trained tokenizer and text encoder.\n    \n    Parameters:\n        prompts (list or str): A single text prompt or a list of text prompts to be encoded.\n        maxlen (int, optional): Maximum length for the tokenized sequences. \n                               Defaults to the maximum length supported by the tokenizer.\n    \n    Returns: torch.Tensor: Text embeddings corresponding to the input prompts.\n    \"\"\"\n    if maxlen is None:\n        maxlen = tokenizer.model_max_length\n    # Tokenize the prompts and create input tensors\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    # Generate text embeddings from the input tensors using a text encoder\n    text_embeddings = text_encoder(inp.input_ids.to(device))[0].half()\n    return text_embeddings\n\nThe main parameters needed for image generation are:\n\nPrompt(s). In our implementation, by default, the negative prompt is an empty string.\nImage dimensions (height and width).\nNumber of inference steps. Less inference will result in a noisier image.\nGuidance scale, which controls the influence of the text prompt on image generation. Lower guidance gives the model more freedom to ‘imagine’.\nBatch size\nRandom seed\n\nDefault values are provided in the function definition below, where applicable.\n\ndef mk_samples(prompts, negative_prompt=[''], guidance=7.5, seed=100, steps=70, height = 512, width = 512):\n    \"\"\"\n    Generates image samples based on the given text prompts using a pre-trained diffusion model.\n\n    Parameters:\n        prompts (list[str]): A list containing text string(s).\n        negative_prompt (list[str]), optional): A list of containing the negative text prompt. One string only.\n        guidance (float, optional): Guidance scale for the diffusion process.\n        seed (int, optional): Random seed for reproducibility.\n        steps (int, optional): Number of diffusion steps.\n        height (int, optional): Height of the output images.\n        width (int, optional): Width of the output images.\n\n    Returns:\n        torch.Tensor: Image samples generated based on the input prompts.\n    \"\"\"\n    bs = len(prompts)\n    text = text_enc(prompts)\n    #uncond = text_enc([\"\"] * bs, text.shape[1]) # implemented negative prompt instead:\n    uncond = text_enc(negative_prompt * bs, maxlen=text.shape[1])\n    \n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.config.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(device).half() * scheduler.init_noise_sigma\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        # predict the noise residual (and separate the text_embeddings and uncond_embeddings):\n        with torch.no_grad(): \n            pred_uncond, pred_text = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        # perform guidance\n        pred = pred_uncond + guidance * (pred_text - pred_uncond) \n        # compute the \"previous\" (next step) noisy sample\n        latents = scheduler.step(pred, ts, latents).prev_sample\n    #decompress latents\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n   \n\n\ndef mk_img(t):\n    \"\"\" \n    Converts a tensor representation of an image to a PIL Image for display.\n    Parameters: t (torch.Tensor): Tensor representation of an image, where values are in the range -1 to 1.\n    Returns: PIL.Image.Image: Image object suitable for display, with pixel values scaled to the range 0 to 255.\n    \"\"\"\n    # Scale and convert tensor values to a numpy array for image creation\n    image = (t / 2 + 0.5).clamp(0, 1).detach().cpu().permute(1, 2, 0).numpy()\n    # Convert the numpy array to a PIL Image with pixel values in the range 0 to 255\n    return Image.fromarray((image * 255).round().astype(\"uint8\"))\n\n\nTesting the functions\n\nprompts = [\n    'A spaceman with Martian sunset in the background',\n    'A great ape eating a plate of chips. Realistic fur.'\n]\nnegative_prompt = ['deformed, anime, cartoon, art'] # Note: current implementation accepts only a single string in the list, not a list of strings.\n\n\n# Example outputs and debugging.\n\ntext_input = tokenizer(prompts[0], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nprint(\"tokenizer 'input_ids' key: \" + str(text_input.input_ids))\n\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\nprint(\"text embeddings shape: \" + str(text_embeddings.shape))\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * len(prompts[0]), padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nprint(\"uncond embeddings shape: \" + str(uncond_embeddings.shape))\n\ntokenizer 'input_ids' key: tensor([[49406,   320,  7857,   786,   593, 30214,  3424,   530,   518,  5994,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\ntext embeddings shape: torch.Size([1, 77, 768])\nuncond embeddings shape: torch.Size([48, 77, 768])\n\n\n\nimages = mk_samples(prompts, negative_prompt)\n\n\n\n\n\nfrom IPython.display import display\n\n\nfor img in images: display(mk_img(img))"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#implement-diffuser-class",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#implement-diffuser-class",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Implement Diffuser Class",
    "text": "Implement Diffuser Class\nThe above functions work quite nicely, although the negative prompts could use a little work. At the moment we can only specify one negative prompt per batch of images. In most cases, that is fine anyway. For example, generally, we’d always want to avoid deformed images.\nLet’s put all the pieces together into a class:\n\nclass Diffuser:\n    \"\"\"\n    A class representing a text-to-image diffusion model.\n    \n    Args:\n        prompts (list[str]): List of text prompts.\n        negative_prompt (list[str], optional): Negative text prompt (a single string only). Default is an empty string.\n        guidance (float, optional): Guidance for diffusion process. Default is 7.5.\n        seed (int, optional): Random seed for reproducibility. Default is 100.\n        steps (int, optional): Number of diffusion steps. Default is 70.\n        width (int, optional): Width of the output image. Default is 512.\n        height (int, optional): Height of the output image. Default is 512.\n    \"\"\"\n    \n    def __init__(self, prompts, negative_prompt=[''], guidance=7.5, seed=100, steps=70, width=512, height=512):\n        self.prompts = prompts\n        self.bs = len(prompts)\n        self.negative_prompt = negative_prompt\n        self.guidance = guidance\n        self.seed = seed\n        self.steps = steps\n        self.w = width\n        self.h = height\n  \n    def diffuse(self, progress=0): # Progress indicator. Default is 0.\n        embs = self.set_embs()\n        lats = self.set_lats()\n        for i, ts in enumerate(tqdm(scheduler.timesteps)): lats = self.denoise(lats, embs, ts)\n        return self.decompress_lats(lats)\n  \n    def set_embs(self):\n        txt_inp = self.tokenizer_seq(self.prompts)\n        neg_inp = self.tokenizer_seq(self.negative_prompt * len(self.prompts))\n\n        txt_embs = self.make_embs(txt_inp['input_ids'])\n        neg_embs = self.make_embs(neg_inp['input_ids'])\n        return torch.cat([neg_embs, txt_embs])\n  \n    def tokenizer_seq(self, prompts, max_len=None):\n        if max_len is None: max_len = tokenizer.model_max_length\n        return tokenizer(prompts, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')    \n  \n    def make_embs(self, input_ids):\n        return text_encoder(input_ids.to(device))[0].half()\n\n    def set_lats(self):\n        torch.manual_seed(self.seed)\n        lats = torch.randn((self.bs, unet.config.in_channels, self.h//8, self.w//8))\n        scheduler.set_timesteps(self.steps)\n        return lats.to(device).half() * scheduler.init_noise_sigma\n\n    def denoise(self, latents, embeddings, timestep):\n        inp = scheduler.scale_model_input(torch.cat([latents]*2), timestep)\n        with torch.no_grad(): pred_neg, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n        pred = pred_neg + self.guidance * (pred_txt - pred_neg)\n        return scheduler.step(pred, timestep, latents).prev_sample\n\n    def decompress_lats(self, latents):\n        with torch.no_grad(): imgs = vae.decode(1/0.18215*latents).sample\n        imgs = (imgs / 2 + 0.5).clamp(0, 1)\n        imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n        return [(img*255).round().astype('uint8') for img in imgs]\n\n    def update_params(self, **kwargs):\n        allowed_params = ['prompts', 'negative_prompt', 'guidance', 'seed', 'steps', 'width', 'height']\n        for k, v in kwargs.items():\n            if k not in allowed_params:\n                raise ValueError(f\"Invalid parameter name: {k}\")\n        if k == 'prompts':\n            self.prompts = v\n            self.bs = len(v)\n        else:\n            setattr(self, k, v)\n\n\nprompts = [\n    'A spaceman with Martian sunset in the background',\n    'A great ape eating a plate of chips. Realistic fur.'\n]\nnegative_prompt = ['deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, \\\n                    text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, \\\n                    extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, \\\n                    bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, \\\n                    missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\n]\n\n\n# Create an instance of the Diffuser class\ndiffuser = Diffuser(prompts, negative_prompt)\n\n# Perform diffusion\nresult_images = diffuser.diffuse()\n\nfor img_array in result_images:\n    plt.imshow(img_array)\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-sd.html#latents-and-callbacks",
    "href": "fastai/fastai-P2L10/fastai-P2L10-sd.html#latents-and-callbacks",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Latents and callbacks",
    "text": "Latents and callbacks\nStable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models.\nGeneral diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image. For a more detailed overview of how they work, check this colab.\nDiffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.\nLatent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\nThe Stable Diffusion pipeline can send intermediate latents to a callback function we provide. By running these latents through the image decoder, we can see how the denoising process progresses and the image unfolds.\n\nImplement callbacks\nFor the next part of the FastAI ‘homework’, let’s implement callbacks. We’ll modify Diffuser.diffuse() to output the latent at a pre-specified interval. A big thank you to ForBo7 for the key ideas here.\nSee: ForBo7 blog post\n\ndef diffuse_with_callback(self, interval=0):\n    \"\"\"\n    Diffuses the input text prompts to generate images using a pre-trained diffusion model.\n    \n    Parameters:\n        interval (int, optional): Specifies the interval for displaying image callbacks.\n    \n    Returns:\n        torch.Tensor: Image samples generated based on the input prompts.\n    \"\"\"\n    embs = self.set_embs()\n    lats = self.set_lats()\n    \n    if interval &gt; 0: # Check if callbacks are needed.\n        row = [] \n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n            # Check if desired interval is reached.\n            # If the current loop number matches the interval, it should divide the interval cleanly.\n            if (i % interval) == 0: \n                row.append(self.decompress_lats(lats)[0])\n  \n        row = np.concatenate(row, axis=1) # Place all images into one long line.\n        display(Image.fromarray(row))\n\n    else:\n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n    \n    return self.decompress_lats(lats)\n\n\nprompts = ['A spaceman with Martian sunset in the background']\nnegative_prompt = ['deformed, anime, cartoon, art']\n\n\n# Create an instance of the Diffuser class\ndiffuser = Diffuser(prompts, negative_prompt)\n\n# Replace the existing diffuse method with the new diffuse_and_callback method.\ndiffuser.diffuse = diffuse_with_callback.__get__(diffuser, Diffuser)\n\n# Perform diffusion\nresult_images = diffuser.diffuse(interval=5)[0]\n\nImage.fromarray(result_images)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "spa-dev: an obvious work-in-progress"
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html",
    "href": "dsaposts/two-sum-integer/index.html",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "",
    "text": "Difficulty: Easy\nGiven an array of integers nums and an integer target, return the indices i and j such that nums[i] + nums[j] == target and i != j.\nAssume that every input has exactly one pair of indices i and j that satisfy the condition.\nReturn the answer with the smaller index first.\nExample:\nInput: \nnums = [3,4,5,6], target = 7\nOutput: [0,1]\n# Explanation: nums[0] + nums[1] == 7, so we return [0, 1].\nAdditional examples are provided in the tests below."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#problem-description",
    "href": "dsaposts/two-sum-integer/index.html#problem-description",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "",
    "text": "Difficulty: Easy\nGiven an array of integers nums and an integer target, return the indices i and j such that nums[i] + nums[j] == target and i != j.\nAssume that every input has exactly one pair of indices i and j that satisfy the condition.\nReturn the answer with the smaller index first.\nExample:\nInput: \nnums = [3,4,5,6], target = 7\nOutput: [0,1]\n# Explanation: nums[0] + nums[1] == 7, so we return [0, 1].\nAdditional examples are provided in the tests below."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#initial-solution",
    "href": "dsaposts/two-sum-integer/index.html#initial-solution",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "Initial Solution",
    "text": "Initial Solution\n\nfrom typing import List, Callable\n\n\nclass InitialSolution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        for i, i_num in enumerate(nums):\n            for j, j_num in enumerate(nums[1:]): \n                if i_num + j_num == target and i != j+1: # must 'and' to ensure i != j \n                    return [i,j+1]\n        return [] # not really needed as we are told a solution exists\n\nNote: I first tried to ensure i != j by starting the for j loop at [1:] instead of 0. This failed the following case, which led me to add the and operator in the equality check, at which point it was apparent the slicing was unnecessary. Careful reading of the question indicates that the indices are not allowed to be equal, but there is no restriction on the numbers being equal.\nnums = [2,5,5,11]\ntarget = 10\noutput = [1,2]\nLet’s clean up the function:\n\nclass BruteForceSolution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        for i, i_num in enumerate(nums):\n            for j, j_num in enumerate(nums): \n                if i_num + j_num == target and i != j:\n                    return [i,j]\n        return []\n\n\n# Examples for testing purposes.\n#nums, target = [3,4,5,6], 7 # output [0,1]\n#nums, target = [4,5,6], 10 # output [0,2]\n#nums, target = [3,3], 6 # output [0,1] \n#nums, target = [3,2,4], 6 # output [1,2]\nnums, target = [2,5,5,11], 10 # output [1,2] # tricky\n\n#solution = InitialSolution()\nsolution = BruteForceSolution()\nsolution.twoSum(nums, target)\n\nInitial Results:\nBruteForceSolution passes NeetCode submission.\n\nTime Complexity: O(n)*O(n) = O(n2)"
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#tests",
    "href": "dsaposts/two-sum-integer/index.html#tests",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "Tests",
    "text": "Tests\n\ndef test(fn: Callable[[List[int], int], List[int]]) -&gt; None:\n    nums, target = [3, 4, 5, 6], 7\n    output = [0, 1]\n    assert output == fn(nums, target)\n    \n    nums, target = [4, 5, 6], 10\n    output = [0, 2]\n    assert output == fn(nums, target)\n    \n    nums, target = [3, 3], 6\n    output = [0, 1]\n    assert output == fn(nums, target)\n    \n    nums, target = [3, 2, 4], 6\n    output = [1, 2]\n    assert output == fn(nums, target)\n    \n    nums, target = [2, 5, 5, 11], 10\n    output = [1, 2]\n    assert output == fn(nums, target)\n    print('Tests Passed')\n\n\nsolution = BruteForceSolution()\ntest(solution.twoSum)"
  },
  {
    "objectID": "dsaposts/welcome/index.html",
    "href": "dsaposts/welcome/index.html",
    "title": "Welcome To Data Structures & Algorithms",
    "section": "",
    "text": "Welcome! This is intended to be a repository of my solutions to LeetCode problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website. Work in Progress."
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html",
    "href": "dsaposts/contains-duplicate/index.html",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "",
    "text": "Difficulty: Easy\nGiven an integer array nums, return True if any value appears more than once in the array, otherwise return False.\nExample 1:\nInput: nums = [1, 2, 3, 3]\nOutput: True\nExample 2:\nInput: nums = [1, 2, 3, 4]\nOutput: False"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#problem-description",
    "href": "dsaposts/contains-duplicate/index.html#problem-description",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "",
    "text": "Difficulty: Easy\nGiven an integer array nums, return True if any value appears more than once in the array, otherwise return False.\nExample 1:\nInput: nums = [1, 2, 3, 3]\nOutput: True\nExample 2:\nInput: nums = [1, 2, 3, 4]\nOutput: False"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#initial-solution",
    "href": "dsaposts/contains-duplicate/index.html#initial-solution",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "Initial Solution",
    "text": "Initial Solution\nThis was really quite easy, using the Python set() function to create a set of unique numbers. We just need to ensure it returns True if the amount (length) of numbers in the set is different to that of the original list. Hence, the not operator is used below. Alternatively, replace == with != for the same result.\nNote that set() will not retain the order of the list, so we must compare its overall length and not try to compare the content one-by-one (or convert it to a list and see if the two lists match exactly).\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def hasDuplicate(self, nums: List[int]) -&gt; bool:\n        return not len(set(nums)) == len(nums)\n\n\nnums1 = [1, 2, 3, 3]\nnums2 = [1, 2, 3, 4]\n\n\nsolution = InitialSolution()\nprint(solution.hasDuplicate(nums1))\nprint(solution.hasDuplicate(nums2))\n\nInitial Results:\nSuccess. This solution passes the full suite of tests on NeetCode."
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#neetcode-solution",
    "href": "dsaposts/contains-duplicate/index.html#neetcode-solution",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\nThe following code makes it clear that we are using a hashset to store the values. It does not create the entire set all at once (like we do above, always giving O(n) time complexity as it runs through the whole list). Instead, it goes step by step; thus only in the worst case would this be O(n). We must create the hashset, which in the worst case uses O(n) space.\n\nclass Solution:\n    def hasDuplicate(self, nums: List[int]) -&gt; bool:\n        hashset = set()\n        for n in nums:\n            if n in hashset:\n                return True\n            hashset.add(n)\n        return False\n\n\nsolution = Solution()\nprint(solution.hasDuplicate(nums1))\nprint(solution.hasDuplicate(nums2))\n\nConclusion:\nAn easy start to NeetCode problems. The solution has a worst-case complexity of:\n\nTime complexity: O(n)\nSpace complexity: O(n)"
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html",
    "title": "LeetCode 49: Group Anagrams",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of strings strs, group all anagrams together into sublists. You may return the output in any order.\nExample 1:\nInput: strs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\nOutput: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\nExample 2:\nInput: strs = [\"x\"]\nOutput: [[\"x\"]]\nExample 3:\nInput: strs = [\"\"]\nOutput: [[\"\"]]"
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html#problem-description",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html#problem-description",
    "title": "LeetCode 49: Group Anagrams",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of strings strs, group all anagrams together into sublists. You may return the output in any order.\nExample 1:\nInput: strs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\nOutput: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\nExample 2:\nInput: strs = [\"x\"]\nOutput: [[\"x\"]]\nExample 3:\nInput: strs = [\"\"]\nOutput: [[\"\"]]"
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html#initial-solution",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html#initial-solution",
    "title": "LeetCode 49: Group Anagrams",
    "section": "Initial Solution",
    "text": "Initial Solution\nThe key to this problem was to recognize that when anagrams are sorted alphabetically, they become the same ‘word’. With that in mind, a decent solution seemed relatively easy to come up with. Let’s just sort the words and place the original word in a dictionary with the keys being the unique sorted/garbled word.\nThere are a few parts that could potentially trip up a beginner:\nFirstly, calling sorted() on a string returns a list of the sorted characters and not a whole sorted word:\nword = 'hello'\nsorted_word = sorted(word)\nprint(sorted_word)\n&gt;&gt;&gt; ['e', 'h', 'l', 'l', 'o']\nSo we must join() the list back together afterwards.\nSecondly, adding values to a pre-existing key in the dictionary, instead of overwriting them, takes a little care, but there are a few options noted in this post. For example, using append() and extend(). The TLDR is to use extend() when you have multiple values to append, rather than using append() multiple times. E.g.\na = {}\na.setdefault('abc', []).append(1)       # {'abc': [1]}\na.setdefault('abc', []).extend([2, 3])  # a is now {'abc': [1, 2, 3]}\nsetdefault() will avoid a KeyError when we try to access a key that does not exist yet. It inserts the key with the specified default: []\nLet’s put this all together in our initial attempt at solving the anagram problem.\n\nimport logging\nfrom typing import List\n\n\nclass InitialSolution:\n    def __init__(self):\n        # Set up logging config\n        logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n        self.logger = logging.getLogger(__name__)\n\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        # works fine without the if statement, but let's save some later computation:\n        if strs == [\"\"]:\n            return [[\"\"]]\n        \n        words = {}\n        for word in strs:\n            self.logger.debug(f\"Processing word: {word}\")\n            sorted_word = ''.join(sorted(word))  # n⋅log(n) character sorting * m strings in the list\n            self.logger.debug(f\"Sorted 'word': {sorted_word}\")\n            words.setdefault(sorted_word, []).append(word)\n            self.logger.debug(f\"Updated dict value(s): '{sorted_word}': {words[sorted_word]}\")\n \n        return list(words.values())\n\n\n# Example for testing purposes.\nstrs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\n# Output: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\n# Note that output order does not matter.\n\nsolution = InitialSolution()\nsolution.groupAnagrams(strs)\n\nDEBUG: Processing word: act\nDEBUG: Sorted 'word': act\nDEBUG: Updated dict value(s): 'act': ['act']\nDEBUG: Processing word: pots\nDEBUG: Sorted 'word': opst\nDEBUG: Updated dict value(s): 'opst': ['pots']\nDEBUG: Processing word: tops\nDEBUG: Sorted 'word': opst\nDEBUG: Updated dict value(s): 'opst': ['pots', 'tops']\nDEBUG: Processing word: cat\nDEBUG: Sorted 'word': act\nDEBUG: Updated dict value(s): 'act': ['act', 'cat']\nDEBUG: Processing word: stop\nDEBUG: Sorted 'word': opst\nDEBUG: Updated dict value(s): 'opst': ['pots', 'tops', 'stop']\nDEBUG: Processing word: hat\nDEBUG: Sorted 'word': aht\nDEBUG: Updated dict value(s): 'aht': ['hat']\n\n\n[['act', 'cat'], ['pots', 'tops', 'stop'], ['hat']]\n\n\nInitial Results:\nIntialSolution passes NeetCode submission (if we comment out the logging).\n\nTime Complexity: O(m⋅n⋅logn) — due to sorting each string.\nSpace Complexity: O(m⋅n) — due to storing the strings in the dictionary keys and values."
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html#neetcode-solution",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html#neetcode-solution",
    "title": "LeetCode 49: Group Anagrams",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nfrom collections import defaultdict\n\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        result = defaultdict(list)\n\n        for s in strs:\n            count = [0] * 26 # one for each character in a-z\n            for c in s:\n                # use ord to get unicode value of the character (minus that of 'a' to zero out 'a'):\n                count[ord(c) - ord('a')] += 1 # update the count for the corresponding character\n            result[tuple(count)].append(s) # convert count list to tuple, as lists can't be dict keys.\n        return result.values()\n\n\nsolution = Solution()\nlist(solution.groupAnagrams(strs)) \n\n[['act', 'cat'], ['pots', 'tops', 'stop'], ['hat']]\n\n\nI used list() above to display the result nicely in the notebook. The code seems to work fine in NeetCode both with and without the conversion to a list; no TypeError is given.\nConclusion:\nNeetcode’s solution uses a hashmap (defaultdict) called result that stores lists of anagrams, where the key is a tuple representing the character counts of the strings. Note that a defaultdict never raises a KeyError, but instead provides a default value for a key that does not exist.\n\nTime complexity O(m⋅n) — where m is the total number of strings and n is the length of each string (multiplied by 26 possible letters)\nSpace Complexity: O(m⋅n) — due to the space needed for storing the strings in the defaultdict values.\n\nGiven an extremely long string, the above solution would be optimal: O(m⋅n⋅logn) vs. O(m⋅n⋅26)"
  },
  {
    "objectID": "dsa.html",
    "href": "dsa.html",
    "title": "LeetCode",
    "section": "",
    "text": "Data Structures & Algorithms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 49: Group Anagrams\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 217: Contains Duplicate\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\neasy\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 1: Two Integer Sum\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\neasy\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Data Structures & Algorithms\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nspa-dev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html",
    "title": "FastAI Part 2 Lesson 12",
    "section": "",
    "text": "A modified version of Jeremy’s notebook for Practical Deep Learning for Coders: Part 2 Lesson 12. Additional notes were added, together with new sections for the ‘homework’. This includes an implementation of K-Means Clustering from scratch.\nYou may find that the code has excessive comments and other beginner issues (e.g., print statements rather than logging). This workbook represents my personal notes on the course and is not really intended to be a (nicely composed) blog post or tutorial.\nThe original unmodified notebook is available on the FastAI courses GitHub site, specifically here:"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Create data",
    "text": "Create data\n\nn_clusters=6\nn_samples =250 # per cluster\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = torch.rand(n_clusters, 2) * 70 - 35 # creates uniform random variable between -35 and 35. \n# This will multiply each element by 70 to scale tensor from 0 to 70, then minus 35, resulting in a distribtion between -35 and 35.\n\n\ncentroids.shape\n\ntorch.Size([6, 2])\n\n\n\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\ndef sample(m): return MultivariateNormal(m, torch.diag(tensor([5.,5.]))).sample((n_samples,))\n# where m is the mean; here the covariance matrix is a diagonal matrix with 5 on the diagonals.\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None, title=None): \n    # modified Jeremy's function to add a title when specified.\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)\n    if title:\n        ax.set_title(title)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#mean-shift",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#mean-shift",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Mean shift",
    "text": "Mean shift\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\nSource: FastAI notebook\n\nmidp = data.mean(0) # mid-point\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\n\n\n\n\nDefinition of the gaussian kernel:\n\ndef gaussian(d, bw): return torch.exp(-0.5*((d/bw))**2) / (bw*math.sqrt(2*math.pi))\n# where d is distance and bw is the bandwidth\n\n\ndef plot_func(f):\n    x = torch.linspace(0,10,100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\n\n\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i # triangular weighting. It's a simpler function, we will compare with gaussian. \n\n\nplot_func(partial(tri, i=8))\n\n\n\n\n\n\n\n\n\nX = data.clone() # clone used, as we will be shifting the points towards the center\nx = data[0] # lowercase x - first invidual point; matrix is capital X\n\n\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape,X.shape,x[None].shape #  None adds an additional axis.\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\n(x[None]-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x-X)[:8] # This will work without the extra axis, because last axis matches and second last doesn't exist, so will be ok for broadcasting\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\nVideo paused for break at approx. 56:30 min\n\n# rewrite using torch.einsum\ndist = ((x-X)**2).sum(1).sqrt() # dist = euclidian distance\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#einsum-homework",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#einsum-homework",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Einsum Homework",
    "text": "Einsum Homework\ntorch.einsum example syntax (for batch matrix multiplication)\nNote lowercase b = batch size\nAs = torch.randn(3, 2, 5)\nBs = torch.randn(3, 5, 4)\ntorch.einsum('bij,bjk-&gt;bik', As, Bs)\n\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# Homework\ndif = x-X\ndist_homework = torch.einsum('ij,ij-&gt;i', dif, dif).sqrt()\ndist_homework[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\nLet’s check to see if the entire output of the homework function is that same as Jeremy’s code, not only the 8 values listed above.\n\nprint(\"Total number of values that are close:\", torch.isclose(dist, dist_homework).sum().item())\nprint(\"Are all values close?\", torch.isclose(dist, dist_homework).all().item()) # NB, there is an .allclose() we can use instead!\n\nTotal number of values that are close: 1500\nAre all values close? True\n\n\nContinue video at approx 1:04:30\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape,X.shape # shapes don't match\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\n\nweight[:,None].shape # add unit axis. We will want weights for each x and y value.\n\ntorch.Size([1500, 1])\n\n\n\nweight[:,None]*X # we can now multiply by X (then later divide by the sum of the weights to create weighted average)\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\n# Put the above into one function to perform one update step\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x-X)**2).sum(1))\n#         weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:,None]*X).sum(0)/weight.sum()\n\n\n# Let's try to avoid modifying X in place during the loop. \n# The following code works but messes up the animation (points no longer move).\ndef one_update_fixed(X):\n    result = torch.zeros_like(X) # initialize result\n    for i, x in enumerate(X):\n        dist = torch.sqrt( ((x-X)**2).sum(1) )\n#        weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        result[i] = (weight[:,None]*X).sum(0)/weight.sum()\n    return result \n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n%time X=meanshift(data)  # Note: %timeit is typically more accurate and can do multiple runs\n\nCPU times: user 242 ms, sys: 0 ns, total: 242 ms\nWall time: 242 ms\n\n\n\nplot_data(centroids+2, X, n_samples)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Animation",
    "text": "Animation\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\n\n# create your own animation\nX = data.clone()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#gpu-batched-algorithm",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#gpu-batched-algorithm",
    "title": "FastAI Part 2 Lesson 12",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\n# check GPU resources\nprint(\"GPU available?\", torch.cuda.is_available())\nprint(\"How many GPUs?\", torch.cuda.device_count())\ntorch.cuda.empty_cache() # clean cache\n\nGPU available? True\nHow many GPUs? 1\n\n\n\n#!nvidia-smi # show system management interface\n\n\nbs=5\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a,b): return (((a[None]-b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None,:]-x[:,None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[...,None].shape, X[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\n\nnum = (weight[...,None]*X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ntorch.einsum('ij,jk-&gt;ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs,n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n#             weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight@X/div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\n%timeit -n 5 _=meanshift(data, 1250).cpu()\n\n912 µs ± 48.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\n\n\n\n\nVideo @ approx 1:34\nJeremy’s Homework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.\nBonus: Implement it in APL too!\nSuper bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.\nSuper super bonus: Publish a paper that describes it :D"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#introduction-to-k-means-clustering",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#introduction-to-k-means-clustering",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Introduction to K-Means Clustering",
    "text": "Introduction to K-Means Clustering\nSource: wikipedia. Apologies for the screenshots. It was the quickest way to show this.\n\n\n\n\n\n\nFootnotes\n\nPelleg, Dan; Moore, Andrew. “Accelerating exact k-means algorithms with geometric reasoning”. Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM Press, 1999, pp. 277–281.\nMacKay, David. “Information Theory, Inference and Learning Algorithms”. Cambridge University Press, 2003, pp. 284–292.\nSince the square root is a monotone function, this also is the minimum Euclidean distance assignment.\nHartigan, J. A.; Wong, M. A. “Algorithm AS 136: A k-Means Clustering Algorithm”. Journal of the Royal Statistical Society, Series C, 1979, vol. 28, no. 1, pp. 100–108.\n\n\n\n\nDefinitions:\nCluster: A cluster is a collection of data points that are grouped together because they share certain similarities, often in terms of their proximity to each other. In k-means clustering, each data point in the dataset is assigned to one of the k clusters based on its distance to the centroids. A cluster is essentially a subset of the entire dataset.\nCentroid: A centroid is a representative point of a cluster. It is the “center” of the cluster and is typically calculated as the mean (average) of all the points within that cluster. In each iteration of the k-means algorithm, the centroids are updated to be the new centers of their respective clusters based on the current assignments of points to clusters.\n\n# Import type hints for code readability and documentation\nfrom typing import List, Tuple\nfrom torch import Tensor"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-initial-implementation",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-initial-implementation",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Clustering: Initial Implementation",
    "text": "K-Means Clustering: Initial Implementation\n\nInitialize Centroids Using the Forgy Method\nIn this method, the centroids are randomly chosen from the actual data points. The function below essentially shuffles the indices of the points (random permutation) then picks the first k points. Note, there is no equivalent to random.sample in PyTorch.\n\ndef initialize_centroids(k, points):\n    \"\"\"\n    Initializes centroids using the Forgy method.\n\n    Parameters:\n    k (int): The number of centroids to initialize.\n    points (torch.Tensor): A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n\n    Returns:\n    torch.Tensor: A tensor of shape (k, d) containing the initial centroids.\n    \"\"\"\n    indx = torch.randperm(len(points))[:k]   \n    return points[indx]\n\n# we could also try centroids = torch.randn(k, points.shape[1])\n\n\n# Put data back on CPU for now:\ndata = data.cpu()\n\n\n# Test to see if the Forgy method is working. The centroids appear to be correctly initialized.\nk = 6\nrandom_centroids = initialize_centroids(k, data)\nplot_data(random_centroids, data, n_samples, title='Random (Forgy) Initialized Centroids')\n\n\n\n\n\n\n\n\n\n\nK-Means Clustering Algorithm Version 1.0\n\ndef distance(A, B): # Euclidian distance\n    #return ((A-B)**2).sum(1).sqrt() # works fine. \n    return torch.norm(A - B, dim=1) # per docs, this may be deprecated in future\n\n\ndef k_means_cluster_v1(k, points, max_iters=20, tol=1e-08):\n    # Initialize centroids: choose k centroids (Forgy, Random Partition, etc.)\n    centroids = initialize_centroids(k, points)  # returns a tensor of shape k,d\n    \n    for iteration in range(max_iters):\n        clusters = [[] for _ in range(k)] # initilize clusters \n              \n        # Assign each point to the nearest centroid. The index of the points is stored in the 'clusters' list.\n        for i, point in enumerate(points): # We may attempt to vectorize this loop later, using the batched distance function.\n            distances_to_each_centroid = distance(point, centroids)\n            cluster_assignment = torch.argmin(distances_to_each_centroid) # argmin returns the index of the min value (closest centroid)\n            clusters[cluster_assignment].append(i)\n        \n        # Calculate new centroids\n        new_centroids = torch.zeros_like(centroids)\n        for idx, cluster in enumerate(clusters):\n            if cluster:  # Avoid division by zero\n                new_centroids[idx] = torch.mean(points[cluster], dim=0) # calculate new centroid as the mean of the cluster\n            else:  # If a cluster is empty, re-initialize a single centroid for the cluster\n                new_centroids[idx] = initialize_centroids(1, points)[0]\n        \n        # Test convergence\n        if torch.allclose(new_centroids, centroids, atol=tol):  # default absolute tolerance for isclose is 1e-08 \n            # print(f\"Converged after {iteration + 1} iterations!\")\n            return clusters, new_centroids\n        \n        centroids = new_centroids\n    \n    # print(f\"Reached maximum number of iterations: {max_iters}\")\n    return clusters, centroids # clusters returned for debugging purposes; can remove.\n\n\nk = 6\nclustering_test, final_centroids = k_means_cluster_v1(k, data)\n\n# these tensors must be on .cpu() for plotting purposes\nplot_data(final_centroids.cpu(), data.cpu(), n_samples, title='K-Means Clustering With Multiple Loops')\n\n\n\n\n\n\n\n\n\n%timeit -n 5 _ = k_means_cluster_v1(k, data)\n# Slow, typically at 200 to 300 ms per loop.\n\n83.8 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation-of-k-means",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation-of-k-means",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Animation of K-Means",
    "text": "Animation of K-Means\nUses a modification of our basic function that creates a list to store the history of the centroids and clusters at each step.\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\n# Function to plot data and centroids\ndef plot_data_k_means(ax, points, clusters, centroids):\n    ax.clear()\n    colors = ['r', 'y', 'g', 'b', 'c', 'm']  # Extend if needed\n    for idx, cluster in enumerate(clusters):\n        cluster_points = points[cluster]\n        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[idx], s=1, label=f'Cluster {idx+1}')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=20, label='Centroid')\n    ax.legend()\n    ax.set_title('K-Means Clustering Animation')\n    ax.set_xlabel('Dimension 1')\n    ax.set_ylabel('Dimension 2')\n\n\n# Modified k-means clustering to capture each iteration's state\ndef k_means_cluster_historic(k, points, max_iters=20):\n    centroids = initialize_centroids(k, points)\n    history = []\n\n    for iteration in range(max_iters):\n        clusters = [[] for _ in range(k)]\n        \n        for i, point in enumerate(points):\n            distances_to_each_centroid = distance(point, centroids)\n            cluster_assignment = torch.argmin(distances_to_each_centroid).item()\n            clusters[cluster_assignment].append(i)\n        \n        new_centroids = torch.zeros_like(centroids)\n        for idx, cluster in enumerate(clusters):\n            if cluster:\n                new_centroids[idx] = torch.mean(points[cluster], dim=0)\n            else:\n                new_centroids[idx] = initialize_centroids(1, points)[0]\n        \n        history.append((clusters, centroids)) \n        # Is tensor mutated in place? If so we should use centroids.clone() to store its state. \n        # Safer to use clusters.copy() too? Although these seem to work fine without.\n        \n        if torch.all(torch.isclose(new_centroids, centroids, atol=1e-08)):\n            print(f\"Converged after {iteration + 1} iterations!\")\n            break\n        \n        centroids = new_centroids\n\n    history.append((clusters, centroids.clone())) #may be redundant?\n    return clusters, centroids, history\n\n\n# Function to animate k-means clustering\ndef animate_k_means(k, points, max_iters=20, interval=500):\n    _, _, history = k_means_cluster_historic(k, points, max_iters)\n    fig, ax = plt.subplots()\n    \n    def update(frame):\n        clusters, centroids = history[frame]\n        plot_data_k_means(ax, points, clusters, centroids)\n    \n    ani = FuncAnimation(fig, update, frames=len(history), interval=interval, repeat=False)\n    plt.close()\n    return HTML(ani.to_jshtml())\n\n\n# Put data back on CPU for now (plotting function gives an error on GPU):\ndata = data.cpu()\n\n# Display the animation\nanimation = animate_k_means(k, data, max_iters=20, interval=500)\nanimation\n\nConverged after 8 iterations!\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#vectorizing-gpu-batched-k-means-algorithm",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#vectorizing-gpu-batched-k-means-algorithm",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Vectorizing GPU Batched K-Means Algorithm",
    "text": "Vectorizing GPU Batched K-Means Algorithm\nNote to reader: Skip this part. I reproduced a little of Jeremy’s work, vectorized (enabled batch processing) the calculation of distances, but ultimately moved on.\nNote: Jeremy’s plotting function only works on the CPU.\n\n# Let's reproduce Jeremy's work above:\nbs=5\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a,b): return (((a[None]-b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(x, X).shape\n\ntorch.Size([1500, 5])\n\n\nLets see if we can vectorize the following loop in our K-means algorithm:\nclusters = [[] for _ in range(k)] # initilize clusters \n      \nfor i, point in enumerate(points): \n    distances_to_each_centroid = distance(point, centroids)\n    cluster_assignment = torch.argmin(distances_to_each_centroid)\n    clusters[cluster_assignment].append(i)\n\n\n# First, we create a very simple list of points, with pre-defined centroids, so we can easily view results.\nsimple_points = torch.tensor([[1., 2.], [1., 4.], [1., 0.], [10., 2.], [10., 4.], [10., 0.]])\nsimple_centroids = torch.tensor([[10.5,  2.], [ 1.5,  2.]]) # x-offset by 0.5 for plotting\n\n\nsimple_points.shape, simple_centroids.shape\n\n(torch.Size([6, 2]), torch.Size([2, 2]))\n\n\n\n# Let's create a batch. Here of size 3 (points 0, 1, and 2).\nbatched_points = simple_points[:3]\n\n\nbatched_points.shape\n\ntorch.Size([3, 2])\n\n\nThe batched_points tensor has a shape of [3,2], i.e., three (x,y) points. If we try to use our current distance function to calculate the distance to the centroids of shape [2,2], i.e., two centroids located at (x,y), we will receive an error, specifically:\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0.\nLet’s modify the inputs to the distance function to ensure the sizes match on the appropriate dimensions.\n\n# Testing to create an additional dimension, using None (equivalent to np.newaxis in NumPy).\n#Alternatively, use torch.unsqueeze_(index_of_new_dimension).\nprint(batched_points[None].shape)\nprint(simple_centroids[:,None].shape) # None at index 1, to align the last dimensions correctly; [:,:,None] would be incorrect\n# Shows we are nicely aligned at the last dimension [2]\n\ntorch.Size([1, 3, 2])\ntorch.Size([2, 1, 2])\n\n\n\ndef dist_batch(points, centroids): # Euclidian distance\n    return (((points[None]) - (centroids[:,None]))**2).sum(2).sqrt() # sum over the last dimension (index [2])\n    #return torch.norm(points[None] - centroids[:,None], dim=2) \n    # Einsum practice:\n    #dif = points[None] - centroids[:,None]\n    #return torch.einsum('ijk,ijk-&gt;ij', dif, dif).sqrt()\n    \ndist_batch(batched_points, simple_centroids)\n\ntensor([[9.500, 9.708, 9.708],\n        [0.500, 2.062, 2.062]])\n\n\n\n# We will revise the following loop to incorporate batches.\n\nclusters = [[] for _ in range(len(simple_centroids))]  # Initialize clusters list\n\nfor i, point in enumerate(simple_points):\n    distances_to_each_centroid = distance(point, simple_centroids)\n    cluster_assignment = torch.argmin(distances_to_each_centroid)\n    clusters[cluster_assignment].append(i)\n\nprint(clusters)\n\n[[3, 4, 5], [0, 1, 2]]\n\n\n\nTesting Assignment of Points to Clusters (in the For loop)\nLet’s test out our new code for running batches of points\n\ntorch.set_default_device('cpu') # for plotting \n\n# Test of the clusters list\nsimple_points = torch.tensor([[1., 2.], [1., 4.], [1., 0.], [10., 2.], [10., 4.], [10., 0.]])\nsimple_centroids = torch.tensor([[10.5,  2.], [ 1.5,  2.]]) # x-offset by 0.5 for plotting\n\ndef distance(A, B):\n    return ((A-B)**2).sum(1).sqrt() \n\n# Assign points to clusters based on distances to centroids\nclusters = [[] for _ in range(len(simple_centroids))]  # Initialize clusters list\n\nfor i, point in enumerate(simple_points):\n    distances_to_each_centroid = distance(point, simple_centroids)\n    cluster_assignment = torch.argmin(distances_to_each_centroid)\n    clusters[cluster_assignment].append(i)\n\n# Plotting\ncolors = ['r', 'g', 'b', 'c', 'm', 'y']  \nfor cluster_idx, cluster in enumerate(clusters):\n    cluster_points = simple_points[cluster]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[cluster_idx], label=f'Cluster {cluster_idx}')\n    \n# Plot centroids\nplt.scatter(simple_centroids[:, 0], simple_centroids[:, 1], c='black', marker='X', s=100, label='Centroids (x-offset by 0.5 units)')\n\n# Plot configuration\nplt.xlabel('X coordinate')\nplt.ylabel('Y coordinate')\nplt.title('Simple Example with Offset Centroids', fontsize=10)\nplt.suptitle('K-Means Clustering',fontsize=12) # Actually a title. Matplotlib places the subtitle above the title!\nplt.legend()\nplt.show()\n\nprint('Cluster assignment for two centroids=', clusters)\n\n\n\n\n\n\n\n\nCluster assignment for two centroids= [[3, 4, 5], [0, 1, 2]]\n\n\n\n\nRunning batches through the (vectorized) loop that calculates distances\nSuccessfully completed. The two outputs match: [[3, 4, 5], [0, 1, 2]].\n\ntorch.set_default_device('cuda')\ndata = data.cuda()\n\nk = len(simple_centroids) # k=2 for testing purposes (2 pre-specified centroids)\nbs = 3\npoints = simple_points\nn = len(points)\n# add the following within a loop of max iters:\n\nclusters = [[] for _ in range(k)]  # Initialize clusters list\nprint('empty clusters=', clusters)\nfor i in range (0, n, bs):\n    s = slice(i, min(i+bs, n))\n    print(s)\n    distances_to_each_centroid = dist_batch(points[s], simple_centroids)\n    print(distances_to_each_centroid)\n    cluster_assignment = torch.argmin(distances_to_each_centroid, dim=0, keepdim=False) # bug here? dim = 1? Investigate.\n    print(cluster_assignment)\n    # OK, but now we are stuck with a For loop for cluster assignment.    \n    for j, cluster_idx in enumerate(cluster_assignment):\n        clusters[cluster_idx].append(i + j)\n    print('completed iteration')\nprint('clusters variable =', clusters)\n\nempty clusters= [[], []]\nslice(0, 3, None)\ntensor([[9.500, 9.708, 9.708],\n        [0.500, 2.062, 2.062]])\ntensor([1, 1, 1])\ncompleted iteration\nslice(3, 6, None)\ntensor([[0.500, 2.062, 2.062],\n        [8.500, 8.732, 8.732]])\ntensor([0, 0, 0])\ncompleted iteration\nclusters variable = [[3, 4, 5], [0, 1, 2]]\n\n\nOK, so the outputs are the same. We have (possibly) successfully batched the ‘distance’ calculations. However, we still have to loop through and do cluster assignment, and then calculate the means of the clusters (i.e., create centroids). To tackle the cluster assignment, we could store a list of cluster assignments outside the loop, then perform that after all batches are done. However, I’ll take step back and see if there is a different approach. I think the code would become over-complicated if I were to continue with this line of thinking."
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-2.0",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-2.0",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Version 2.0",
    "text": "K-Means Version 2.0\nI asked ChatGPT to see if it could improve my code. For whatever reason, it took my batches out and came up with the following code (after some debugging, mainly related to variable type and dimension mismatches).\n\ntorch.set_default_device('cuda')\ngpu_data = data.clone().cuda()\n\n\ndef dist_batch(points, centroids):\n    # Calculate the distance from each point to each centroid\n    return torch.cdist(points, centroids) # L2 norm by default (p=2.0) \n\ndef k_means_cluster_v2(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)  # returns a tensor of shape k,d\n   \n    for i in range(max_iters):\n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n\n        # Debugging: Print shapes of relevant tensors\n        #print(f\"Shapes: cluster_assignments={cluster_assignments.shape}, points={points.shape}\")\n\n        cluster_sums = torch.zeros_like(centroids)\n        cluster_counts = torch.zeros(k, dtype=torch.int32)\n        \n        # Debugging: Print cluster_assignments to check its values\n        #print(\"Cluster Assignments:\", cluster_assignments)\n\n        for idx in range(k):\n            assigned_points = points[cluster_assignments == idx]\n            if len(assigned_points) &gt; 0:\n                cluster_sums[idx] = assigned_points.sum(dim=0)\n                cluster_counts[idx] = assigned_points.shape[0]\n\n        # Avoid division by zero\n        mask = cluster_counts &gt; 0\n        new_centroids = torch.where(mask.unsqueeze(1), cluster_sums / cluster_counts.unsqueeze(1), centroids)\n\n        if torch.allclose(new_centroids, centroids, atol=tol):\n            #print(f\"Converged after {i + 1} iterations!\")\n            break\n\n        centroids = new_centroids\n    \n    #else:\n        #print(f\"Reached maximum number of iterations: {max_iters}\")\n\n    return centroids\n\nfinal_centroids = k_means_cluster_v2(6, gpu_data)\nplot_data(final_centroids.cpu(), data.cpu(), n_samples)\n\n\n\n\n\n\n\n\n\n%timeit -n 5 _ = k_means_cluster_v2(k, gpu_data)\n\n971 µs ± 167 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nThis version is much quicker (on GPU at approx. 1-2 ms vs. 200-300 ms above). Note that the speeds often seem to depend on how many iterations are required for convergence. For a better test, we should take out the need for convergence and loop for a defined number of times (perhaps 5).\nFor fun, I asked ChatGPT if it could remove the loop over the range of k. Not sure if that is worthwhile, since k is usually low (6 in the example here), although the list of data points can be long. In any case, I was interested to learn a few more PyTorch functions."
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-3.0-an-implementation-using-torch.scatter_",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-3.0-an-implementation-using-torch.scatter_",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Version 3.0: An implementation using torch.scatter_()",
    "text": "K-Means Version 3.0: An implementation using torch.scatter_()\nThe first algorithm it came up with suggested using torch.scatter_(). Further explanation on this function in the cells below. After fixing a few issues, we had a working version:\n\n# Removed a For loop and replaced with .scatter_ function\n\ndef dist_batch(points, centroids):\n    # Calculate the distance from each point to each centroid\n    return torch.cdist(points, centroids) # L2 norm by default (p=2.0) \n\ndef k_means_cluster_v3(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)  # returns a tensor of shape (k, d)\n\n    for _ in range(max_iters): # replace _ with i if not using print statements below\n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n\n        # Debugging: Print shapes of relevant tensors\n        #print(f\"Shapes: cluster_assignments={cluster_assignments.shape}, points={points.shape}\")\n\n        # Create tensors to hold the sums of points and counts of points per cluster\n        cluster_sums = torch.zeros_like(centroids)\n        cluster_counts = torch.zeros(k, dtype=torch.int32)\n        \n        # Debugging: Print cluster_assignments to check its values\n        #print(\"Cluster Assignments:\", cluster_assignments)\n\n        # Use scatter_add_ to sum points and count points per cluster\n        cluster_sums.scatter_add_(0, cluster_assignments.unsqueeze(1).expand_as(points), points)\n        cluster_counts.scatter_add_(0, cluster_assignments, torch.ones_like(cluster_assignments, dtype=torch.int32))\n\n        # Equivalent code with scatter_reduce # However, no reason to use 'sum' when we have an option for 'mean'. See Version 3.1.\n        #cluster_sums.scatter_reduce_(0, cluster_assignments.unsqueeze(1).expand_as(points), points, reduce=\"sum\")\n        #cluster_counts.scatter_reduce_(0, cluster_assignments, torch.ones_like(cluster_assignments, dtype=torch.int32), reduce=\"sum\")\n\n        # Original For loop:\n        #for idx in range(k):\n        #    assigned_points = points[cluster_assignments == idx]\n        #    if len(assigned_points) &gt; 0:\n        #        cluster_sums[idx] = assigned_points.sum(dim=0)\n        #        cluster_counts[idx] = assigned_points.shape[0]\n        \n        # Avoid division by zero \n        mask = cluster_counts &gt; 0\n        # Calculate new centroid as the mean of each cluster\n        new_centroids = torch.where(mask.unsqueeze(1), cluster_sums / cluster_counts.unsqueeze(1), centroids)\n        \n        if torch.allclose(new_centroids, centroids, atol=tol):\n            #print(f\"Converged after {i + 1} iterations!\")\n            break\n\n        centroids = new_centroids\n\n    #else:\n        #print(f\"Reached maximum number of iterations: {max_iters}\")\n\n    return centroids\n\nfinal_centroids = k_means_cluster_v3(6, gpu_data)\nplot_data(final_centroids.cpu(), data.cpu(), n_samples)\n\n\n\n\n\n\n\n\n\ntorch.set_default_device('cuda')\n%timeit -n 5 _ = k_means_cluster_v3(k, data.cuda())\n\n769 µs ± 99.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\ntorch.set_default_device('cpu')\n%timeit -n 5 _ = k_means_cluster_v3(k, data.cpu())\n\n554 µs ± 87.7 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nInteresting. This version is actually slower on GPU than CPU. I suspect it’s copying a lot of stuff back and forth, or I’m simply doing something wrong here!\nExplaining the code above\nChatGTP provided the following explanation of the initial changes it made:\n\nCluster Sums: scatter_add_ is used to sum points assigned to each cluster.\n\ncluster_sums.scatter_add_(0, cluster_assignments.unsqueeze(1).expand_as(points), points)\n\ncluster_assignments.unsqueeze(1).expand_as(points) creates an expanded index tensor to match the dimensions of points.\nscatter_add_ adds the points to the corresponding clusters based on cluster_assignments.\n\n\nCluster Counts: scatter_add_ is also used to count points in each cluster.\n\ncluster_counts.scatter_add_(0, cluster_assignments, torch.ones_like(cluster_assignments, dtype=torch.int32))\n\nHere, we scatter ones into cluster_counts to count the number of points assigned to each cluster.\n\nTo understand torch.scatter_() and related functions, I suggest the following blog post. The PyTorch documentation is somewhat difficult to understand.\nhttps://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\nAfter reviewing the documentation, I noticed that scatter_reduce has an option to return the mean. Instead of performing sums / counts to create the average, let’s try do this directly with scatter_reduce.\nFrom the documentation:\nTensor.scatter_reduce_(dim, index, src, reduce, *, include_self=True) → Tensor\nReduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (“sum”, “prod”, “mean”, “amax”, “amin”). For each value in src, it is reduced to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. If include_self=“True”, the values in the self tensor are included in the reduction.\n\n# Using the scatter_reduce 'mean' argument\n\ndef k_means_cluster_v3_1(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)\n\n    for _ in range(max_iters):  \n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n        cluster_means = torch.zeros_like(centroids)\n        cluster_means.scatter_reduce_(0, cluster_assignments.unsqueeze(1).expand_as(points), points, reduce=\"mean\")\n\n        if torch.allclose(cluster_means, centroids, atol=tol):\n            break\n\n        centroids = cluster_means\n        \n    return centroids\n\ntorch.set_default_device('cuda')\nfinal_centroids = k_means_cluster_v3_1(6, gpu_data)\nplot_data(final_centroids.cpu(), data.cpu(), n_samples)\n\n\n\n\n\n\n\n\n\ntorch.set_default_device('cuda')\n%timeit -n 5 _ = k_means_cluster_v3_1(k, data.cuda())\n\n669 µs ± 147 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\ntorch.set_default_device('cpu')\n%timeit -n 5 _ = k_means_cluster_v3_1(k, data.cpu())\n\n459 µs ± 45.3 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nWell, that is about the same, maybe slightly quicker, and the code looks cleaner too."
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data-with-xy-and-z-axis",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data-with-xy-and-z-axis",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Create data with x,y, and z axis",
    "text": "Create data with x,y, and z axis\n\nn_xyz_clusters=6\nn_samples_per_cluster =250\n\nxyz_centroids = torch.rand(n_xyz_clusters, 3)*70-35 # creates uniform random variable between -35 and 35.\nxyz_centroids.shape\n\ntorch.Size([6, 3])\n\n\n\ndef sample_from_distribution(mean: Tensor, n_samples: int) -&gt; Tensor:\n    \"\"\"\n    Generates samples from a multivariate normal distribution.\n\n    Parameters:\n        mean (Tensor): A tensor of shape 3 representing a mean vector\n        n_samples (int): The number of samples to generate.\n\n    Returns:\n        Tensor: A tensor of shape (n_samples, 3) containing the generated samples.\n    \"\"\"\n    # Create a 3x3 diagonal covariance matrix with variances [5., 5., 5.]\n    cov_matrix = torch.diag(torch.tensor([5., 5., 5.]))\n    \n    # Create a multivariate normal distribution with the given mean vector and covariance matrix\n    distribution = MultivariateNormal(mean, cov_matrix)\n    \n    # Generate and return samples from the distribution\n    return distribution.sample((n_samples,))\n\n\nnew_slices = [sample_from_distribution(c, n_samples_per_cluster) for c in xyz_centroids]\nxyz_data = torch.cat(new_slices)\nxyz_data.shape\n\ntorch.Size([1500, 3])"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#plot-3d-data",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#plot-3d-data",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Plot 3D data",
    "text": "Plot 3D data\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d_data(centroids, data, n_samples, ax=None, title=None): \n    \"\"\"\n    Plot 3D data points and centroids.\n    \"\"\"\n    if ax is None:\n        fig = plt.figure(figsize=(10, 8))\n        ax = fig.add_subplot(111, projection='3d')\n    \n    for i, centroid in enumerate(centroids):\n        samples = data[i * n_samples:(i + 1) * n_samples]\n        ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2], s=1)\n    \n    for centroid in centroids:\n        ax.scatter(*centroid, s=100, marker=\"x\", color='k', linewidths=3, zorder=10)  # attempt to plot centroids on top; not well.\n        # Note that *centroid unpacks the 3D coordinate (i.e., x, y, z) from the iterable\n    if title:\n        ax.set_title(title)\n\nplot_3d_data(xyz_centroids.cpu(), xyz_data.cpu(), n_samples_per_cluster, title=\"3D Data Clustering\")"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-in-3d",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-in-3d",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Clustering in 3D",
    "text": "K-Means Clustering in 3D\n\ntest_data = xyz_data.clone()\n\n\ndef initialize_centroids(k, points):\n    # See documentation above. Forgy method of initialization.\n    indx = torch.randperm(len(points))[:k]   \n    return points[indx]\n\n\ndef dist_batch(points, centroids):\n    # Same defined function defined above.\n    return torch.cdist(points, centroids) # L2 norm by default (p=2.0) \n\n\n# Same function as version 3.1 above. Copied here in case modification is needed.\ndef k_means_cluster(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)\n\n    for _ in range(max_iters):  \n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n        cluster_means = torch.zeros_like(centroids)\n        cluster_means.scatter_reduce_(0, cluster_assignments.unsqueeze(1).expand_as(points), points, reduce=\"mean\")\n\n        if torch.allclose(cluster_means, centroids, atol=tol):\n            break\n\n        centroids = cluster_means\n    return centroids\n\n\nfinal_3D_centroids = k_means_cluster(6, test_data); final_3D_centroids\n\ntensor([[ 14.190,   2.650, -30.780],\n        [ 24.511, -13.043,  20.713],\n        [ 21.041, -20.131, -34.454],\n        [-27.653,   9.463, -16.193],\n        [ -8.283, -13.172, -33.587],\n        [-12.459, -18.228, -21.804]])\n\n\n\nplot_3d_data(final_3D_centroids, xyz_data, n_samples_per_cluster, title=\"3D Data Clustering\")"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#results",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#results",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Results",
    "text": "Results\nThe 3D clustering appears to be successful, without any modifications to the k-means function. I’m actually surprised it worked without giving me a dimension mismatch error, or at least it seems to work.\nNote that the code has not been tested much. It would really benefit from some checks."
  }
]