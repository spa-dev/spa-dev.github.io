[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website. Work in Progress."
  },
  {
    "objectID": "dsa.html",
    "href": "dsa.html",
    "title": "LeetCode",
    "section": "",
    "text": "Data Structures & Algorithms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 15: Three Sum\n\n\n\n\n\n\ncode\n\n\ntwo pointers\n\n\nmedium\n\n\n\n\n\n\n\n\n\nMar 9, 2025\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 167: Two Sum II - Input Array Is Sorted\n\n\n\n\n\n\ncode\n\n\ntwo pointers\n\n\nmedium\n\n\n\n\n\n\n\n\n\nAug 26, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 125: Valid Palindrome\n\n\n\n\n\n\ncode\n\n\ntwo pointers\n\n\nlogging\n\n\neasy\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 128: Longest Consecutive Sequence\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 36: Valid Sudoku\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 238: Product of Array Except Self\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 271: String Encode and Decode\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 347: Top K Frequent Elements\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 49: Group Anagrams\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\nmedium\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 217: Contains Duplicate\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\neasy\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nLeetCode 1: Two Integer Sum\n\n\n\n\n\n\ncode\n\n\narrays and hashing\n\n\neasy\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Data Structures & Algorithms\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nspa-dev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html",
    "title": "FastAI Part 2 Lesson 12",
    "section": "",
    "text": "A modified version of Jeremy’s notebook for Practical Deep Learning for Coders: Part 2 Lesson 12. Additional notes were added, together with new sections for the ‘homework’. This includes an implementation of K-Means Clustering from scratch.\nYou may find that the code has excessive comments and other beginner issues (e.g., print statements rather than logging). This workbook represents my personal notes on the course and is not really intended to be a (nicely composed) blog post or tutorial.\nThe original unmodified notebook is available on the FastAI courses GitHub site, specifically here:"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Create data",
    "text": "Create data\n\nn_clusters = 6\nn_samples = 250  # per cluster\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = (torch.rand(n_clusters, 2) * 70 - 35)  \n# Creates uniform random variable between -35 and 35.\n# This will multiply each element by 70 to scale the \n# tensor from 0 to 70, then minus 35, resulting in a \n# distribtion between -35 and 35.\n\n\ncentroids.shape\n\ntorch.Size([6, 2])\n\n\n\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\nfrom torch import tensor\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\ndef sample(m):\n    return MultivariateNormal(m, torch.diag(tensor([5.0, 5.0]))).sample((n_samples,))\n\n# where m is the mean; \n# here, the covariance matrix is a diagonal matrix with 5 on the diagonals.\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None, title=None):\n    # modified Jeremy's function to add a title when specified.\n    if ax is None:\n        _, ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i * n_samples : (i + 1) * n_samples]\n        ax.scatter(samples[:, 0], samples[:, 1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color=\"k\", mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color=\"m\", mew=2)\n    if title:\n        ax.set_title(title)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#mean-shift",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#mean-shift",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Mean shift",
    "text": "Mean shift\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\nSource: FastAI notebook\n\nmidp = data.mean(0)  # mid-point\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp] * 6, data, n_samples)\n\n\n\n\n\n\n\n\nDefinition of the gaussian kernel:\n\ndef gaussian(d, bw):\n    return torch.exp(-0.5 * ((d / bw)) ** 2) / (bw * math.sqrt(2 * math.pi))\n\n# where d is distance and bw is the bandwidth\n\n\ndef plot_func(f):\n    x = torch.linspace(0, 10, 100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\n\n\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i):\n    return (-d + i).clamp_min(0) / i  \n\n# triangular weighting. It's a simpler function, we will compare with gaussian.\n\n\nplot_func(partial(tri, i=8))\n\n\n\n\n\n\n\n\n\nX = data.clone()  # clone; as we will shift the points towards the center\nx = data[0]  # lowercase x - first invidual point; matrix is capital X\n\n\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape, X.shape, x[None].shape  #  None adds an additional axis.\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\n(x[None] - X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x - X)[:8]  \n\n# This will work without the extra axis, because \n# last axis matches and second last doesn't exist, \n# so will be ok for broadcasting\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\nVideo paused for break at approx. 56:30 min\n\n# rewrite using torch.einsum\ndist = ((x - X) ** 2).sum(1).sqrt()  # dist = euclidian distance\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#einsum-homework",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#einsum-homework",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Einsum Homework",
    "text": "Einsum Homework\ntorch.einsum example syntax (for batch matrix multiplication)\nNote lowercase b = batch size\nAs = torch.randn(3, 2, 5)\nBs = torch.randn(3, 5, 4)\ntorch.einsum('bij,bjk-&gt;bik', As, Bs)\n\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# Homework\ndif = x - X\ndist_homework = torch.einsum(\"ij,ij-&gt;i\", dif, dif).sqrt()\ndist_homework[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\nLet’s check to see if the entire output of the homework function is that same as Jeremy’s code, not only the 8 values listed above.\n\nprint(\n    \"Total number of values that are close:\",\n    torch.isclose(dist, dist_homework).sum().item(),\n)\nprint(\n    \"Are all values close?\", torch.isclose(dist, dist_homework).all().item()\n)  \n\n# NB, there is an .allclose() we can use instead!\n\nTotal number of values that are close: 1500\nAre all values close? True\n\n\nContinue video at approx 1:04:30\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape, X.shape  # shapes don't match\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\n\n# add unit axis. We will want weights for each x and y value.\nweight[:, None].shape  \n\ntorch.Size([1500, 1])\n\n\n\nweight[:, None] * X  \n# we can now multiply by X\n# then later divide by sum of the weights to create weighted average\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\n# Put the above into one function to perform one update step\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x - X) ** 2).sum(1))\n        #  weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:, None] * X).sum(0) / weight.sum()\n\n\n# Let's try to avoid modifying X in place during the loop.\n# The following code works but messes up the animation (points no longer move).\ndef one_update_fixed(X):\n    result = torch.zeros_like(X)  # initialize result\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x - X) ** 2).sum(1))\n        #  weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        result[i] = (weight[:, None] * X).sum(0) / weight.sum()\n    return result\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5):\n        one_update(X)\n    return X\n\n\n%time X=meanshift(data)  \n# Note: %timeit is typically more accurate and can do multiple runs\n\nCPU times: user 242 ms, sys: 0 ns, total: 242 ms\nWall time: 242 ms\n\n\n\nplot_data(centroids + 2, X, n_samples)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Animation",
    "text": "Animation\n\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\n\n\ndef do_one(d):\n    if d:\n        one_update(X)\n    ax.clear()\n    plot_data(centroids + 2, X, n_samples, ax=ax)\n\n\n# create your own animation\nX = data.clone()\nfig, ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#gpu-batched-algorithm",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#gpu-batched-algorithm",
    "title": "FastAI Part 2 Lesson 12",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\n# check GPU resources\nprint(\"GPU available?\", torch.cuda.is_available())\nprint(\"How many GPUs?\", torch.cuda.device_count())\ntorch.cuda.empty_cache()  # clean cache\n\nGPU available? True\nHow many GPUs? 1\n\n\n\n#!nvidia-smi # show system management interface\n\n\nbs = 5\nX = data.clone()\nx = X[:bs]\nx.shape, X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a, b):\n    return (((a[None] - b[:, None]) ** 2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None, :].shape, x[:, None].shape, (X[None, :] - x[:, None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape, X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[..., None].shape, X[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\n\nnum = (weight[..., None] * X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ntorch.einsum(\"ij,jk-&gt;ik\", weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\nweight @ X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\n\nnum / div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i + bs, n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n            #  weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight @ X / div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\n%timeit -n 5 _=meanshift(data, 1250).cpu()\n\n912 µs ± 48.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids + 2, X, n_samples)\n\n\n\n\n\n\n\n\nVideo @ approx 1:34\nJeremy’s Homework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.\nBonus: Implement it in APL too!\nSuper bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.\nSuper super bonus: Publish a paper that describes it :D"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#introduction-to-k-means-clustering",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#introduction-to-k-means-clustering",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Introduction to K-Means Clustering",
    "text": "Introduction to K-Means Clustering\nThe following text is copied from wikipedia.\nGiven a set of observations \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\), where each observation is a \\(d\\)-dimensional real vector, \\(k\\)-means clustering aims to partition the \\(n\\) observations into \\(k\\) \\((k \\leq n)\\) sets \\(\\mathbf{S} = \\{ S_1, S_2, \\ldots, S_k \\}\\) so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find: \\[\\begin{equation}\n    \\operatorname{arg\\,min}_\\mathbf{S} \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_i} \\left\\| \\mathbf{x} - \\boldsymbol{\\mu}_i \\right\\|^2 = \\operatorname{arg\\,min}_\\mathbf{S} \\sum_{i=1}^k |S_i| \\operatorname{Var}(S_i)\n\\end{equation}\\] where \\(\\boldsymbol{\\mu}_i\\) is the mean (also called centroid) of points in \\(S_i\\), i.e. \\[\\begin{equation}\n    \\boldsymbol{\\mu}_i = \\frac{1}{|S_i|} \\sum_{\\mathbf{x} \\in S_i} \\mathbf{x},\n\\end{equation}\\] \\(|S_i|\\) is the size of \\(S_i\\), and \\(\\|\\cdot\\|\\) is the usual \\(L^2\\) norm. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster: \\[\\begin{equation}\n    \\operatorname{arg\\,min}_\\mathbf{S} \\sum_{i=1}^{k} \\frac{1}{|S_i|} \\sum_{\\mathbf{x}, \\mathbf{y} \\in S_i} \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^2\n\\end{equation}\\]\nStandard Algorithm (Naive k-means)\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called “the k-means algorithm”; it is also referred to as Lloyd’s algorithm, particularly in the computer science community. It is sometimes also referred to as “naïve k-means,” because there exist much faster alternatives (1).\nGiven an initial set of \\(k\\) means \\(m_1^{(1)}, \\ldots, m_k^{(1)}\\) (see below), the algorithm proceeds by alternating between two steps (2):\nAssignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance (3). Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means. \\[\\begin{equation}\nS_i^{(t)} = \\left\\{ x_p : \\| x_p - m_i^{(t)} \\|^2 \\leq \\| x_p - m_j^{(t)} \\|^2 \\ \\forall j, 1 \\leq j \\leq k \\right\\},\n\\end{equation}\\] where each \\(x_p\\) is assigned to exactly one \\(S^{(t)}\\), even if it could be assigned to two or more of them.\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster. \\[\\begin{equation}\nm_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{x_j \\in S_i^{(t)}} x_j\n\\end{equation}\\]\nThe objective function in \\(k\\)-means is the WCSS (within-cluster sum of squares). After each iteration, the WCSS decreases and so we have a nonnegative monotonically decreasing sequence. This guarantees that \\(k\\)-means always converges, but not necessarily to the global optimum.\nThe algorithm has converged when the assignments no longer change or equivalently, when the WCSS has become stable. The algorithm is not guaranteed to find the optimum (4).\nReferences\n\nPelleg, Dan; Moore, Andrew. “Accelerating exact k-means algorithms with geometric reasoning”. Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM Press, 1999, pp. 277–281.\nMacKay, David. “Information Theory, Inference and Learning Algorithms”. Cambridge University Press, 2003, pp. 284–292.\nSince the square root is a monotone function, this also is the minimum Euclidean distance assignment.\nHartigan, J. A.; Wong, M. A. “Algorithm AS 136: A k-Means Clustering Algorithm”. Journal of the Royal Statistical Society, Series C, 1979, vol. 28, no. 1, pp. 100–108.\n\n\n\n\nDefinitions:\nCluster: A cluster is a collection of data points that are grouped together because they share certain similarities, often in terms of their proximity to each other. In k-means clustering, each data point in the dataset is assigned to one of the k clusters based on its distance to the centroids. A cluster is essentially a subset of the entire dataset.\nCentroid: A centroid is a representative point of a cluster. It is the “center” of the cluster and is typically calculated as the mean (average) of all the points within that cluster. In each iteration of the k-means algorithm, the centroids are updated to be the new centers of their respective clusters based on the current assignments of points to clusters.\n\n# Import type hints for code readability and documentation\nfrom typing import List, Tuple\n\nfrom torch import Tensor"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-initial-implementation",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-initial-implementation",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Clustering: Initial Implementation",
    "text": "K-Means Clustering: Initial Implementation\n\nInitialize Centroids Using the Forgy Method\nIn this method, the centroids are randomly chosen from the actual data points. The function below essentially shuffles the indices of the points (random permutation) then picks the first k points. Note, there is no equivalent to random.sample in PyTorch.\n\ndef initialize_centroids(k, points):\n    \"\"\"\n    Initializes centroids using the Forgy method.\n\n    Parameters:\n    k (int): Number of centroids to initialize.\n    points (torch.Tensor): Tensor of shape (n, d) where \n                           n is the number of points and \n                           d is the dimensionality of each point.\n\n    Returns:\n    torch.Tensor: A tensor of shape (k, d) containing the initial centroids.\n    \"\"\"\n    indx = torch.randperm(len(points))[:k]\n    return points[indx]\n\n\n# we could also try centroids = torch.randn(k, points.shape[1])\n\n\n# Put data back on CPU for now:\ndata = data.cpu()\n\n\n# Test to see if the Forgy method is working. \n\nk = 6\nrandom_centroids = initialize_centroids(k, data)\nplot_data(\n    random_centroids, \n    data, \n    n_samples, \n    title=\"Random (Forgy) Initialized Centroids\"\n)\n\n# The centroids appear to be correctly initialized below.\n\n\n\n\n\n\n\n\n\n\nK-Means Clustering Algorithm Version 1.0\n\ndef distance(A, B):  # Euclidian distance\n    # return ((A-B)**2).sum(1).sqrt() # works fine.\n    return torch.norm(A - B, dim=1)  # per docs, this may be deprecated in future\n\n\ndef k_means_cluster_v1(k, points, max_iters=20, tol=1e-08):\n    # Initialize centroids: choose k centroids (Forgy, Random Partition, etc.)\n    centroids = initialize_centroids(k, points)  # returns a tensor of shape k,d\n\n    for iteration in range(max_iters):\n        clusters = [[] for _ in range(k)]  # initilize clusters\n\n        # Assign each point to the nearest centroid. \n        # The index of the points is stored in the 'clusters' list.\n        for i, point in enumerate(points):  \n            # We may attempt to vectorize this loop later, \n            # by using the batched distance function.\n            distances_to_each_centroid = distance(point, centroids)\n            cluster_assignment = torch.argmin(distances_to_each_centroid)  \n            clusters[cluster_assignment].append(i)\n\n        # Calculate new centroids\n        new_centroids = torch.zeros_like(centroids)\n        for idx, cluster in enumerate(clusters):\n            if cluster:  # Avoid division by zero\n                # calculate new centroid as the mean of the cluster\n                new_centroids[idx] = torch.mean(points[cluster], dim=0)  \n            # If a cluster is empty, re-initialize a single centroid: \n            else:  \n                new_centroids[idx] = initialize_centroids(1, points)[0]\n\n        # Test convergence\n        if torch.allclose(new_centroids, centroids, atol=tol):  \n            #  default absolute tolerance for isclose is 1e-08\n            #print(f\"Converged after {iteration + 1} iterations!\")\n            return clusters, new_centroids\n\n        centroids = new_centroids\n\n    # print(f\"Reached maximum number of iterations: {max_iters}\")\n    return clusters, centroids  # clusters returned for debugging purposes only\n\n\nk = 6\nclustering_test, final_centroids = k_means_cluster_v1(k, data)\n\n# these tensors must be on .cpu() for plotting purposes\nplot_data(\n    final_centroids.cpu(),\n    data.cpu(),\n    n_samples,\n    title=\"K-Means Clustering With Multiple Loops\",\n)\n\n\n\n\n\n\n\n\n\n%timeit -n 5 _ = k_means_cluster_v1(k, data)\n# Slow, typically at 200 to 300 ms per loop.\n\n83.8 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation-of-k-means",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#animation-of-k-means",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Animation of K-Means",
    "text": "Animation of K-Means\nUses a modification of our basic function that creates a list to store the history of the centroids and clusters at each step.\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\n\n\ndef plot_data_k_means(ax, points, clusters, centroids):\n    \"\"\" Plots data and centroids \"\"\"\n    ax.clear()\n    colors = [\"r\", \"y\", \"g\", \"b\", \"c\", \"m\"]  # Extend if needed\n    for idx, cluster in enumerate(clusters):\n        cluster_points = points[cluster]\n        ax.scatter(\n            cluster_points[:, 0],\n            cluster_points[:, 1],\n            c=colors[idx],\n            s=1,\n            label=f\"Cluster {idx + 1}\",\n        )\n    ax.scatter(\n        centroids[:, 0], \n        centroids[:, 1], \n        c=\"black\", \n        marker=\"x\", \n        s=20, \n        label=\"Centroid\"\n    )\n    ax.legend()\n    ax.set_title(\"K-Means Clustering Animation\")\n    ax.set_xlabel(\"Dimension 1\")\n    ax.set_ylabel(\"Dimension 2\")\n\n\ndef k_means_cluster_historic(k, points, max_iters=20):\n    \"\"\" Modified k-means clustering to capture each iteration's state \"\"\"\n    centroids = initialize_centroids(k, points)\n    history = []\n\n    for iteration in range(max_iters):\n        clusters = [[] for _ in range(k)]\n\n        for i, point in enumerate(points):\n            distances_to_each_centroid = distance(point, centroids)\n            cluster_assignment = torch.argmin(distances_to_each_centroid).item()\n            clusters[cluster_assignment].append(i)\n\n        new_centroids = torch.zeros_like(centroids)\n        for idx, cluster in enumerate(clusters):\n            if cluster:\n                new_centroids[idx] = torch.mean(points[cluster], dim=0)\n            else:\n                new_centroids[idx] = initialize_centroids(1, points)[0]\n\n        history.append((clusters, centroids))\n        # Is tensor mutated in place? If so, use centroids.clone() to store its state.\n        # Safer to use clusters.copy() too? Seems to work fine without!\n\n        if torch.all(torch.isclose(new_centroids, centroids, atol=1e-08)):\n            print(f\"Converged after {iteration + 1} iterations!\")\n            break\n\n        centroids = new_centroids\n\n    history.append((clusters, centroids.clone()))  # may be redundant?\n    return clusters, centroids, history\n\n\n# Function to animate k-means clustering\ndef animate_k_means(k, points, max_iters=20, interval=500):\n    _, _, history = k_means_cluster_historic(k, points, max_iters)\n    fig, ax = plt.subplots()\n\n    def update(frame):\n        clusters, centroids = history[frame]\n        plot_data_k_means(ax, points, clusters, centroids)\n\n    ani = FuncAnimation(\n        fig, update, frames=len(history), interval=interval, repeat=False\n    )\n    plt.close()\n    return HTML(ani.to_jshtml())\n\n\n# Put data back on CPU for now (plotting function gives an error on GPU):\ndata = data.cpu()\n\n# Display the animation\nanimation = animate_k_means(k, data, max_iters=20, interval=500)\nanimation\n\nConverged after 8 iterations!\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#vectorizing-gpu-batched-k-means-algorithm",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#vectorizing-gpu-batched-k-means-algorithm",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Vectorizing GPU Batched K-Means Algorithm",
    "text": "Vectorizing GPU Batched K-Means Algorithm\nNote to reader: Skip this part. I reproduced a little of Jeremy’s work, vectorized (enabled batch processing) the calculation of distances, but ultimately moved on.\nNote: Jeremy’s plotting function only works on the CPU.\n\n# Let's reproduce Jeremy's work above:\nbs = 5\nX = data.clone()\nx = X[:bs]\nx.shape, X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a, b):\n    return (((a[None] - b[:, None]) ** 2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(x, X).shape\n\ntorch.Size([1500, 5])\n\n\nLets see if we can vectorize the following loop in our K-means algorithm:\nclusters = [[] for _ in range(k)] # initilize clusters \n      \nfor i, point in enumerate(points): \n    distances_to_each_centroid = distance(point, centroids)\n    cluster_assignment = torch.argmin(distances_to_each_centroid)\n    clusters[cluster_assignment].append(i)\n\n\n# First, we create a very simple list of points, with pre-defined centroids, \n# so we can easily view results.\n\nsimple_points = torch.tensor(\n    [[1.0, 2.0], [1.0, 4.0], [1.0, 0.0], [10.0, 2.0], [10.0, 4.0], [10.0, 0.0]]\n)\nsimple_centroids = torch.tensor([[10.5, 2.0], [1.5, 2.0]])\n\n# Note: x-offset by 0.5 for plotting\n\n\nsimple_points.shape, simple_centroids.shape\n\n(torch.Size([6, 2]), torch.Size([2, 2]))\n\n\n\n# Let's create a batch. Here of size 3 (points 0, 1, and 2).\nbatched_points = simple_points[:3]\n\n\nbatched_points.shape\n\ntorch.Size([3, 2])\n\n\nThe batched_points tensor has a shape of [3,2], i.e., three (x,y) points. If we try to use our current distance function to calculate the distance to the centroids of shape [2,2], i.e., two centroids located at (x,y), we will receive an error, specifically:\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0.\nLet’s modify the inputs to the distance function to ensure the sizes match on the appropriate dimensions.\n\n# Testing to create an additional dimension, \n# using None (equivalent to np.newaxis in NumPy).\n# Alternatively, use torch.unsqueeze_(index_of_new_dimension).\n\nprint(batched_points[None].shape)\nprint(simple_centroids[:, None].shape)\n\n# None at index 1, to align the last dimensions correctly; \n# note that [:,:,None] would be incorrect\n# Output shows we are nicely aligned at the last dimension [2]\n\ntorch.Size([1, 3, 2])\ntorch.Size([2, 1, 2])\n\n\n\ndef dist_batch(points, centroids):  # Euclidian distance\n    return (\n        (((points[None]) - (centroids[:, None])) ** 2).sum(2).sqrt()\n    )  # sum over the last dimension (index [2])\n    \n    # Alternative:\n    # return torch.norm(points[None] - centroids[:,None], dim=2)\n    \n    # Einsum practice:\n    # dif = points[None] - centroids[:,None]\n    # return torch.einsum('ijk,ijk-&gt;ij', dif, dif).sqrt()\n\ndist_batch(batched_points, simple_centroids)\n\ntensor([[9.500, 9.708, 9.708],\n        [0.500, 2.062, 2.062]])\n\n\n\n# We will revise the following loop to incorporate batches.\n\nclusters = [[] for _ in range(len(simple_centroids))]  # Initialize clusters list\n\nfor i, point in enumerate(simple_points):\n    distances_to_each_centroid = distance(point, simple_centroids)\n    cluster_assignment = torch.argmin(distances_to_each_centroid)\n    clusters[cluster_assignment].append(i)\n\nprint(clusters)\n\n[[3, 4, 5], [0, 1, 2]]\n\n\n\nTesting Assignment of Points to Clusters (in the For loop)\nLet’s test out our new code for running batches of points\n\ntorch.set_default_device(\"cpu\")  # for plotting\n\n# Test of the clusters list\nsimple_points = torch.tensor(\n    [[1.0, 2.0], [1.0, 4.0], [1.0, 0.0], [10.0, 2.0], [10.0, 4.0], [10.0, 0.0]]\n)\nsimple_centroids = torch.tensor(\n    [[10.5, 2.0], [1.5, 2.0]]\n)  # x-offset by 0.5 for plotting\n\ndef distance(A, B):\n    return ((A - B) ** 2).sum(1).sqrt()\n\n# Assign points to clusters based on distances to centroids:\n\n# Initialize clusters list\nclusters = [[] for _ in range(len(simple_centroids))]  \n\nfor i, point in enumerate(simple_points):\n    distances_to_each_centroid = distance(point, simple_centroids)\n    cluster_assignment = torch.argmin(distances_to_each_centroid)\n    clusters[cluster_assignment].append(i)\n\n# Plotting\ncolors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\"]\nfor cluster_idx, cluster in enumerate(clusters):\n    cluster_points = simple_points[cluster]\n    plt.scatter(\n        cluster_points[:, 0],\n        cluster_points[:, 1],\n        c=colors[cluster_idx],\n        label=f\"Cluster {cluster_idx}\",\n    )\n\n# Plot centroids\nplt.scatter(\n    simple_centroids[:, 0],\n    simple_centroids[:, 1],\n    c=\"black\",\n    marker=\"X\",\n    s=100,\n    label=\"Centroids (x-offset by 0.5 units)\",\n)\n\n# Plot configuration\nplt.xlabel(\"X coordinate\")\nplt.ylabel(\"Y coordinate\")\nplt.title(\"Simple Example with Offset Centroids\", fontsize=10)\nplt.suptitle(\n    \"K-Means Clustering\", fontsize=12\n)  # Actually a title. Matplotlib places the subtitle above the title!\nplt.legend()\nplt.show()\n\nprint(\"Cluster assignment for two centroids=\", clusters)\n\n\n\n\n\n\n\n\nCluster assignment for two centroids= [[3, 4, 5], [0, 1, 2]]\n\n\n\n\nRunning batches through the (vectorized) loop that calculates distances\nSuccessfully completed. The two outputs match: [[3, 4, 5], [0, 1, 2]].\n\ntorch.set_default_device(\"cuda\")\ndata = data.cuda()\n\nk = len(simple_centroids)  # k=2 for testing purposes\nbs = 3\npoints = simple_points\nn = len(points)\n\n# add the following within a loop of max iters:\n\nclusters = [[] for _ in range(k)]\nprint(\"empty clusters=\", clusters)\nfor i in range(0, n, bs):\n    s = slice(i, min(i + bs, n))\n    print(s)\n    distances_to_each_centroid = dist_batch(points[s], simple_centroids)\n    print(distances_to_each_centroid)\n    cluster_assignment = torch.argmin(\n        distances_to_each_centroid, dim=0, keepdim=False\n    )  # bug here? dim = 1? Investigate.\n    print(cluster_assignment)\n    # OK, but now we are stuck with a For loop for cluster assignment.\n    for j, cluster_idx in enumerate(cluster_assignment):\n        clusters[cluster_idx].append(i + j)\n    print(\"completed iteration\")\nprint(\"clusters variable =\", clusters)\n\nempty clusters= [[], []]\nslice(0, 3, None)\ntensor([[9.500, 9.708, 9.708],\n        [0.500, 2.062, 2.062]])\ntensor([1, 1, 1])\ncompleted iteration\nslice(3, 6, None)\ntensor([[0.500, 2.062, 2.062],\n        [8.500, 8.732, 8.732]])\ntensor([0, 0, 0])\ncompleted iteration\nclusters variable = [[3, 4, 5], [0, 1, 2]]\n\n\nOK, so the outputs are the same. We have (possibly) successfully batched the ‘distance’ calculations. However, we still have to loop through and do cluster assignment, and then calculate the means of the clusters (i.e., create centroids). To tackle the cluster assignment, we could store a list of cluster assignments outside the loop, then perform that after all batches are done. However, I’ll take step back and see if there is a different approach. I think the code would become over-complicated if I were to continue with this line of thinking."
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-2.0",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-2.0",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Version 2.0",
    "text": "K-Means Version 2.0\nI asked ChatGPT to see if it could improve my code. For whatever reason, it took my batches out and came up with the following code (after some debugging, mainly related to variable type and dimension mismatches).\n\ntorch.set_default_device(\"cuda\")\ngpu_data = data.clone().cuda()\n\n\ndef dist_batch(points, centroids):\n    # Calculate the distance from each point to each centroid\n    return torch.cdist(points, centroids)  # L2 norm by default (p=2.0)\n\n\ndef k_means_cluster_v2(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)  # returns a tensor of shape k,d\n\n    for i in range(max_iters):\n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n\n        # Debugging: Print shapes of relevant tensors\n        # print(f\"Shapes: cluster_assignments={cluster_assignments.shape}, points={points.shape}\")\n\n        cluster_sums = torch.zeros_like(centroids)\n        cluster_counts = torch.zeros(k, dtype=torch.int32)\n\n        # Debugging: Print cluster_assignments to check its values\n        # print(\"Cluster Assignments:\", cluster_assignments)\n\n        for idx in range(k):\n            assigned_points = points[cluster_assignments == idx]\n            if len(assigned_points) &gt; 0:\n                cluster_sums[idx] = assigned_points.sum(dim=0)\n                cluster_counts[idx] = assigned_points.shape[0]\n\n        # Avoid division by zero\n        mask = cluster_counts &gt; 0\n        new_centroids = torch.where(\n            mask.unsqueeze(1), cluster_sums / cluster_counts.unsqueeze(1), centroids\n        )\n\n        if torch.allclose(new_centroids, centroids, atol=tol):\n            # print(f\"Converged after {i + 1} iterations!\")\n            break\n\n        centroids = new_centroids\n\n    # else:\n    # print(f\"Reached maximum number of iterations: {max_iters}\")\n\n    return centroids\n\nfinal_centroids = k_means_cluster_v2(6, gpu_data)\nplot_data(final_centroids.cpu(), data.cpu(), n_samples)\n\n\n\n\n\n\n\n\n\n%timeit -n 5 _ = k_means_cluster_v2(k, gpu_data)\n\n971 µs ± 167 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nThis version is much quicker (on GPU at approx. 1-2 ms vs. 200-300 ms above). Note that the speeds often seem to depend on how many iterations are required for convergence. For a better test, we should take out the need for convergence and loop for a defined number of times (perhaps 5).\nFor fun, I asked ChatGPT if it could remove the loop over the range of k. Not sure if that is worthwhile, since k is usually low (6 in the example here), although the list of data points can be long. In any case, I was interested to learn a few more PyTorch functions."
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-3.0-an-implementation-using-torch.scatter_",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-version-3.0-an-implementation-using-torch.scatter_",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Version 3.0: An implementation using torch.scatter_()",
    "text": "K-Means Version 3.0: An implementation using torch.scatter_()\nThe first algorithm it came up with suggested using torch.scatter_(). Further explanation on this function in the cells below. After fixing a few issues, we had a working version:\n\n# Removed a For loop and replaced with .scatter_ function\n\ndef dist_batch(points, centroids):\n    # Calculate the distance from each point to each centroid\n    return torch.cdist(points, centroids)  # L2 norm by default (p=2.0)\n\n\ndef k_means_cluster_v3(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)  # returns a tensor of shape (k, d)\n\n    for _ in range(max_iters):  # replace _ with i if not using print statements below\n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n\n        # Debugging: Print shapes of relevant tensors\n        # print(f\"Shapes: cluster_assignments={cluster_assignments.shape}, points={points.shape}\")\n\n        # Create tensors to hold the sums of points and counts of points per cluster\n        cluster_sums = torch.zeros_like(centroids)\n        cluster_counts = torch.zeros(k, dtype=torch.int32)\n\n        # Debugging: Print cluster_assignments to check its values\n        # print(\"Cluster Assignments:\", cluster_assignments)\n\n        # Use scatter_add_ to sum points and count points per cluster\n        cluster_sums.scatter_add_(\n            0, cluster_assignments.unsqueeze(1).expand_as(points), points\n        )\n        cluster_counts.scatter_add_(\n            0,\n            cluster_assignments,\n            torch.ones_like(cluster_assignments, dtype=torch.int32),\n        )\n\n        # Equivalent code with scatter_reduce: \n        # However, no reason to use 'sum' when we have an option for 'mean'. \n        # See Version 3.1.\n        # cluster_sums.scatter_reduce_(0, cluster_assignments.unsqueeze(1).expand_as(points), points, reduce=\"sum\")\n        # cluster_counts.scatter_reduce_(0, cluster_assignments, torch.ones_like(cluster_assignments, dtype=torch.int32), reduce=\"sum\")\n\n        # Original For loop:\n        # for idx in range(k):\n        #    assigned_points = points[cluster_assignments == idx]\n        #    if len(assigned_points) &gt; 0:\n        #        cluster_sums[idx] = assigned_points.sum(dim=0)\n        #        cluster_counts[idx] = assigned_points.shape[0]\n\n        # Avoid division by zero\n        mask = cluster_counts &gt; 0\n        # Calculate new centroid as the mean of each cluster\n        new_centroids = torch.where(\n            mask.unsqueeze(1), cluster_sums / cluster_counts.unsqueeze(1), centroids\n        )\n\n        if torch.allclose(new_centroids, centroids, atol=tol):\n            # print(f\"Converged after {i + 1} iterations!\")\n            break\n\n        centroids = new_centroids\n\n    # else:\n    # print(f\"Reached maximum number of iterations: {max_iters}\")\n\n    return centroids\n\nfinal_centroids = k_means_cluster_v3(6, gpu_data)\nplot_data(final_centroids.cpu(), data.cpu(), n_samples)\n\n\n\n\n\n\n\n\n\ntorch.set_default_device(\"cuda\")\n%timeit -n 5 _ = k_means_cluster_v3(k, data.cuda())\n\n769 µs ± 99.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\ntorch.set_default_device(\"cpu\")\n%timeit -n 5 _ = k_means_cluster_v3(k, data.cpu())\n\n554 µs ± 87.7 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nInteresting. This version is actually slower on GPU than CPU. I suspect it’s copying a lot of stuff back and forth, or I’m simply doing something wrong here!\nExplaining the code above\nChatGTP provided the following explanation of the initial changes it made:\n\nCluster Sums: scatter_add_ is used to sum points assigned to each cluster.\n\ncluster_sums.scatter_add_(0, cluster_assignments.unsqueeze(1).expand_as(points), points)\n\ncluster_assignments.unsqueeze(1).expand_as(points) creates an expanded index tensor to match the dimensions of points.\nscatter_add_ adds the points to the corresponding clusters based on cluster_assignments.\n\n\nCluster Counts: scatter_add_ is also used to count points in each cluster.\n\ncluster_counts.scatter_add_(0, cluster_assignments, torch.ones_like(cluster_assignments, dtype=torch.int32))\n\nHere, we scatter ones into cluster_counts to count the number of points assigned to each cluster.\n\nTo understand torch.scatter_() and related functions, I suggest the following blog post. The PyTorch documentation is somewhat difficult to understand.\nhttps://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\nAfter reviewing the documentation, I noticed that scatter_reduce has an option to return the mean. Instead of performing sums / counts to create the average, let’s try do this directly with scatter_reduce.\nFrom the documentation:\nTensor.scatter_reduce_(dim, index, src, reduce, *, include_self=True) → Tensor\nReduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (“sum”, “prod”, “mean”, “amax”, “amin”). For each value in src, it is reduced to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. If include_self=“True”, the values in the self tensor are included in the reduction.\n\n# Using the scatter_reduce 'mean' argument\n\ndef k_means_cluster_v3_1(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)\n\n    for _ in range(max_iters):\n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n        cluster_means = torch.zeros_like(centroids)\n        cluster_means.scatter_reduce_(\n            0, cluster_assignments.unsqueeze(1).expand_as(points), points, reduce=\"mean\"\n        )\n\n        if torch.allclose(cluster_means, centroids, atol=tol):\n            break\n\n        centroids = cluster_means\n\n    return centroids\n\n\ntorch.set_default_device(\"cuda\")\nfinal_centroids = k_means_cluster_v3_1(6, gpu_data)\nplot_data(final_centroids.cpu(), data.cpu(), n_samples)\n\n\n\n\n\n\n\n\n\ntorch.set_default_device(\"cuda\")\n%timeit -n 5 _ = k_means_cluster_v3_1(k, data.cuda())\n\n669 µs ± 147 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\ntorch.set_default_device(\"cpu\")\n%timeit -n 5 _ = k_means_cluster_v3_1(k, data.cpu())\n\n459 µs ± 45.3 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nWell, that is about the same, maybe slightly quicker, and the code looks cleaner too."
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data-with-xy-and-z-axis",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#create-data-with-xy-and-z-axis",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Create data with x,y, and z axis",
    "text": "Create data with x,y, and z axis\n\nn_xyz_clusters = 6\nn_samples_per_cluster = 250\n\nxyz_centroids = (\n    torch.rand(n_xyz_clusters, 3) * 70 - 35\n)  # creates uniform random variable between -35 and 35.\nxyz_centroids.shape\n\ntorch.Size([6, 3])\n\n\n\ndef sample_from_distribution(mean: Tensor, n_samples: int) -&gt; Tensor:\n    \"\"\"\n    Generates samples from a multivariate normal distribution.\n\n    Parameters:\n        mean (Tensor): A tensor of shape 3 representing a mean vector\n        n_samples (int): The number of samples to generate.\n\n    Returns:\n        Tensor: A tensor of shape (n_samples, 3) containing the generated samples.\n    \"\"\"\n    # Create a 3x3 diagonal covariance matrix with variances [5., 5., 5.]\n    cov_matrix = torch.diag(torch.tensor([5.0, 5.0, 5.0]))\n\n    # Create a multivariate normal distribution \n    # with the given mean vector and covariance matrix\n    distribution = MultivariateNormal(mean, cov_matrix)\n\n    # Generate and return samples from the distribution\n    return distribution.sample((n_samples,))\n\n\nnew_slices = [sample_from_distribution(c, n_samples_per_cluster) for c in xyz_centroids]\nxyz_data = torch.cat(new_slices)\nxyz_data.shape\n\ntorch.Size([1500, 3])"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#plot-3d-data",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#plot-3d-data",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Plot 3D data",
    "text": "Plot 3D data\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef plot_3d_data(centroids, data, n_samples, ax=None, title=None):\n    \"\"\"\n    Plot 3D data points and centroids.\n    \"\"\"\n    if ax is None:\n        fig = plt.figure(figsize=(10, 8))\n        ax = fig.add_subplot(111, projection=\"3d\")\n\n    for i, centroid in enumerate(centroids):\n        samples = data[i * n_samples : (i + 1) * n_samples]\n        ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2], s=1)\n\n    for centroid in centroids:\n        ax.scatter(\n            *centroid, s=100, marker=\"x\", color=\"k\", linewidths=3, zorder=10\n        )  # attempt to plot centroids on top; not well.\n        # Note that *centroid unpacks the 3D coordinates (i.e., x, y, z)\n    if title:\n        ax.set_title(title)\n\n\nplot_3d_data(\n    xyz_centroids.cpu(),\n    xyz_data.cpu(),\n    n_samples_per_cluster,\n    title=\"3D Data Clustering\",\n)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-in-3d",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#k-means-clustering-in-3d",
    "title": "FastAI Part 2 Lesson 12",
    "section": "K-Means Clustering in 3D",
    "text": "K-Means Clustering in 3D\n\ntest_data = xyz_data.clone()\n\n\ndef initialize_centroids(k, points):\n    # See documentation above. Forgy method of initialization.\n    indx = torch.randperm(len(points))[:k]\n    return points[indx]\n\n\ndef dist_batch(points, centroids):\n    # Same as function defined above.\n    return torch.cdist(points, centroids)  # L2 norm by default (p=2.0)\n\n\n# Same function as version 3.1 above. Copied here in case modification is needed.\n\ndef k_means_cluster(k, points, max_iters=20, tol=1e-08):\n    centroids = initialize_centroids(k, points)\n\n    for _ in range(max_iters):\n        distances_to_each_centroid = dist_batch(points, centroids)\n        cluster_assignments = torch.argmin(distances_to_each_centroid, dim=1)\n        cluster_means = torch.zeros_like(centroids)\n        cluster_means.scatter_reduce_(\n            0, cluster_assignments.unsqueeze(1).expand_as(points), points, reduce=\"mean\"\n        )\n\n        if torch.allclose(cluster_means, centroids, atol=tol):\n            break\n\n        centroids = cluster_means\n    return centroids\n\n\nfinal_3D_centroids = k_means_cluster(6, test_data)\nfinal_3D_centroids\n\ntensor([[ 14.190,   2.650, -30.780],\n        [ 24.511, -13.043,  20.713],\n        [ 21.041, -20.131, -34.454],\n        [-27.653,   9.463, -16.193],\n        [ -8.283, -13.172, -33.587],\n        [-12.459, -18.228, -21.804]])\n\n\n\nplot_3d_data(\n    final_3D_centroids, xyz_data, n_samples_per_cluster, title=\"3D Data Clustering\"\n)"
  },
  {
    "objectID": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#results",
    "href": "fastai/fastai-P2L12/fastai-P2L12-meanshift.html#results",
    "title": "FastAI Part 2 Lesson 12",
    "section": "Results",
    "text": "Results\nThe 3D clustering appears to be successful, without any modifications to the k-means function. I’m actually surprised it worked without giving me a dimension mismatch error, or at least it seems to work.\nNote that the code has not been tested much. It would really benefit from some checks."
  },
  {
    "objectID": "dsaposts/three-sum/ThreeIntegerSum.html",
    "href": "dsaposts/three-sum/ThreeIntegerSum.html",
    "title": "LeetCode 15: Three Sum",
    "section": "",
    "text": "Difficulty: Medium\nGiven an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] where nums[i] + nums[j] + nums[k] == 0, and the indices i != j, i != k, and j != k.\nThe solution should not contain duplicate triplets. Output order does not matter.\nExample 1:\nInput: nums = [-1,0,1,2,-1,-4]\nOutput: [[-1,-1,2],[-1,0,1]]\nExplanation:\nnums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0\nnums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0\nnums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0\nThe distinct triplets are [-1,0,1] and [-1,-1,2].\nExample 2:\nInput: nums = [0,1,1]\nOutput: []\nExplanation: The only possible triplet does not sum up to 0.\nExample 3:\nInput: nums = [0,0,0]\nOutput: [[0,0,0]]\nExplanation: The only possible triplet sums up to 0.\nConstraints:\n3 &lt;= nums.length &lt;= 3000 # LeetCode\n3 &lt;= nums.length &lt;= 1000 # NeetCode\n-10^5 &lt;= nums[i] &lt;= 10^5"
  },
  {
    "objectID": "dsaposts/three-sum/ThreeIntegerSum.html#problem-description",
    "href": "dsaposts/three-sum/ThreeIntegerSum.html#problem-description",
    "title": "LeetCode 15: Three Sum",
    "section": "",
    "text": "Difficulty: Medium\nGiven an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] where nums[i] + nums[j] + nums[k] == 0, and the indices i != j, i != k, and j != k.\nThe solution should not contain duplicate triplets. Output order does not matter.\nExample 1:\nInput: nums = [-1,0,1,2,-1,-4]\nOutput: [[-1,-1,2],[-1,0,1]]\nExplanation:\nnums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0\nnums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0\nnums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0\nThe distinct triplets are [-1,0,1] and [-1,-1,2].\nExample 2:\nInput: nums = [0,1,1]\nOutput: []\nExplanation: The only possible triplet does not sum up to 0.\nExample 3:\nInput: nums = [0,0,0]\nOutput: [[0,0,0]]\nExplanation: The only possible triplet sums up to 0.\nConstraints:\n3 &lt;= nums.length &lt;= 3000 # LeetCode\n3 &lt;= nums.length &lt;= 1000 # NeetCode\n-10^5 &lt;= nums[i] &lt;= 10^5"
  },
  {
    "objectID": "dsaposts/three-sum/ThreeIntegerSum.html#initial-solution",
    "href": "dsaposts/three-sum/ThreeIntegerSum.html#initial-solution",
    "title": "LeetCode 15: Three Sum",
    "section": "Initial Solution",
    "text": "Initial Solution\n\nfrom typing import List\n\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        triplets = []\n        nums.sort() # O(n log n) sorting\n        \n        for idx, value in enumerate(nums):\n            l = idx + 1\n            r = len(nums) - 1\n\n            # skip duplicate elements:\n            if idx &gt; 0 and value == nums[idx - 1]:\n                continue\n            \n            while l &lt; r:\n                triplet = [value, nums[l], nums[r]]\n                triplet_sum = sum(triplet)\n                \n                if triplet_sum &gt; 0:\n                    r -= 1\n                elif triplet_sum &lt; 0:\n                    l += 1\n                else:\n                    # inefficient check for duplicates:\n                    if triplet not in triplets:\n                        triplets.append(triplet)\n                    l += 1\n                                    \n        return triplets\n\n\nTest function\n\ndef test_three_sum(solution_class):\n    \"\"\" Test of threeSum function. Not extensive.\"\"\"\n    solution = solution_class()\n    test_cases = [\n        {\n            \"nums\": [],\n            \"expected\": []\n        },\n        {\n            \"nums\": [0, 1, 1],\n            \"expected\": []\n        },\n        {\n            \"nums\": [0, 0, 0],\n            \"expected\": [[0, 0, 0]]\n        },\n        {\n            \"nums\": [0, 0, 0, 0],\n            \"expected\": [[0, 0, 0]]\n        },\n        {\n            \"nums\": [-1, 0, 1, 2, -1, -4],\n            \"expected\": [[-1, -1, 2], [-1, 0, 1]]\n        },\n        {\n            \"nums\": [-2, 0, 1, 1, 2],\n            \"expected\": [[-2, 0, 2], [-2, 1, 1]]\n        },\n        {\n            \"nums\": [-1, 0, 1, 2, -1, -4, -2, -3, 3, 0, 4],\n            \"expected\": [[-4, 0, 4], [-4, 1, 3], [-3, -1, 4], [-3, 0, 3], \n                         [-3, 1, 2], [-2, -1, 3], [-2, 0, 2], [-1, -1, 2], [-1, 0, 1]]\n        }\n    ] \n\n    for i, test_case in enumerate(test_cases):\n        nums = test_case[\"nums\"]\n        expected = test_case[\"expected\"]\n        results = solution.threeSum(nums)\n        \n        # Sort both results and expected for comparison\n        results_sorted = sorted([sorted(triplet) for triplet in results])\n        expected_sorted = sorted([sorted(triplet) for triplet in expected])\n\n        if results_sorted == expected_sorted:\n            print(f\"Test case {i+1} passed\")\n        else:\n            print(f\"Test case {i+1} failed\")\n            print(f\"Expected: {expected_sorted}\")\n            print(f\"Got: {results_sorted}\")\n\n\ntest_three_sum(Solution)\n\nTest case 1 passed\nTest case 2 passed\nTest case 3 passed\nTest case 4 passed\nTest case 5 passed\nTest case 6 passed\nTest case 7 passed"
  },
  {
    "objectID": "dsaposts/three-sum/ThreeIntegerSum.html#initial-results",
    "href": "dsaposts/three-sum/ThreeIntegerSum.html#initial-results",
    "title": "LeetCode 15: Three Sum",
    "section": "Initial Results",
    "text": "Initial Results\nMy initial attempt passed NeetCode but is too slow to pass LeetCode, failing the last 2 of 313 testcases. I also created a somewhat messy algorithm that used a hashmap to store sums from pairs of values in an attempt to trade off some memory for time, but that failed the speed test too.\nHaving wasted enough time on this problem, I gave up and looked at the NeetCode solution. However, it later occurred to me that we could store the triplets in a set, then we wouldn’t have to check if they existed in the set, as the set would handle this naturally. The slightly improved algorithm is below.\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        triplets = set()  # Use a set to store unique triplets\n        nums.sort()  # O(n log n) sorting\n        \n        for idx, value in enumerate(nums):  # O(n)\n            l = idx + 1\n            r = len(nums) - 1\n\n            # Skip duplicate elements to avoid duplicate triplets\n            if idx &gt; 0 and value == nums[idx - 1]:  \n                continue\n            \n            while l &lt; r:  # O(n) for each iteration\n                triplet = (value, nums[l], nums[r])  # Use tuple for immutability (needed by set)\n                triplet_sum = sum(triplet)\n                \n                if triplet_sum &gt; 0:\n                    r -= 1  \n                elif triplet_sum &lt; 0:\n                    l += 1 \n                else:\n                    triplets.add(triplet)  # O(1) on average\n                    l += 1  \n                                 \n        return list(map(list, triplets)) # Convert tuple back to list\n\nThis one finally passed the LeetCode submission, but is unfortunately very slow. It beats a mere 19% on runtime and 5% on memory. Pretty terrible, but good enough to pass. Worst-case complexity is:\n\nTime complexity: \\(O(n^2)\\) due to the nested loops\nSpace complexity: \\(O(n^2)\\) due to storing the results list (also the set)"
  },
  {
    "objectID": "dsaposts/three-sum/ThreeIntegerSum.html#neetcode-solution",
    "href": "dsaposts/three-sum/ThreeIntegerSum.html#neetcode-solution",
    "title": "LeetCode 15: Three Sum",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        result = []\n        nums.sort()\n\n        for i, a in enumerate(nums):\n            if a &gt; 0:\n                break\n\n            if i &gt; 0 and a == nums[i - 1]:\n                continue\n\n            l, r = i + 1, len(nums) - 1\n            while l &lt; r:\n                threeSum = a + nums[l] + nums[r]\n                if threeSum &gt; 0:\n                    r -= 1\n                elif threeSum &lt; 0:\n                    l += 1\n                else:\n                    result.append([a, nums[l], nums[r]])\n                    l += 1\n                    r -= 1\n                    while nums[l] == nums[l - 1] and l &lt; r:\n                        l += 1\n                        \n        return result\n\nNeetCode’s solution has two nice optimizations over my code:\n\nEarly exit:\n\nif a &gt; 0:\n    break\nWhen the number a in nums becomes positive, the loop terminates early. Since the array is sorted, if a &gt; 0, it’s impossible to find two other positive numbers whose sum, along with a, equals zero.\n\nEfficient handling off duplicate triplets:\n\nwhile nums[l] == nums[l - 1] and l &lt; r:\n    l += 1\nAfter finding a valid triplet, the left pointer is incremented and the additional while loop ensures we skip over any duplicate numbers adjacent to nums[l]. Thus preventing the same triplet from being added to the result multiple times.\nOn LeetCode, this nice algorithm beats 98% on runtime and 85% on memory. Worst-case complexity is:\n\nTime complexity: \\(O(n^2)\\)\n\nSpace complexity: \\(O(n^2)\\)"
  },
  {
    "objectID": "dsaposts/valid-palindrome/IsPalindrome.html",
    "href": "dsaposts/valid-palindrome/IsPalindrome.html",
    "title": "LeetCode 125: Valid Palindrome",
    "section": "",
    "text": "Difficulty: Easy\nGiven a string s, return True if it is a palindrome, otherwise return False.\nA palindrome is a string that is identical when read from both directions, disregarding case differences and excluding all non-alphanumeric characters.\nThis problem is also known as “Is Palindrome” on NeetCode.\nExamples:\nInput: s = \"Was it a car or a cat I saw?\"\nOutput: true\n\nInput: s = \"tab a cat\"\nOutput: false\nConstraints:\n1 &lt;= s.length &lt;= 1000\ns consists of only printable ASCII characters."
  },
  {
    "objectID": "dsaposts/valid-palindrome/IsPalindrome.html#problem-description",
    "href": "dsaposts/valid-palindrome/IsPalindrome.html#problem-description",
    "title": "LeetCode 125: Valid Palindrome",
    "section": "",
    "text": "Difficulty: Easy\nGiven a string s, return True if it is a palindrome, otherwise return False.\nA palindrome is a string that is identical when read from both directions, disregarding case differences and excluding all non-alphanumeric characters.\nThis problem is also known as “Is Palindrome” on NeetCode.\nExamples:\nInput: s = \"Was it a car or a cat I saw?\"\nOutput: true\n\nInput: s = \"tab a cat\"\nOutput: false\nConstraints:\n1 &lt;= s.length &lt;= 1000\ns consists of only printable ASCII characters."
  },
  {
    "objectID": "dsaposts/valid-palindrome/IsPalindrome.html#initial-solution",
    "href": "dsaposts/valid-palindrome/IsPalindrome.html#initial-solution",
    "title": "LeetCode 125: Valid Palindrome",
    "section": "Initial Solution",
    "text": "Initial Solution\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def isPalindrome(self, s: str) -&gt; bool:\n        # be sure not to use .isalpha :)\n        clean_s = \"\".join(char.lower() for char in s if char.isalnum())\n\n        for i, letter in enumerate(clean_s):\n            if letter != clean_s[-1 - i]:\n                return False\n        else:\n            return True\n       \n        #  In hindsight, the following would be cleaner,\n        #  but not sure if it would allow us to return early?\n        #  return clean_s == clean_s[::-1]\n\n\n#  Test function is defined below\ntest_isPalindrome(InitialSolution)\n\nTests passed\n\n\nIntial Results\nMy first attempt didn’t require too much effort to produce; however, I was disappointed to find it was rated in the bottom 40% for time and bottom 8% for memory. That initial string cleaning definitely cost me some time and memory. My poor attempt forced me to learn why these are called two-pointer algorithms! I basically have only one pointer (i), when we need two so they can act independently at each end of the string, cleaning (or ignoring) as we go.\n\nTime complexity: \\(O(n)\\) for cleaning the string plus \\(O(n)\\) for the palindrome check, i.e., still \\(O(n)\\) overall.\nSpace complexity: \\(O(n)\\) due to the extra memory used by the clean string clean_s.\n\nLet’s try again…\n\nimport logging\n\nclass Solution:\n    def __init__(self):\n        # Set up logging\n        logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s: %(message)s\")\n        self.logger = logging.getLogger(__name__)\n\n    def isPalindrome(self, s: str) -&gt; bool:\n        # Initialize pointers\n        left = 0\n        right = len(s) - 1\n\n        self.logger.debug(\"Initial string: %s\", s)\n        self.logger.debug(\"Initial left pointer: %d, right pointer: %d\", left, right)\n\n        while left &lt; right:\n            while left &lt; right and not s[left].isalnum():\n                self.logger.debug(\n                    \"Skipping non-alphanumeric character at left %d: %s\", left, s[left]\n                )\n                left += 1\n            while right &gt; left and not s[right].isalnum():\n                self.logger.debug(\n                    \"Skipping non-alphanumeric character at right %d: %s\",\n                    right,\n                    s[right],\n                )\n                right -= 1\n            if s[left].lower() != s[right].lower():\n                self.logger.debug(\n                    \"Characters do not match: %s != %s\",\n                    s[left].lower(),\n                    s[right].lower(),\n                )\n                return False\n            self.logger.debug(\n                \"Characters match: %s == %s\", s[left].lower(), s[right].lower()\n            )\n            left += 1\n            right -= 1\n            self.logger.debug(\n                \"Updated left pointer: %d, right pointer: %d\", left, right\n            )\n        else:\n            self.logger.debug(\"String is a palindrome\")\n            return True\n\n\ns = \"No lemon, no melon\"  # True\nsolution = Solution()\nsolution.isPalindrome(s)\n\nDEBUG: Initial string: No lemon, no melon\nDEBUG: Initial left pointer: 0, right pointer: 17\nDEBUG: Characters match: n == n\nDEBUG: Updated left pointer: 1, right pointer: 16\nDEBUG: Characters match: o == o\nDEBUG: Updated left pointer: 2, right pointer: 15\nDEBUG: Skipping non-alphanumeric character at left 2:  \nDEBUG: Characters match: l == l\nDEBUG: Updated left pointer: 4, right pointer: 14\nDEBUG: Characters match: e == e\nDEBUG: Updated left pointer: 5, right pointer: 13\nDEBUG: Characters match: m == m\nDEBUG: Updated left pointer: 6, right pointer: 12\nDEBUG: Skipping non-alphanumeric character at right 12:  \nDEBUG: Characters match: o == o\nDEBUG: Updated left pointer: 7, right pointer: 10\nDEBUG: Characters match: n == n\nDEBUG: Updated left pointer: 8, right pointer: 9\nDEBUG: Skipping non-alphanumeric character at left 8: ,\nDEBUG: Characters match:   ==  \nDEBUG: Updated left pointer: 10, right pointer: 8\nDEBUG: String is a palindrome\n\n\nTrue\n\n\n\nA note on logging\nI used the built-in JupyterLab debugger to debug my initial attempt, but decided to add logging (above) to help visualize the code better after tripping up on various different edge cases. ChatGPT provided the logging statements, but I was curious why it used the old-style % formatting instead of f-strings…\nWell, today I learned, be careful when using f-strings for logging, as the Python logging library was optimized to use %s formatting style. Although, you can add some extra code if you wish to use f-strings efficiently (see Stack Overflow discussion here and here as well as the Python documentation).\n\n\nClean version of code\nThe final clean version of my function is below. In LeetCode, this beat 68% of submissions on time and 77% on memory, so it’s much improved.\n\nclass CleanSolution:\n    def isPalindrome(self, s: str) -&gt; bool:\n        left = 0\n        right = len(s) - 1\n\n        while left &lt; right:\n            while left &lt; right and not s[left].isalnum():\n                left += 1\n            while right &gt; left and not s[right].isalnum():\n                right -= 1\n            if s[left].lower() != s[right].lower():\n                return False\n            left += 1\n            right -= 1\n        else:\n            return True\n\n\ntest_isPalindrome(CleanSolution)\n\nTests passed\n\n\n\n\nTest function\n\ndef test_isPalindrome(solution_class):\n    \"\"\"Simple test function. Not extensive.\"\"\"\n    test_cases = [\n        {\"input\": \"A man, a plan, a canal: Panama\", \"expected\": True},\n        {\"input\": \" \", \"expected\": True},\n        {\"input\": \".,\", \"expected\": True},\n        {\"input\": \"No lemon, no melon\", \"expected\": True},\n        {\"input\": \"Was it a car or a cat I saw?\", \"expected\": True},\n        {\"input\": \"tab a cat\", \"expected\": False},\n        {\"input\": \"0P\", \"expected\": False},\n        {\"input\": \"a\", \"expected\": True},\n        {\"input\": \"a.\", \"expected\": True},\n    ]\n\n    solution = solution_class()\n    results = [solution.isPalindrome(test_case[\"input\"]) for test_case in test_cases]\n    expected = [test_case[\"expected\"] for test_case in test_cases]\n\n    if results == expected:\n        print(\"Tests passed\")\n    else:\n        print(\"Tests failed\")\n        print(\"Expected:\", expected)\n        print(\"Got:\", results)"
  },
  {
    "objectID": "dsaposts/valid-palindrome/IsPalindrome.html#neetcode-solution",
    "href": "dsaposts/valid-palindrome/IsPalindrome.html#neetcode-solution",
    "title": "LeetCode 125: Valid Palindrome",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\nNeetCode’s solution was essentially the same as above but used the following helper function instead of isalnum(). The complete solution is available via the NeetCode problem page.\n\n# Helper function. Use within the Solution class\ndef alphaNum(self, c):\n    return (\n        ord(\"A\") &lt;= ord(c) &lt;= ord(\"Z\")\n        or ord(\"a\") &lt;= ord(c) &lt;= ord(\"z\")\n        or ord(\"0\") &lt;= ord(c) &lt;= ord(\"9\")\n    )\n\nConclusion\nThe ‘clean’ solution discussed above has the following worst-case complexity:\n\nTime complexity: \\(O(n)\\) as the checks ensure each character is processed only once by either pointer.\nSpace complexity: \\(O(1)\\) to store the pointers. No additional data structures are needed."
  },
  {
    "objectID": "dsaposts/valid-sudoku/ValidSudoku.html",
    "href": "dsaposts/valid-sudoku/ValidSudoku.html",
    "title": "LeetCode 36: Valid Sudoku",
    "section": "",
    "text": "Difficulty: Medium\nYou are given a a 9 x 9 Sudoku board board. A Sudoku board is valid if the following rules are followed:\n\nEach row must contain the digits 1-9 without duplicates.\nEach column must contain the digits 1-9 without duplicates.\nEach of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without duplicates.\n\nReturn true if the Sudoku board is valid, otherwise return false\nNote: A board does not need to be full or be solvable to be valid.\nExamples are provided in the code below.\nConstraints:\nboard.length == 9\nboard[i].length == 9\nboard[i][j] is a digit 1-9 or '.'."
  },
  {
    "objectID": "dsaposts/valid-sudoku/ValidSudoku.html#problem-description",
    "href": "dsaposts/valid-sudoku/ValidSudoku.html#problem-description",
    "title": "LeetCode 36: Valid Sudoku",
    "section": "",
    "text": "Difficulty: Medium\nYou are given a a 9 x 9 Sudoku board board. A Sudoku board is valid if the following rules are followed:\n\nEach row must contain the digits 1-9 without duplicates.\nEach column must contain the digits 1-9 without duplicates.\nEach of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without duplicates.\n\nReturn true if the Sudoku board is valid, otherwise return false\nNote: A board does not need to be full or be solvable to be valid.\nExamples are provided in the code below.\nConstraints:\nboard.length == 9\nboard[i].length == 9\nboard[i][j] is a digit 1-9 or '.'."
  },
  {
    "objectID": "dsaposts/valid-sudoku/ValidSudoku.html#initial-solution",
    "href": "dsaposts/valid-sudoku/ValidSudoku.html#initial-solution",
    "title": "LeetCode 36: Valid Sudoku",
    "section": "Initial Solution",
    "text": "Initial Solution\nOh, man. This will involve a few loops and a lot of conditional statements. There is probably a neat mathematical trick but I’d rather not spend the time trying to figure it out. Let’s brute force the problem…\n\nfrom typing import List\nfrom collections import defaultdict\n\n\nboard1 = [[\"1\",\"2\",\".\",\".\",\"3\",\".\",\".\",\".\",\".\"],\n          [\"4\",\".\",\".\",\"5\",\".\",\".\",\".\",\".\",\".\"],\n          [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\".\",\"3\"],\n          [\"5\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"4\"],\n          [\".\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"5\"],\n          [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],\n          [\".\",\".\",\".\",\".\",\".\",\".\",\"2\",\".\",\".\"],\n          [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"8\"],\n          [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\n# Output: True\n\nboard2 = [[\"1\",\"2\",\".\",\".\",\"3\",\".\",\".\",\".\",\".\"],\n          [\"4\",\".\",\".\",\"5\",\".\",\".\",\".\",\".\",\".\"],\n          [\".\",\"9\",\"1\",\".\",\".\",\".\",\".\",\".\",\"3\"],\n          [\"5\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"4\"],\n          [\".\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"5\"],\n          [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],\n          [\".\",\".\",\".\",\".\",\".\",\".\",\"2\",\".\",\".\"],\n          [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"8\"],\n          [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\n# Output: False (two 1's in the top left 3x3 box)\n\nboard3 = [[\"1\",\"2\",\".\",\".\",\"3\",\".\",\".\",\".\",\".\"],\n          [\"4\",\".\",\".\",\"5\",\".\",\".\",\".\",\".\",\".\"],\n          [\".\",\"9\",\"1\",\".\",\".\",\".\",\".\",\".\",\"3\"],\n          [\"5\",\".\",\"1\",\".\",\"6\",\".\",\".\",\".\",\"4\"],\n          [\".\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"5\"],\n          [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],\n          [\".\",\".\",\".\",\".\",\".\",\".\",\"2\",\".\",\".\"],\n          [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"8\"],\n          [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\n# Output: False (two 1's in column 2)\n\n\nclass Solution:\n    def isValidSudoku(self, board: List[List[str]]) -&gt; bool:\n\n        row_dict = defaultdict(list)\n        columns = defaultdict(list)\n        box = defaultdict(list)\n\n        for i, row in enumerate(board):        \n            for j, value in enumerate(row):\n                # ignore dots       \n                if value == '.':\n                    continue\n                # check if seen in a row before\n                if value in row_dict[i]: return False\n                row_dict[i].append(value)\n\n                # create columns and check if valid\n                if value in columns[j]: return False\n                columns[j].append(value)\n\n                # create 3x3 boxes and check if valid\n                if i &lt; 3:\n                    if j &lt; 3:\n                        if value in box[0]: return False\n                        box[0].append(value)\n                    elif j &lt; 6:\n                        if value in box[1]: return False\n                        box[1].append(value)\n                    else:\n                        if value in box[2]: return False\n                        box[2].append(value)\n                elif i &lt; 6:\n                    if j &lt; 3:\n                        if value in box[3]: return False\n                        box[3].append(value)\n                    elif j &lt; 6:\n                        if value in box[4]: return False\n                        box[4].append(value)\n                    else:\n                        if value in box[5]: return False\n                        box[5].append(value)\n                else:\n                    if j &lt; 3:\n                        if value in box[6]: return False\n                        box[6].append(value)\n                    elif j &lt; 6:\n                        if value in box[7]: return False\n                        box[7].append(value)\n                    else:\n                        if value in box[8]: return False\n                        box[8].append(value)\n        return True\n\n\nboards = [board1, board2, board3]\nexpected = [True, False, False]\nsolution = Solution()\n\n# Get results\nresults = [solution.isValidSudoku(board) for board in boards]\n\n# Compare results with expected values\nif results == expected:\n    print(\"Tests passed\")\nelse:\n    print(\"Tests failed\")\n    print(\"Expected:\", expected)\n    print(\"Got:\", results)\n\nTests passed\n\n\nSummary\n\nTime Complexity: \\(O(1)\\) as the operations are bounded by a fixed number of iterations and checks in a 9x9 Sudoku board (specifically, 81 operations). If the size of the board were variable, the complexity would be \\(O(n^2)\\) due to the nested loop.\nSpace Complexity: \\(O(1)\\) as the space used by the defaultdicts is also bounded by the fixed number of entries in the 9x9 board (specifically, 3 dictionaries each holding up to 81 entries in total)."
  },
  {
    "objectID": "dsaposts/valid-sudoku/ValidSudoku.html#neetcode-solution",
    "href": "dsaposts/valid-sudoku/ValidSudoku.html#neetcode-solution",
    "title": "LeetCode 36: Valid Sudoku",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nclass Solution:\n    def isValidSudoku(self, board: List[List[str]]) -&gt; bool:\n        cols = defaultdict(set)\n        rows = defaultdict(set)\n        squares = defaultdict(set)  # key = (r /3, c /3)\n\n        for r in range(9):\n            for c in range(9):\n                if board[r][c] == \".\":\n                    continue\n                if (\n                    board[r][c] in rows[r]\n                    or board[r][c] in cols[c]\n                    or board[r][c] in squares[(r // 3, c // 3)]\n                ):\n                    return False\n                cols[c].add(board[r][c])\n                rows[r].add(board[r][c])\n                squares[(r // 3, c // 3)].add(board[r][c])\n\n        return True\n\nConclusion\nNeetCode’s solution is a lot cleaner but is essentially the same approach. However, it does use a neat trick of integer division to compute the 3x3 grids. It also uses defaultdict(set) instead defaultdict(list) which will improve the efficiency of the lookup operations."
  },
  {
    "objectID": "dsaposts/string-encode-decode/StringEncodeDecode.html",
    "href": "dsaposts/string-encode-decode/StringEncodeDecode.html",
    "title": "LeetCode 271: String Encode and Decode",
    "section": "",
    "text": "Difficulty: Medium\nDesign an algorithm to encode a list of strings to a single string. The encoded string is then decoded back to the original list of strings.\nPlease implement encode and decode\nExample 1:\nInput: [\"neet\",\"code\",\"love\",\"you\"]\nOutput:[\"neet\",\"code\",\"love\",\"you\"]\nExample 2:\nInput: [\"we\",\"say\",\":\",\"yes\"]\nOutput: [\"we\",\"say\",\":\",\"yes\"]\nConstraints:\n0 &lt;= strs.length &lt; 100\n0 &lt;= strs[i].length &lt; 200\nstrs[i] contains only UTF-8 characters."
  },
  {
    "objectID": "dsaposts/string-encode-decode/StringEncodeDecode.html#problem-description",
    "href": "dsaposts/string-encode-decode/StringEncodeDecode.html#problem-description",
    "title": "LeetCode 271: String Encode and Decode",
    "section": "",
    "text": "Difficulty: Medium\nDesign an algorithm to encode a list of strings to a single string. The encoded string is then decoded back to the original list of strings.\nPlease implement encode and decode\nExample 1:\nInput: [\"neet\",\"code\",\"love\",\"you\"]\nOutput:[\"neet\",\"code\",\"love\",\"you\"]\nExample 2:\nInput: [\"we\",\"say\",\":\",\"yes\"]\nOutput: [\"we\",\"say\",\":\",\"yes\"]\nConstraints:\n0 &lt;= strs.length &lt; 100\n0 &lt;= strs[i].length &lt; 200\nstrs[i] contains only UTF-8 characters."
  },
  {
    "objectID": "dsaposts/string-encode-decode/StringEncodeDecode.html#initial-solution",
    "href": "dsaposts/string-encode-decode/StringEncodeDecode.html#initial-solution",
    "title": "LeetCode 271: String Encode and Decode",
    "section": "Initial Solution",
    "text": "Initial Solution\nNot sure why this was classed as medium. Am I missing something here? (Apparently so). I chose to use a new line \"\\n\" as the delimiter between words, and the submission passed on NeetCode. I’m not sure it would on LeetCode, but it’s a premium problem there, so not free to check.\n\nimport logging\nfrom typing import List\n\n\n# Set up logging config (global scope)\nlogging.basicConfig(level=logging.WARNING, format=\"%(levelname)s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\nclass InitialSolution:\n    \n    def __init__(self):\n        self.logger = logger\n\n    def encode(self, strs: List[str]) -&gt; str:\n        encoded = \"\"\n\n        for string in strs:\n            encoded += string + \"\\n\"\n            self.logger.debug(f\"encoded string: {encoded}\")\n\n        return encoded\n\n    def decode(self, s: str) -&gt; List[str]:\n        decoded = []\n        word = \"\"\n\n        for letter in s:\n            word += letter\n            self.logger.debug(f\"added letter to word; word = {word}\")\n\n            if letter == \"\\n\":\n                decoded.append(word[0:-1])\n                self.logger.debug(f\"added word to final list: {decoded}\")\n                word = \"\"\n\n        return decoded\n\n\nstrs = [\"neet\", \"code\", \"love\", \"you\"]\n# Expected output:[\"neet\",\"code\",\"love\",\"you\"]\n\nsolution = InitialSolution()\n# suppress debugging output to avoid the mess when using '/n'\nlogger.setLevel(logging.WARNING)\n\nstring = solution.encode(strs)\nstring\n\n'neet\\ncode\\nlove\\nyou\\n'\n\n\n\nlogger.setLevel(logging.DEBUG)\nsolution.decode(string)\n\nDEBUG: added letter to word; word = n\nDEBUG: added letter to word; word = ne\nDEBUG: added letter to word; word = nee\nDEBUG: added letter to word; word = neet\nDEBUG: added letter to word; word = neet\n\nDEBUG: added word to final list: ['neet']\nDEBUG: added letter to word; word = c\nDEBUG: added letter to word; word = co\nDEBUG: added letter to word; word = cod\nDEBUG: added letter to word; word = code\nDEBUG: added letter to word; word = code\n\nDEBUG: added word to final list: ['neet', 'code']\nDEBUG: added letter to word; word = l\nDEBUG: added letter to word; word = lo\nDEBUG: added letter to word; word = lov\nDEBUG: added letter to word; word = love\nDEBUG: added letter to word; word = love\n\nDEBUG: added word to final list: ['neet', 'code', 'love']\nDEBUG: added letter to word; word = y\nDEBUG: added letter to word; word = yo\nDEBUG: added letter to word; word = you\nDEBUG: added letter to word; word = you\n\nDEBUG: added word to final list: ['neet', 'code', 'love', 'you']\n\n\n['neet', 'code', 'love', 'you']\n\n\nHere’s a cleaner version that has the same approach. A few if statements are needed to account for edge cases on LeetCode. However, these cause an inconsistency between the type hints in the function definition and the actual type of the variables. The encode function now returns Union[List[str], str] rather than the original str. Not sure if that would be acceptable either.\n\nclass AlternativeSolution:\n\n    def encode(self, strs: List[str]) -&gt; str:  # incorrect type annotation\n        if strs == [\"\"]:\n            return [\"\"]\n        if strs == []:\n            return []\n        return \"\\n\".join(strs)\n\n    def decode(self, s: str) -&gt; List[str]:  # incorrect type annotation\n        if s == [\"\"]:\n            return [\"\"]\n        if s == []:\n            return []\n        return s.split(\"\\n\")"
  },
  {
    "objectID": "dsaposts/string-encode-decode/StringEncodeDecode.html#initial-results",
    "href": "dsaposts/string-encode-decode/StringEncodeDecode.html#initial-results",
    "title": "LeetCode 271: String Encode and Decode",
    "section": "Initial Results",
    "text": "Initial Results\nMy intial solution passed the LeetCode submission. I had a little trouble determining the worst-case complexity here. It appears that the intial encode function may be \\(O(n^2)\\) due to the repeated cost of string concatenation*. The alternative solution may avoid the quadratic time by using use Python’s better optimized join operation.\n*Per this source:\n\nString concatenation is best done with ’‘.join(seq) which is an \\(O(n)\\) process. In contrast, using the’+’ or ‘+=’ operators can result in an \\(O(n^2)\\) process because new strings may be built for each intermediate step. The CPython 2.4 interpreter mitigates this issue somewhat; however, ’’.join(seq) remains the best practice.\n\nThe decode function appears to be \\(O(n)\\) due to the linear nature of processing each character exactly once."
  },
  {
    "objectID": "dsaposts/string-encode-decode/StringEncodeDecode.html#neetcode-solution",
    "href": "dsaposts/string-encode-decode/StringEncodeDecode.html#neetcode-solution",
    "title": "LeetCode 271: String Encode and Decode",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nclass Solution:\n\n    def encode(self, strs: List[str]) -&gt; str:\n        res = \"\"\n        for s in strs:\n            res += str(len(s)) + \"#\" + s\n        return res\n\n    def decode(self, s: str) -&gt; List[str]:\n        res = []\n        i = 0\n\n        while i &lt; len(s):\n            j = i\n            while s[j] != \"#\":\n                j += 1\n            length = int(s[i:j])\n            i = j + 1\n            j = i + length\n            res.append(s[i:j])\n            i = j\n\n        return res\n\nConclusion\nAlthough my original code passes, apparently it’s against the spirit of the exercise. A simple delimiter could suffer from ‘delimiter collision’. One solution to the delimiter collision problem is use an escape character (‘\\’ is the escape character for new line). I tried a few and NeetCode accepted the ASCII escape character ‘\\033’ but not ‘’, suggesting we got lucky with ‘’ not appearing in the test cases.\nNeetCode’s solution to the encode-decode problem uses a simple length-prefixing approach instead of an escape character. The encode method takes the list of strings and concatenates each string with its length prefixed by a separator “#”. The decode method then reverses the encoding process, parsing the encoded string by iterating through it, extracting each string based on its prefixed length and the separator.\n\nTime complexity: encode is possibly \\(O(n^2)\\); see above. decode is \\(O(n)\\)\nSpace complexity: both \\(O(n)\\)"
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html",
    "title": "LeetCode 49: Group Anagrams",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of strings strs, group all anagrams together into sublists. You may return the output in any order.\nExample 1:\nInput: strs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\nOutput: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\nExample 2:\nInput: strs = [\"x\"]\nOutput: [[\"x\"]]\nExample 3:\nInput: strs = [\"\"]\nOutput: [[\"\"]]"
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html#problem-description",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html#problem-description",
    "title": "LeetCode 49: Group Anagrams",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of strings strs, group all anagrams together into sublists. You may return the output in any order.\nExample 1:\nInput: strs = [\"act\",\"pots\",\"tops\",\"cat\",\"stop\",\"hat\"]\nOutput: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\nExample 2:\nInput: strs = [\"x\"]\nOutput: [[\"x\"]]\nExample 3:\nInput: strs = [\"\"]\nOutput: [[\"\"]]"
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html#initial-solution",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html#initial-solution",
    "title": "LeetCode 49: Group Anagrams",
    "section": "Initial Solution",
    "text": "Initial Solution\nThe key to this problem was to recognize that when anagrams are sorted alphabetically, they become the same ‘word’. With that in mind, a decent solution seemed relatively easy to come up with. Let’s just sort the words and place the original word in a dictionary with the keys being the unique sorted/garbled word.\nThere are a few parts that could potentially trip up a beginner:\nFirstly, calling sorted() on a string returns a list of the sorted characters and not a whole sorted word:\nword = 'hello'\nsorted_word = sorted(word)\nprint(sorted_word)\n&gt;&gt;&gt; ['e', 'h', 'l', 'l', 'o']\nSo we must join() the list back together afterwards.\nSecondly, adding values to a pre-existing key in the dictionary, instead of overwriting them, takes a little care, but there are a few options noted in this post. For example, using append() and extend(). The TLDR is to use extend() when you have multiple values to append, rather than using append() multiple times. E.g.\na = {}\na.setdefault('abc', []).append(1)       # {'abc': [1]}\na.setdefault('abc', []).extend([2, 3])  # a is now {'abc': [1, 2, 3]}\nsetdefault() will avoid a KeyError when we try to access a key that does not exist yet. It inserts the key with the specified default: []\nLet’s put this all together in our initial attempt at solving the anagram problem.\n\nimport logging\nfrom typing import List\n\n\nclass InitialSolution:\n    def __init__(self):\n        # Set up logging config\n        logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s: %(message)s\")\n        self.logger = logging.getLogger(__name__)\n\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        # works fine without the if statement, but let's save some later computation:\n        if strs == [\"\"]:\n            return [[\"\"]]\n\n        words = {}\n        for word in strs:\n            self.logger.debug(f\"Processing word: {word}\")\n            sorted_word = \"\".join(sorted(word))\n            # n⋅log(n) character sorting * m strings in the list\n            self.logger.debug(f\"Sorted 'word': {sorted_word}\")\n            words.setdefault(sorted_word, []).append(word)\n            self.logger.debug(\n                f\"Updated dict value(s): '{sorted_word}': {words[sorted_word]}\"\n            )\n\n        return list(words.values())\n\n\n# Example for testing purposes.\nstrs = [\"act\", \"pots\", \"tops\", \"cat\", \"stop\", \"hat\"]\n# Output: [[\"hat\"],[\"act\", \"cat\"],[\"stop\", \"pots\", \"tops\"]]\n# Note that output order does not matter.\n\nsolution = InitialSolution()\nsolution.groupAnagrams(strs)\n\nDEBUG: Processing word: act\nDEBUG: Sorted 'word': act\nDEBUG: Updated dict value(s): 'act': ['act']\nDEBUG: Processing word: pots\nDEBUG: Sorted 'word': opst\nDEBUG: Updated dict value(s): 'opst': ['pots']\nDEBUG: Processing word: tops\nDEBUG: Sorted 'word': opst\nDEBUG: Updated dict value(s): 'opst': ['pots', 'tops']\nDEBUG: Processing word: cat\nDEBUG: Sorted 'word': act\nDEBUG: Updated dict value(s): 'act': ['act', 'cat']\nDEBUG: Processing word: stop\nDEBUG: Sorted 'word': opst\nDEBUG: Updated dict value(s): 'opst': ['pots', 'tops', 'stop']\nDEBUG: Processing word: hat\nDEBUG: Sorted 'word': aht\nDEBUG: Updated dict value(s): 'aht': ['hat']\n\n\n[['act', 'cat'], ['pots', 'tops', 'stop'], ['hat']]\n\n\nInitial Results\nIntialSolution passes NeetCode submission (if we comment out the logging).\n\nTime complexity: \\(O(m⋅n⋅log(n))\\) due to sorting each string.\nSpace complexity: \\(O(m⋅n)\\) due to storing the strings in the dictionary keys and values."
  },
  {
    "objectID": "dsaposts/group-anagrams/GroupAnagrams.html#neetcode-solution",
    "href": "dsaposts/group-anagrams/GroupAnagrams.html#neetcode-solution",
    "title": "LeetCode 49: Group Anagrams",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nfrom collections import defaultdict\n\n\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        result = defaultdict(list)\n\n        for s in strs:\n            count = [0] * 26  # one for each character in a-z\n            for c in s:\n                # use ord to get unicode value of the character\n                count[ord(c) - ord(\"a\")] += 1\n            # convert count list to tuple, as lists can't be dict keys\n            result[tuple(count)].append(s)\n        return result.values()\n\n\nsolution = Solution()\nlist(solution.groupAnagrams(strs))\n\n[['act', 'cat'], ['pots', 'tops', 'stop'], ['hat']]\n\n\nI used list() above to display the result nicely in the notebook. The code seems to work fine in NeetCode both with and without the conversion to a list; no TypeError is given.\nConclusion\nNeetcode’s solution uses a hashmap (defaultdict) called result that stores lists of anagrams, where the key is a tuple representing the character counts of the strings. Note that a defaultdict never raises a KeyError, but instead provides a default value for a key that does not exist.\n\nTime complexity \\(O(m⋅n)\\) where m is the total number of strings and n is the length of each string (multiplied by 26 possible letters)\nSpace complexity: \\(O(m⋅n)\\) due to the space needed for storing the strings in the defaultdict values.\n\nGiven an extremely long string, the above solution would be optimal: \\(O(m⋅n⋅log(n))\\) vs. \\(O(m⋅n⋅26)\\)"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html",
    "href": "dsaposts/contains-duplicate/index.html",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "",
    "text": "Difficulty: Easy\nGiven an integer array nums, return True if any value appears more than once in the array, otherwise return False.\nExample 1:\nInput: nums = [1, 2, 3, 3]\nOutput: True\nExample 2:\nInput: nums = [1, 2, 3, 4]\nOutput: False"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#problem-description",
    "href": "dsaposts/contains-duplicate/index.html#problem-description",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "",
    "text": "Difficulty: Easy\nGiven an integer array nums, return True if any value appears more than once in the array, otherwise return False.\nExample 1:\nInput: nums = [1, 2, 3, 3]\nOutput: True\nExample 2:\nInput: nums = [1, 2, 3, 4]\nOutput: False"
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#initial-solution",
    "href": "dsaposts/contains-duplicate/index.html#initial-solution",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "Initial Solution",
    "text": "Initial Solution\nThis was really quite easy, using the Python set() function to create a set of unique numbers. We just need to ensure it returns True if the amount (length) of numbers in the set is different to that of the original list. Hence, the not operator is used below. Alternatively, replace == with != for the same result.\nNote that set() will not retain the order of the list, so we must compare its overall length and not try to compare the content one-by-one (or convert it to a list and see if the two lists match exactly).\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def hasDuplicate(self, nums: List[int]) -&gt; bool:\n        return not len(set(nums)) == len(nums)\n\n\nnums1 = [1, 2, 3, 3]\nnums2 = [1, 2, 3, 4]\n\n\nsolution = InitialSolution()\nprint(solution.hasDuplicate(nums1))\nprint(solution.hasDuplicate(nums2))\n\nInitial Results\nSuccess. This solution passes the full suite of tests on NeetCode."
  },
  {
    "objectID": "dsaposts/contains-duplicate/index.html#neetcode-solution",
    "href": "dsaposts/contains-duplicate/index.html#neetcode-solution",
    "title": "LeetCode 217: Contains Duplicate",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\nThe following code makes it clear that we are using a hashset to store the values. It does not create the entire set all at once (like we do above, always giving O(n) time complexity as it runs through the whole list). Instead, it goes step by step; thus only in the worst case would this be O(n). We must create the hashset, which in the worst case uses O(n) space.\n\nclass Solution:\n    def hasDuplicate(self, nums: List[int]) -&gt; bool:\n        hashset = set()\n        for n in nums:\n            if n in hashset:\n                return True\n            hashset.add(n)\n        return False\n\n\nsolution = Solution()\nprint(solution.hasDuplicate(nums1))\nprint(solution.hasDuplicate(nums2))\n\nConclusion\nAn easy start to NeetCode problems. The solution has a worst-case complexity of:\n\nTime complexity: O(n)\nSpace complexity: O(n)"
  },
  {
    "objectID": "dsaposts/welcome/index.html",
    "href": "dsaposts/welcome/index.html",
    "title": "Welcome To Data Structures & Algorithms",
    "section": "",
    "text": "Welcome! This is a repository of my solutions to LeetCode/NeetCode problems.\nThe general format consists of my initial attempt at solving the problem, followed by the solution given by NeetCode. My solutions will often not be optimal and may contain beginner-level mistakes, incorrect analyses of complexity, or non-standard formatting. After all, this was intended as a learning exercise for me. Comments and corrections are welcome!\nThe NeetCode roadmap is a fantastic resource for learning to code and highly recommended."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html",
    "href": "dsaposts/two-sum-integer/index.html",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "",
    "text": "Difficulty: Easy\nGiven an array of integers nums and an integer target, return the indices i and j such that nums[i] + nums[j] == target and i != j.\nAssume that every input has exactly one pair of indices i and j that satisfy the condition.\nReturn the answer with the smaller index first.\nExample:\nInput: \nnums = [3,4,5,6], target = 7\nOutput: [0,1]\n# Explanation: nums[0] + nums[1] == 7, so we return [0, 1].\nAdditional examples are provided in the tests below."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#problem-description",
    "href": "dsaposts/two-sum-integer/index.html#problem-description",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "",
    "text": "Difficulty: Easy\nGiven an array of integers nums and an integer target, return the indices i and j such that nums[i] + nums[j] == target and i != j.\nAssume that every input has exactly one pair of indices i and j that satisfy the condition.\nReturn the answer with the smaller index first.\nExample:\nInput: \nnums = [3,4,5,6], target = 7\nOutput: [0,1]\n# Explanation: nums[0] + nums[1] == 7, so we return [0, 1].\nAdditional examples are provided in the tests below."
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#initial-solution",
    "href": "dsaposts/two-sum-integer/index.html#initial-solution",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "Initial Solution",
    "text": "Initial Solution\n\nfrom typing import List, Callable\n\n\nclass InitialSolution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        for i, i_num in enumerate(nums):\n            for j, j_num in enumerate(nums[1:]): \n                if i_num + j_num == target and i != j+1: # must 'and' to ensure i != j \n                    return [i,j+1]\n        return [] # not really needed as we are told a solution exists\n\nNote: I first tried to ensure i != j by starting the for j loop at [1:] instead of 0. This failed the following case, which led me to add the and operator in the equality check, at which point it was apparent the slicing was unnecessary. Careful reading of the question indicates that the indices are not allowed to be equal, but there is no restriction on the numbers being equal.\nnums = [2,5,5,11]\ntarget = 10\noutput = [1,2]\nLet’s clean up the function:\n\nclass BruteForceSolution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        for i, i_num in enumerate(nums):\n            for j, j_num in enumerate(nums): \n                if i_num + j_num == target and i != j:\n                    return [i,j]\n        return []\n\n\n# Examples for testing purposes.\n#nums, target = [3,4,5,6], 7 # output [0,1]\n#nums, target = [4,5,6], 10 # output [0,2]\n#nums, target = [3,3], 6 # output [0,1] \n#nums, target = [3,2,4], 6 # output [1,2]\nnums, target = [2,5,5,11], 10 # output [1,2] # tricky\n\n#solution = InitialSolution()\nsolution = BruteForceSolution()\nsolution.twoSum(nums, target)\n\nInitial Results\nBruteForceSolution passes NeetCode submission.\n\nTime Complexity: O(n)*O(n) = O(n2)"
  },
  {
    "objectID": "dsaposts/two-sum-integer/index.html#tests",
    "href": "dsaposts/two-sum-integer/index.html#tests",
    "title": "LeetCode 1: Two Integer Sum",
    "section": "Tests",
    "text": "Tests\n\ndef test(fn: Callable[[List[int], int], List[int]]) -&gt; None:\n    nums, target = [3, 4, 5, 6], 7\n    output = [0, 1]\n    assert output == fn(nums, target)\n    \n    nums, target = [4, 5, 6], 10\n    output = [0, 2]\n    assert output == fn(nums, target)\n    \n    nums, target = [3, 3], 6\n    output = [0, 1]\n    assert output == fn(nums, target)\n    \n    nums, target = [3, 2, 4], 6\n    output = [1, 2]\n    assert output == fn(nums, target)\n    \n    nums, target = [2, 5, 5, 11], 10\n    output = [1, 2]\n    assert output == fn(nums, target)\n    print('Tests Passed')\n\n\nsolution = BruteForceSolution()\ntest(solution.twoSum)"
  },
  {
    "objectID": "dsaposts/top-k-elements/TopKElementsinList.html",
    "href": "dsaposts/top-k-elements/TopKElementsinList.html",
    "title": "LeetCode 347: Top K Frequent Elements",
    "section": "",
    "text": "Difficulty: Medium\nGiven an integer array nums and an integer k, return the k most frequent elements within the array.\nThe test cases are generated such that the answer is always unique.\nYou may return the output in any order.\nExample 1:\nInput: nums = [1,2,2,3,3,3], k = 2\nOutput: [2,3]\nExample 2:\nInput: nums = [7,7], k = 1\nOutput: [7]"
  },
  {
    "objectID": "dsaposts/top-k-elements/TopKElementsinList.html#problem-description",
    "href": "dsaposts/top-k-elements/TopKElementsinList.html#problem-description",
    "title": "LeetCode 347: Top K Frequent Elements",
    "section": "",
    "text": "Difficulty: Medium\nGiven an integer array nums and an integer k, return the k most frequent elements within the array.\nThe test cases are generated such that the answer is always unique.\nYou may return the output in any order.\nExample 1:\nInput: nums = [1,2,2,3,3,3], k = 2\nOutput: [2,3]\nExample 2:\nInput: nums = [7,7], k = 1\nOutput: [7]"
  },
  {
    "objectID": "dsaposts/top-k-elements/TopKElementsinList.html#initial-solution",
    "href": "dsaposts/top-k-elements/TopKElementsinList.html#initial-solution",
    "title": "LeetCode 347: Top K Frequent Elements",
    "section": "Initial Solution",
    "text": "Initial Solution\nMy first thought on seeing this is that it’s such a common problem that no doubt someone has already written an efficient topK algorithm. Pandas has an nlargest() function, maybe that would work? (not without some wrangling). Therefore, I decided I’d make a super simple algorithm without using any external libraries or googling for helpful functions.\nAfter completing the exercise, I learned that heapq.nlargest() is probably that helpful function you’re looking for, e.g., as shown in the ‘two-liner’ solution below. However, the most efficient answer is really nice, with O(n) time. We’ll see later…\n\n# imports for the initial attempt\nimport heapq\nimport logging\n\n# imports for the two-liner solution mentioned above\nfrom collections import Counter, defaultdict\nfrom typing import Callable, List\n\n\n# Set up logging config (global scope)\nlogging.basicConfig(level=logging.WARNING, format=\"%(levelname)s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\nclass InitialSolution:\n    def __init__(self):\n        self.logger = logger\n\n    def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:\n        count_dict = defaultdict(int)\n        self.logger.debug(f\"dict at initilization: {dict(count_dict)}\")\n        for num in nums:\n            count_dict[num] += 1\n            self.logger.debug(f\"dict updated: {dict(count_dict)}\")\n\n        top_k_keys = []\n        for _ in range(k):\n            highest_freq = -1\n            most_freq_key = None  # Could use max() to determine this instead.\n            for key, freq in count_dict.items():  # nested loop results in O(k⋅n) time\n                if freq &gt; highest_freq:\n                    most_freq_key = key\n                    highest_freq = freq\n            self.logger.debug(\n                f\"most frequent key: {most_freq_key}, frequency: {highest_freq}\"\n            )\n            top_k_keys.append(most_freq_key)\n            count_dict.pop(most_freq_key)\n            self.logger.debug(\n                f\"popped off most frequent and updated dict: {dict(count_dict)}\"\n            )\n            self.logger.debug(f\"current top k: {top_k_keys}\")\n\n        return top_k_keys\n\n    # Second attempt added here for testing:\n    def topKTwoLiner(self, nums: List[int], k: int) -&gt; List[int]:\n        count = Counter(nums)\n        return heapq.nlargest(k, count, key=count.get)\n\n\n# Example for testing purposes. Order of output does not matter.\nnums, k = [1, 2, 2, 3, 3, 3], 2  # Output: [2,3]\nsolution = InitialSolution()\nlogger.setLevel(logging.DEBUG)\nsolution.topKFrequent(nums, k)\n\nDEBUG: dict at initilization: {}\nDEBUG: dict updated: {1: 1}\nDEBUG: dict updated: {1: 1, 2: 1}\nDEBUG: dict updated: {1: 1, 2: 2}\nDEBUG: dict updated: {1: 1, 2: 2, 3: 1}\nDEBUG: dict updated: {1: 1, 2: 2, 3: 2}\nDEBUG: dict updated: {1: 1, 2: 2, 3: 3}\nDEBUG: most frequent key: 3, frequency: 3\nDEBUG: popped off most frequent and updated dict: {1: 1, 2: 2}\nDEBUG: current top k: [3]\nDEBUG: most frequent key: 2, frequency: 2\nDEBUG: popped off most frequent and updated dict: {1: 1}\nDEBUG: current top k: [3, 2]\n\n\n[3, 2]"
  },
  {
    "objectID": "dsaposts/top-k-elements/TopKElementsinList.html#tests",
    "href": "dsaposts/top-k-elements/TopKElementsinList.html#tests",
    "title": "LeetCode 347: Top K Frequent Elements",
    "section": "Tests",
    "text": "Tests\n\ndef test(fn: Callable[[List[int], int], List[int]]) -&gt; None:\n    test_cases = [\n        # [nums], k, [expected]\n        ([1, 2, 2, 3, 3, 3], 2, [2, 3]),\n        ([1, 1, 1, 2, 2, 3], 2, [1, 2]),\n        ([-1, -1, -1, 2, 2, 3], 2, [-1, 2]),\n        ([7, 7], 1, [7]),\n        ([4, 4, 4, 6, 6, 2, 2, 2], 2, [4, 2]),\n    ]\n\n    # Set logging level to suppress debug output during tests\n    logger.setLevel(logging.WARNING)\n\n    for nums, k, expected in test_cases:\n        output = fn(nums, k)\n        assert set(output) == set(\n            expected\n        ), f\"Test failed for nums={nums}, k={k}. Expected {expected}, but got {output}\"\n        # set() ensures that the comparison is order independent\n\n    print(\"Tests Passed\")\n\n\n# Set logging level suppress debug output during tests\nlogger.setLevel(logging.WARNING)\n\nsolution = InitialSolution()\ntest(solution.topKFrequent)\ntest(solution.topKTwoLiner)\n\nTests Passed\nTests Passed"
  },
  {
    "objectID": "dsaposts/top-k-elements/TopKElementsinList.html#initial-results",
    "href": "dsaposts/top-k-elements/TopKElementsinList.html#initial-results",
    "title": "LeetCode 347: Top K Frequent Elements",
    "section": "Initial Results",
    "text": "Initial Results\nMy intial solution passed the LeetCode/NeetCode submission. However, the runtime was super slow and beats only 6% of entries, although top 94% for memory.\nWe could speed it up a little by using Python’s in-built max() function to find the most frequent key, but we’d be better off with a different approach. For example, by sorting the dictionary and slicing off the top k elements of the sorted dictionary.\nThe solution has the following worst-case complexity:\n\nTime complexity: \\(O(n + k⋅n)\\); if \\(k = n\\) this becomes \\(O(n^2)\\).\nSpace complexity: \\(O(n)\\) comprising \\(O(n)\\) for the dictionary plus \\(O(k)\\) to store the list of top k elements."
  },
  {
    "objectID": "dsaposts/top-k-elements/TopKElementsinList.html#neetcode-solution",
    "href": "dsaposts/top-k-elements/TopKElementsinList.html#neetcode-solution",
    "title": "LeetCode 347: Top K Frequent Elements",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:\n        count = {}\n        freq = [[] for i in range(len(nums) + 1)]\n\n        for n in nums:\n            count[n] = 1 + count.get(n, 0)\n        for n, c in count.items():\n            freq[c].append(n)\n\n        res = []\n        for i in range(len(freq) - 1, 0, -1):\n            for n in freq[i]:\n                res.append(n)\n                if len(res) == k:\n                    return res\n\n\nsolution = Solution()\ntest(solution.topKFrequent)\n\nTests Passed\n\n\nConclusion\nNeetcode’s solution is very nice. The key insight is the use of bucket sort, which allows it to avoid typical O(n⋅log n) sorting and instead achieves linear time complexity. Note that freq has a length of len(nums) + 1, to accommodate frequencies from 0 to the maximum possible frequency (which is at most the length of the input). By using the frequency as an index into the freq list, it creates a natural ordering of elements by their frequencies without explicit sorting.\n\nTime complexity: \\(O(n)\\)\nSpace complexity: \\(O(n)\\) comprising \\(O(n + n + k)\\)"
  },
  {
    "objectID": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html",
    "href": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html",
    "title": "LeetCode 238: Product of Array Except Self",
    "section": "",
    "text": "Difficulty: Medium\nGiven an integer array nums, return an array output where output[i] is the product of all the elements of nums except nums[i].\nEach product is guaranteed to fit in a 32-bit integer.\nLeetCode version: You must write an algorithm that runs in \\(O(n)\\) time and without using the division operation.\nNeetCode version: Follow-up: Could you solve it in \\(O(n)\\) time without using the division operation?\nExample 1:\nInput: nums = [1,2,4,6]\nOutput: [48,24,12,8]\nExample 2:\nInput: nums = [-1,0,1,2,3]\nOutput: [0,-6,0,0,0]\nConstraints:\n 2 &lt;= nums.length &lt;= 1000\n-20 &lt;= nums[i] &lt;= 20"
  },
  {
    "objectID": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html#problem-description",
    "href": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html#problem-description",
    "title": "LeetCode 238: Product of Array Except Self",
    "section": "",
    "text": "Difficulty: Medium\nGiven an integer array nums, return an array output where output[i] is the product of all the elements of nums except nums[i].\nEach product is guaranteed to fit in a 32-bit integer.\nLeetCode version: You must write an algorithm that runs in \\(O(n)\\) time and without using the division operation.\nNeetCode version: Follow-up: Could you solve it in \\(O(n)\\) time without using the division operation?\nExample 1:\nInput: nums = [1,2,4,6]\nOutput: [48,24,12,8]\nExample 2:\nInput: nums = [-1,0,1,2,3]\nOutput: [0,-6,0,0,0]\nConstraints:\n 2 &lt;= nums.length &lt;= 1000\n-20 &lt;= nums[i] &lt;= 20"
  },
  {
    "objectID": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html#initial-solution",
    "href": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html#initial-solution",
    "title": "LeetCode 238: Product of Array Except Self",
    "section": "Initial Solution",
    "text": "Initial Solution\nThis is known as “Products of Array Discluding Self” on NeetCode. The problem there allows you to use the division operation, so I went with the easy route first.\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def productExceptSelf(self, nums: List[int]) -&gt; List[int]:\n        product = 1\n        contains_zero = False\n        zeros = []\n\n        for i, n in enumerate(nums):\n            if n == 0:\n                contains_zero = True\n                zeros.append(i)\n            else:\n                product *= n\n\n        if not contains_zero:\n            return [product // n for n in nums]\n\n        # If more than one zero, all products will be zero:\n        if len(zeros) &gt; 1:\n            return [0] * len(nums)\n\n        # If exactly one zero\n        return [0 if i not in zeros else product for i, _ in enumerate(nums)]\n\n\nnums_tests = [\n    ([1, 2, 4, 6], [48, 24, 12, 8]),\n    ([-1, 0, 1, 2, 3], [0, -6, 0, 0, 0]),\n    ([0, 0], [0, 0])\n]\n\nsolution = InitialSolution()\n\nfor nums, expected_output in nums_tests:\n    result = solution.productExceptSelf(nums)\n    print(f\"Input: {nums}\\nExpected: {expected_output}\\nOutput: {result}\\n\")\n\nInput: [1, 2, 4, 6]\nExpected: [48, 24, 12, 8]\nOutput: [48, 24, 12, 8]\n\nInput: [-1, 0, 1, 2, 3]\nExpected: [0, -6, 0, 0, 0]\nOutput: [0, -6, 0, 0, 0]\n\nInput: [0, 0]\nExpected: [0, 0]\nOutput: [0, 0]\n\n\n\nWell, my code became a little convoluted trying to deal with the edge cases involving zeros. I tried it on LeetCode, not realizing that division was not allowed. Somehow it was accepted and beat 81% of submissions for time and 92% for memory.\nNo doubt there is a better way to deal with the zeros; maybe just count them up as we go, rather than flag their presence. That way we could break out of the loop early to return [0] * len(nums) as soon as we see two zeros.\n\nTime complexity: \\(O(n)\\) for the intial loop + \\(O(n)\\) for the results = \\(O(n)\\) overall\nSpace complexity: \\(O(n)\\) for the results list and potentially for the zeros list too if nums is all 0.\n\nLeetCode analyzed this as using \\(O(1)\\) space, but considers that “The output array does not count as extra space for space complexity analysis”.\nAfter reading youtube comments on this problem, I learned that LeetCode will apparently reject a solution that contains the division operator. My solution used integer division // which somehow escaped this rule. Others complained that Leetcode would also reject multiple subtraction operations, so I really don’t understand what’s going on. I guess they relaxed the requirements recently. Relatedly, some bright spark on youtube suggested using math.pow(num,-1) to hack the requirements 😄"
  },
  {
    "objectID": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html#neetcode-solution",
    "href": "dsaposts/product-array-except-self/ProductArrayExceptSelf.html#neetcode-solution",
    "title": "LeetCode 238: Product of Array Except Self",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nclass Solution:\n    def productExceptSelf(self, nums: List[int]) -&gt; List[int]:\n        res = [1] * (len(nums))\n\n        for i in range(1, len(nums)):\n            res[i] = res[i-1] * nums[i-1]\n        postfix = 1\n        for i in range(len(nums) - 1, -1, -1):\n            res[i] *= postfix\n            postfix *= nums[i]\n        return res\n\nConclusion\nNeetCode’s solution computes the product of all elements in the array except the current element using a two-pass approach. It uses prefix and postfix multiplication techniques to achieve this in linear time without using division."
  },
  {
    "objectID": "dsaposts/longest_consec_seq/LongestConsecSeq.html",
    "href": "dsaposts/longest_consec_seq/LongestConsecSeq.html",
    "title": "LeetCode 128: Longest Consecutive Sequence",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of integers nums, return the length of the longest consecutive sequence of elements.\nA consecutive sequence is a sequence of elements in which each element is exactly 1 greater than the previous element.\nWrite an algorithm that runs in \\(O(n)\\) time.\nExamples:\nInput: nums = [2,20,4,10,3,4,5]\nOutput: 4\nExplanation: The longest consecutive sequence is [2, 3, 4, 5].\n\nInput: nums = [0,3,2,5,4,6,1,1]\nOutput: 7\nConstraints:\n0 &lt;= nums.length &lt;= 1000 on NeetCode or &lt;= 10^5 on LeetCode\n-10^9 &lt;= nums[i] &lt;= 10^9"
  },
  {
    "objectID": "dsaposts/longest_consec_seq/LongestConsecSeq.html#problem-description",
    "href": "dsaposts/longest_consec_seq/LongestConsecSeq.html#problem-description",
    "title": "LeetCode 128: Longest Consecutive Sequence",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of integers nums, return the length of the longest consecutive sequence of elements.\nA consecutive sequence is a sequence of elements in which each element is exactly 1 greater than the previous element.\nWrite an algorithm that runs in \\(O(n)\\) time.\nExamples:\nInput: nums = [2,20,4,10,3,4,5]\nOutput: 4\nExplanation: The longest consecutive sequence is [2, 3, 4, 5].\n\nInput: nums = [0,3,2,5,4,6,1,1]\nOutput: 7\nConstraints:\n0 &lt;= nums.length &lt;= 1000 on NeetCode or &lt;= 10^5 on LeetCode\n-10^9 &lt;= nums[i] &lt;= 10^9"
  },
  {
    "objectID": "dsaposts/longest_consec_seq/LongestConsecSeq.html#initial-solution",
    "href": "dsaposts/longest_consec_seq/LongestConsecSeq.html#initial-solution",
    "title": "LeetCode 128: Longest Consecutive Sequence",
    "section": "Initial Solution",
    "text": "Initial Solution\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def longestConsecutive(self, nums: List[int]) -&gt; int:\n        if not nums:\n            return 0\n\n        nums_set = set(nums)  # set() is O(n) time\n        longest_seq = 0\n\n        for num in nums_set:\n            if num - 1 not in nums_set:  # fails LeetCode without this\n                current_num = num  # unnecessary new variable\n                current_seq = 1\n\n                while current_num + 1 in nums_set:\n                    current_num += 1\n                    current_seq += 1\n\n                longest_seq = max(longest_seq, current_seq)\n\n        return longest_seq\n\n\nTest function\n\ndef test_longest_consecutive(solution_class):\n    \"\"\"Simple test function. Not extensive.\"\"\"\n    nums_empty = []\n    nums0 = [0]\n    nums1 = [2, 20, 4, 10, 3, 4, 5]\n    nums2 = [0, 3, 2, 5, 4, 6, 1, 1]\n    nums3 = [0, 3, 7, 2, 5, 8, 4, 6, 0, 1]\n\n    nums_list = [nums_empty, nums0, nums1, nums2, nums3]\n    expected = [0, 1, 4, 7, 9]\n\n    solution = solution_class()\n    results = [solution.longestConsecutive(nums) for nums in nums_list]\n\n    if results == expected:\n        print(\"Tests passed\")\n    else:\n        print(\"Tests failed\")\n        print(\"Expected:\", expected)\n        print(\"Got:\", results)\n\n\ntest_longest_consecutive(InitialSolution)\n\nTests passed\n\n\nIntial Results\nThis seemed hard for a medium-level problem. I struggled to create something that didn’t involve sorting, as most sorting algorithms are \\(n⋅log(n)\\) or \\(n^2\\) time complexity. Radix and count sort are notable exceptions. My solution must be a pretty decent one as it beats 68% of submissions for run time, although it only beats 18% for memory. As we see later in NeetCode’s solution, mine has a few extra variables adding to the memory usage.\nI needed to add the check to see if the number was already part of a sequence (if num - 1 not in nums_set:) to get it to pass LeetCode. The constraints are looser on NeetCode, so the original attempt was fine there, although slow. On LeetCode, it failed without the check (timed out) when given a really long consecutive sequence of length 100000. The key piece is that little check, as without it the time complexity is \\(O(n⋅m)\\), where \\(n\\) is the number of elements in nums, and \\(m\\) is the maximum length of a consecutive sequence. In the worst case, this is \\(O(n^2)\\) if the maximum length is a single long sequence of length \\(n\\).\n\nTime complexity: \\(O(n)\\) comprising \\(O(n)\\) for the set operation and \\(O(n)\\) to iterate over the set and perform checks.\nSpace complexity: \\(O(n)\\) predominantly due to storing the set."
  },
  {
    "objectID": "dsaposts/longest_consec_seq/LongestConsecSeq.html#neetcode-solution",
    "href": "dsaposts/longest_consec_seq/LongestConsecSeq.html#neetcode-solution",
    "title": "LeetCode 128: Longest Consecutive Sequence",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\n\nclass Solution:\n    def longestConsecutive(self, nums: List[int]) -&gt; int:\n        numSet = set(nums)\n        longest = 0\n\n        for n in numSet:\n            if (n - 1) not in numSet:\n                length = 1\n                while (n + length) in numSet:\n                    length += 1\n                longest = max(length, longest)\n        return longest\n\n\ntest_longest_consecutive(Solution)\n\nTests passed\n\n\nConclusion\nNeetCode’s solution is cleaner but essentially the same approach. It simplifies the tracking of the current sequence length within the while loop, saving a small amount of memory (it beats around 63% of submissions on memory)."
  },
  {
    "objectID": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html",
    "href": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html",
    "title": "LeetCode 167: Two Sum II - Input Array Is Sorted",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of integers numbers that is sorted in non-decreasing order.\nReturn the 1-indexed indices of two numbers, [index1, index2], such that they add up to a given target number target and index1 &lt; index2. Note that index1 and index2 cannot be equal; therefore, you may not use the same element twice.\nThere will always be exactly one valid solution.\nThe solution must use \\(O(1)\\) additional space.\nExample 1:\nInput: numbers = [1,2,3,4], target = 3\nOutput: [1,2]\n\nExplanation:\nThe sum of 1 and 2 is 3. Assuming a 1-indexed array, we return [1, 2].\nExample 2:\nInput: numbers = [-1,0], target = -1\nOutput: [1,2]\nConstraints:\n2 &lt;= numbers.length &lt;= 1000\n-1000 &lt;= numbers[i] &lt;= 1000\n-1000 &lt;= target &lt;= 1000\nnumbers is sorted in increasing order"
  },
  {
    "objectID": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#problem-description",
    "href": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#problem-description",
    "title": "LeetCode 167: Two Sum II - Input Array Is Sorted",
    "section": "",
    "text": "Difficulty: Medium\nGiven an array of integers numbers that is sorted in non-decreasing order.\nReturn the 1-indexed indices of two numbers, [index1, index2], such that they add up to a given target number target and index1 &lt; index2. Note that index1 and index2 cannot be equal; therefore, you may not use the same element twice.\nThere will always be exactly one valid solution.\nThe solution must use \\(O(1)\\) additional space.\nExample 1:\nInput: numbers = [1,2,3,4], target = 3\nOutput: [1,2]\n\nExplanation:\nThe sum of 1 and 2 is 3. Assuming a 1-indexed array, we return [1, 2].\nExample 2:\nInput: numbers = [-1,0], target = -1\nOutput: [1,2]\nConstraints:\n2 &lt;= numbers.length &lt;= 1000\n-1000 &lt;= numbers[i] &lt;= 1000\n-1000 &lt;= target &lt;= 1000\nnumbers is sorted in increasing order"
  },
  {
    "objectID": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#initial-solutions",
    "href": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#initial-solutions",
    "title": "LeetCode 167: Two Sum II - Input Array Is Sorted",
    "section": "Initial Solution(s)",
    "text": "Initial Solution(s)\nThis seems to be the same problem as Leetcode problem #1 (Two Sum) but with a 1-indexed array instead of a 0-indexed array. I just added + 1 into the old solution and it passed all the tests. Perhaps I’m missing something…\n\nfrom typing import List\n\n\nclass InitialSolution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        prevMap = {}  # val -&gt; index\n\n        for i, n in enumerate(numbers):\n            diff = target - n\n            if diff in prevMap:\n                return [prevMap[diff] + 1, i + 1]\n            prevMap[n] = i\n\nSo, yes, I was missing something! I missed the instruction saying “Your solution must use \\(O(1)\\) additional space.” The hashmap would use \\(O(n)\\) additional space in the worst case. NeetCode has this problem in the two-pointer algorithm category, so let’s try again with that approach.\n\nclass Solution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        # define left and right pointers\n        l = 0\n        r = len(numbers) - 1\n        \n        while l &lt; r:\n            summation = numbers[l] + numbers[r]\n            \n            if summation &gt; target:\n                r -= 1\n            elif summation &lt; target:\n                l += 1\n            else: # since we are told there is always a solution:\n                return [l + 1, r + 1]\n\n\nTest function\n\ndef test_two_sum(solution_class):\n    \"\"\" Test of twoSum function. Not extensive.\"\"\"\n    # Instantiate the class\n    solution = solution_class()\n\n    # Define the test cases\n    test_cases = [\n        {\n            \"numbers\": [2, 3, 4],\n            \"target\": 6,\n            \"expected\": [1, 3]\n        },\n        {\n            \"numbers\": [1, 2, 3, 4],\n            \"target\": 3,\n            \"expected\": [1, 2]\n        },\n         {\n            \"numbers\": [2, 3, 4, 5],\n            \"target\": 8,\n            \"expected\": [2, 4]\n        },\n        {\n            \"numbers\": [-1, 0],\n            \"target\": -1,\n            \"expected\": [1, 2]\n        }\n    ]\n    # Iterate over test cases\n    for i, test_case in enumerate(test_cases):\n        numbers = test_case[\"numbers\"]\n        target = test_case[\"target\"]\n        expected = test_case[\"expected\"]\n        \n        # Get the result from the twoSum method\n        results = solution.twoSum(numbers, target)\n        \n        # Check if the result matches the expected output\n        if results == expected:\n            print(f\"Test case {i+1} passed\")\n        else:\n            print(f\"Test case {i+1} failed\")\n            print(f\"Expected: {expected}\")\n            print(f\"Got: {results}\")\n\n\ntest_two_sum(InitialSolution)\n\nTest case 1 passed\nTest case 2 passed\nTest case 3 passed\nTest case 4 passed"
  },
  {
    "objectID": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#initial-results",
    "href": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#initial-results",
    "title": "LeetCode 167: Two Sum II - Input Array Is Sorted",
    "section": "Initial Results",
    "text": "Initial Results\nMy solution passed the LeetCode submission with a pretty average score. It beat 49% of submissions on runtime and 40% on memory. The distribution of the latter was very skewed, with the vast majority of submissions using the same amount of memory as my solution. In Big O notation, the space complexity is \\(O(1)\\) as we only use a constant amount of extra space (the two pointers l and r, and a few other variables). No additional data structures are used that would grow with the input size.\n\nTime complexity: \\(O(n)\\)\n\nAdditional space complexity: \\(O(1)\\)"
  },
  {
    "objectID": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#neetcode-solution",
    "href": "dsaposts/two-sum-integer-II/TwoSumIntegerII.html#neetcode-solution",
    "title": "LeetCode 167: Two Sum II - Input Array Is Sorted",
    "section": "NeetCode Solution",
    "text": "NeetCode Solution\nNeetCode’s solution was the same as my solution, but with different variable names. The code is provided above."
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html",
    "title": "FastAI Part 2 Lesson 10",
    "section": "",
    "text": "This notebook is based on the notebook for FastAI Practical Deep Learning for Coders: Part 2 Lesson 10.\nIt’s not really from Stable Diffusion from scratch, but instead we will develop the diffusion model from its component parts on Huggingface. Some content, including text explanations, was copied from the official Huggingface blog post.\nSee also: FastAI notebook on GitHub"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#what-is-stable-diffusion",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#what-is-stable-diffusion",
    "title": "FastAI Part 2 Lesson 10",
    "section": "What is Stable Diffusion",
    "text": "What is Stable Diffusion\nThere are three main components in latent diffusion.\n\nAn autoencoder (VAE).\nA U-Net.\nA text-encoder, e.g. CLIP’s Text Encoder.\n\nThe output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n\nPNDM scheduler (used by default)\nDDIM scheduler\nK-LMS scheduler\n\nTo make things a bit different, we’ll use another scheduler. The standard pipeline uses the PNDM Scheduler, but we’ll use Katherine Crowson’s excellent K-LMS scheduler.\nWe need to be careful to use the same noising schedule that was used during training. The schedule is defined by the number of noising steps and the amount of noise added at each step, which is derived from the beta parameters."
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#set-up-environment",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#set-up-environment",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Set up environment",
    "text": "Set up environment\n\n# Kaggle Python 3 environment is defined by the kaggle/python Docker image:\n# https://github.com/kaggle/docker-python\n\nimport os\n\nfor dirname, _, filenames in os.walk(\"/kaggle/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Installations and imports\n\n!pip install -Uq diffusers transformers fastcore\n\nimport logging\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\n# Show a smart progress meter: just wrap any iterable with tqdm(iterable)\nfrom tqdm.auto import tqdm\n\nlogging.disable(logging.WARNING)\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x7f98fa3730b0&gt;\n\n\n\nif not (Path.home() / \".cache/huggingface\" / \"token\").exists():\n    notebook_login()\n\n\n# Set device\ndevice = (\n    \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\nprint(device)\n\ncuda\n\n\nIf your GPU is not big enough to use pipe, run pipe.enable_attention_slicing()\nAs described in the docs:\n&gt; When this option is enabled, the attention module will split the input tensor in slices, to compute attention in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n# pipe.enable_attention_slicing()"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#import-and-initialize-model-components",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#import-and-initialize-model-components",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Import and initialize model components",
    "text": "Import and initialize model components\nHere we perform the following actions:\n\nImport and initialize the tokenizer and text encoder for processing the prompts.\nImport and initialize the VAE and U-Net models.\nImport and initialize the LMSD scheduler.\n\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16\n)\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16\n).to(device)\n\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained(\n    \"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16\n).to(device)\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16\n).to(device)\n\n\nfrom diffusers import LMSDiscreteScheduler\n\nscheduler = LMSDiscreteScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    num_train_timesteps=1000,\n)\nscheduler\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.28.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"steps_offset\": 0,\n  \"timestep_spacing\": \"linspace\",\n  \"trained_betas\": null,\n  \"use_karras_sigmas\": false\n}"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#create-initial-functions-for-testing",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#create-initial-functions-for-testing",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Create initial functions for testing",
    "text": "Create initial functions for testing\nLet’s create a few functions to perform the image generation, specifically:\n\nA text encoder to parse the prompt and return the text embeddings tensor\nA function to generate image samples based on the given text prompts\nA function to convert the tensor representations into images for display\n\nAs part of the FastAI ‘homework’, the ability to use negative prompts is included.\n\ndef text_enc(prompts, maxlen=None):\n    \"\"\"\n    Encodes text prompts into text embeddings using a pre-trained tokenizer \n    and text encoder.\n    \n    Parameters:\n        prompts (list or str): A single text prompt or a list of text prompts \n            to be encoded.\n        maxlen (int, optional): Maximum length for the tokenized sequences. \n            Defaults to the maximum length supported by the tokenizer.\n    \n    Returns:\n        torch.Tensor: Text embeddings corresponding to the input prompts.\n    \"\"\"\n    if maxlen is None:\n        maxlen = tokenizer.model_max_length\n    # Tokenize the prompts and create input tensors\n    inp = tokenizer(\n        prompts,\n        padding=\"max_length\",\n        max_length=maxlen,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    # Generate text embeddings from the input tensors using a text encoder\n    text_embeddings = text_encoder(inp.input_ids.to(device))[0].half()\n    return text_embeddings\n\nThe main parameters needed for image generation are:\n\nPrompt(s). In our implementation, by default, the negative prompt is an empty string.\nImage dimensions (height and width).\nNumber of inference steps. Less inference will result in a noisier image.\nGuidance scale, which controls the influence of the text prompt on image generation. Lower guidance gives the model more freedom to ‘imagine’.\nBatch size\nRandom seed\n\nDefault values are provided in the function definition below, where applicable.\n\ndef mk_samples(\n    prompts,\n    negative_prompt=[\"\"],\n    guidance=7.5,\n    seed=100,\n    steps=70,\n    height=512,\n    width=512,\n):\n    \"\"\"\n    Generates image samples based on the given text prompts using a pre-trained \n    diffusion model.\n    \n    Parameters:\n        prompts (list[str]): A list containing text string(s).\n        negative_prompt (list[str], optional): A list containing the negative \n            text prompt. One string only.\n        guidance (float, optional): Guidance scale for the diffusion process.\n        seed (int, optional): Random seed for reproducibility.\n        steps (int, optional): Number of diffusion steps.\n        height (int, optional): Height of the output images.\n        width (int, optional): Width of the output images.\n    \n    Returns:\n        torch.Tensor: Image samples generated based on the input prompts.\n    \"\"\"\n    bs = len(prompts)\n    text = text_enc(prompts)\n    # uncond = text_enc([\"\"] * bs, text.shape[1]) # implemented negative prompt instead\n    uncond = text_enc(negative_prompt * bs, maxlen=text.shape[1])\n\n    emb = torch.cat([uncond, text])\n    if seed:\n        torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.config.in_channels, height // 8, width // 8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(device).half() * scheduler.init_noise_sigma\n\n    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # predict the noise residual \n        # (and separate the text_embeddings and uncond_embeddings):\n        with torch.no_grad():\n            pred_uncond, pred_text = unet(\n                inp, ts, encoder_hidden_states=emb\n            ).sample.chunk(2)\n        \n        # perform guidance\n        pred = pred_uncond + guidance * (pred_text - pred_uncond)\n        \n        # compute the \"previous\" (next step) noisy sample\n        latents = scheduler.step(pred, ts, latents).prev_sample\n    \n    # decompress latents\n    with torch.no_grad():\n        return vae.decode(1 / 0.18215 * latents).sample\n\n\ndef mk_img(t):\n    \"\"\"\n    Converts a tensor representation of an image to a PIL Image for display.\n    Parameters:\n        t (torch.Tensor): Tensor representation of an image, where values are \n            in the range -1 to 1.\n    Returns:\n        PIL.Image.Image: Image object suitable for display, with pixel values \n            scaled to the range 0 to 255.\n    \"\"\"\n    # Scale and convert tensor values to a numpy array for image creation\n    image = (t / 2 + 0.5).clamp(0, 1).detach().cpu().permute(1, 2, 0).numpy()\n    # Convert the numpy array to a PIL Image\n    return Image.fromarray((image * 255).round().astype(\"uint8\"))\n\n\nTesting the functions\n\nprompts = [\n    \"A spaceman with Martian sunset in the background\",\n    \"A great ape eating a plate of chips. Realistic fur.\",\n]\nnegative_prompt = [\n    \"deformed, anime, cartoon, art\"\n]  \n\n# Note: current implementation accepts only a single string in the list, \n# and not a list of strings.\n\n\n# Example outputs and debugging.\n\ntext_input = tokenizer(\n    prompts[0],\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n)\nprint(\"tokenizer 'input_ids' key: \" + str(text_input.input_ids))\n\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\nprint(\"text embeddings shape: \" + str(text_embeddings.shape))\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * len(prompts[0]),\n    padding=\"max_length\",\n    max_length=max_length,\n    return_tensors=\"pt\",\n)\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nprint(\"uncond embeddings shape: \" + str(uncond_embeddings.shape))\n\ntokenizer 'input_ids' key: tensor([[49406,   320,  7857,   786,   593, 30214,  3424,   530,   518,  5994,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\ntext embeddings shape: torch.Size([1, 77, 768])\nuncond embeddings shape: torch.Size([48, 77, 768])\n\n\n\nimages = mk_samples(prompts, negative_prompt)\n\n\n\n\n\nfrom IPython.display import display\n\n\nfor img in images:\n    display(mk_img(img))"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#implement-diffuser-class",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#implement-diffuser-class",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Implement Diffuser Class",
    "text": "Implement Diffuser Class\nThe above functions work quite nicely, although the negative prompts could use a little work. At the moment we can only specify one negative prompt per batch of images. In most cases, that is fine anyway. For example, generally, we’d always want to avoid deformed images.\nLet’s put all the pieces together into a class:\n\nclass Diffuser:\n    \"\"\"\n    A class representing a text-to-image diffusion model.\n\n    Args:\n        prompts (list[str]): List of text prompts.\n        negative_prompt (list[str], optional): Negative text prompt consisting\n            of a single string only. Default is an empty string.\n        guidance (float, optional): Guidance for diffusion process. Default is 7.5.\n        seed (int, optional): Random seed for reproducibility. Default is 100.\n        steps (int, optional): Number of diffusion steps. Default is 70.\n        width (int, optional): Width of the output image. Default is 512.\n        height (int, optional): Height of the output image. Default is 512.\n    \"\"\"\n\n    def __init__(\n        self,\n        prompts,\n        negative_prompt=[\"\"],\n        guidance=7.5,\n        seed=100,\n        steps=70,\n        width=512,\n        height=512,\n    ):\n        self.prompts = prompts\n        self.bs = len(prompts)\n        self.negative_prompt = negative_prompt\n        self.guidance = guidance\n        self.seed = seed\n        self.steps = steps\n        self.w = width\n        self.h = height\n\n    def diffuse(self, progress=0):  # Progress indicator. Default is 0.\n        embs = self.set_embs()\n        lats = self.set_lats()\n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n        return self.decompress_lats(lats)\n\n    def set_embs(self):\n        txt_inp = self.tokenizer_seq(self.prompts)\n        neg_inp = self.tokenizer_seq(self.negative_prompt * len(self.prompts))\n\n        txt_embs = self.make_embs(txt_inp[\"input_ids\"])\n        neg_embs = self.make_embs(neg_inp[\"input_ids\"])\n        return torch.cat([neg_embs, txt_embs])\n\n    def tokenizer_seq(self, prompts, max_len=None):\n        if max_len is None:\n            max_len = tokenizer.model_max_length\n        return tokenizer(\n            prompts,\n            padding=\"max_length\",\n            max_length=max_len,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n    def make_embs(self, input_ids):\n        return text_encoder(input_ids.to(device))[0].half()\n\n    def set_lats(self):\n        torch.manual_seed(self.seed)\n        lats = torch.randn((self.bs, unet.config.in_channels, self.h // 8, self.w // 8))\n        scheduler.set_timesteps(self.steps)\n        return lats.to(device).half() * scheduler.init_noise_sigma\n\n    def denoise(self, latents, embeddings, timestep):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), timestep)\n        with torch.no_grad():\n            pred_neg, pred_txt = unet(\n                inp, timestep, encoder_hidden_states=embeddings\n            ).sample.chunk(2)\n        pred = pred_neg + self.guidance * (pred_txt - pred_neg)\n        return scheduler.step(pred, timestep, latents).prev_sample\n\n    def decompress_lats(self, latents):\n        with torch.no_grad():\n            imgs = vae.decode(1 / 0.18215 * latents).sample\n        imgs = (imgs / 2 + 0.5).clamp(0, 1)\n        imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n        return [(img * 255).round().astype(\"uint8\") for img in imgs]\n\n    def update_params(self, **kwargs):\n        allowed_params = [\n            \"prompts\",\n            \"negative_prompt\",\n            \"guidance\",\n            \"seed\",\n            \"steps\",\n            \"width\",\n            \"height\",\n        ]\n        for k, v in kwargs.items():\n            if k not in allowed_params:\n                raise ValueError(f\"Invalid parameter name: {k}\")\n        if k == \"prompts\":\n            self.prompts = v\n            self.bs = len(v)\n        else:\n            setattr(self, k, v)\n\n\nprompts = [\n    \"A spaceman with Martian sunset in the background\",\n    \"A great ape eating a plate of chips. Realistic fur.\",\n]\n\nnegative_prompt = [\n    \"deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, \"\n    \"cartoon, drawing, anime, text, cropped, out of frame, worst quality, low \"\n    \"quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, \"\n    \"mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, \"\n    \"blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, \"\n    \"disfigured, gross proportions, malformed limbs, missing arms, missing legs, \"\n    \"extra arms, extra legs, fused fingers, too many fingers, long neck\"\n]\n\n\n# Create an instance of the Diffuser class\ndiffuser = Diffuser(prompts, negative_prompt)\n\n# Perform diffusion\nresult_images = diffuser.diffuse()\n\nfor img_array in result_images:\n    plt.imshow(img_array)\n    plt.axis(\"off\")\n    plt.show()"
  },
  {
    "objectID": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#latents-and-callbacks",
    "href": "fastai/fastai-P2L10/fastai-P2L10-stable-diff.html#latents-and-callbacks",
    "title": "FastAI Part 2 Lesson 10",
    "section": "Latents and callbacks",
    "text": "Latents and callbacks\nStable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models.\nGeneral diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image. For a more detailed overview of how they work, check this colab.\nDiffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.\nLatent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\nThe Stable Diffusion pipeline can send intermediate latents to a callback function we provide. By running these latents through the image decoder, we can see how the denoising process progresses and the image unfolds.\n\nImplement callbacks\nFor the next part of the FastAI ‘homework’, let’s implement callbacks. We’ll modify Diffuser.diffuse() to output the latent at a pre-specified interval. A big thank you to ForBo7 for the key ideas here.\nSee: ForBo7 blog post\n\ndef diffuse_with_callback(self, interval=0):\n    \"\"\"\n    Diffuses the input text prompts to generate images using \n        a pre-trained diffusion model.\n\n    Parameters:\n        interval (int, optional): Specifies interval for displaying image callbacks.\n\n    Returns:\n        torch.Tensor: Image samples generated based on the input prompts.\n    \"\"\"\n    embs = self.set_embs()\n    lats = self.set_lats()\n\n    if interval &gt; 0:  # Check if callbacks are needed.\n        row = []\n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n            # Check if desired interval is reached.\n            if (i % interval) == 0:\n                row.append(self.decompress_lats(lats)[0])\n\n        row = np.concatenate(row, axis=1)  # Place all images into one long line.\n        display(Image.fromarray(row))\n\n    else:\n        for i, ts in enumerate(tqdm(scheduler.timesteps)):\n            lats = self.denoise(lats, embs, ts)\n\n    return self.decompress_lats(lats)\n\n\nprompts = [\"A spaceman with Martian sunset in the background\"]\nnegative_prompt = [\"deformed, anime, cartoon, art\"]\n\n\n# Create an instance of the Diffuser class\ndiffuser = Diffuser(prompts, negative_prompt)\n\n# Replace the existing diffuse method with the new diffuse_and_callback method.\ndiffuser.diffuse = diffuse_with_callback.__get__(diffuser, Diffuser)\n\n# Perform diffusion\nresult_images = diffuser.diffuse(interval=5)[0]\n\nImage.fromarray(result_images)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "spa-dev: an obvious work-in-progress"
  },
  {
    "objectID": "fastai.html",
    "href": "fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "A collection of workbooks for Fast AI Practical Deep Learning for Coders\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Part 2 Lesson 12\n\n\nMean Shift Clustering\n\n\n\nfastai\n\n\nmeanshift\n\n\nk-means\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nspa-dev\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Part 2 Lesson 10\n\n\nStable Diffusion from ‘Scratch’\n\n\n\nfastai\n\n\nstable diffusion\n\n\n\n\n\n\n\n\n\nJun 2, 2024\n\n\nspa-dev\n\n\n\n\n\n\nNo matching items"
  }
]